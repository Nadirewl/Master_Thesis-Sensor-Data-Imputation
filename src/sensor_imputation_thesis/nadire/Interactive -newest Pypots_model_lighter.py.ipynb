{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connected to .venv (Python 3.10.16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776114e4-27e2-432c-8122-3e7e2553d0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 20:19:47 [WARNING]: â€¼ï¸ PyPOTS Ecosystem configuration file does not exist.\n",
      "2025-05-13 20:19:47 [INFO]: Wrote new configs to config.ini successfully.\n",
      "2025-05-13 20:19:47 [INFO]: ğŸ’« Initialized PyPOTS Ecosystem configuration file /home/ec2-user/.pypots/config.ini successfully.\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—\n",
      "â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘\n",
      "   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘\n",
      "   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘\n",
      "   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘\n",
      "   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•\n",
      "ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai \u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "119439"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import Pypots Library\n",
    "from pypots.optim import Adam\n",
    "from pypots.imputation import SAITS\n",
    "#from pypots.utils.metrics import calc_mae\n",
    "from pypots.nn.functional import calc_mae\n",
    "\n",
    "\n",
    "import argparse\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import data_insight\n",
    "from data_insight import setup_duckdb\n",
    "from duckdb import DuckDBPyConnection as DuckDB\n",
    "from duckdb import DuckDBPyRelation as Relation\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "from duckdb import DuckDBPyConnection as DuckDB\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import optuna \n",
    "from optuna.visualization import plot_optimization_history\n",
    "\n",
    "\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, Dataset\n",
    "from pygrinder.missing_completely_at_random import mcar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import sensor_imputation_thesis.shared.load_data as load\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "#PatchTST might be an ideal choise if SAITS is too slow \n",
    "\n",
    "##Drop columns with different indexes while loading data.. Or the mean values \n",
    "\n",
    "df=pd.read_parquet(\"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/Newdataframeforpypots.parquet\")\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf56846-7e21-4453-933c-d6e991c34e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column time has 0 NaN values\n",
      "Column time has 0.0 Missing_rate\n",
      "Column fr_eng has 0 NaN values\n",
      "Column fr_eng has 0.0 Missing_rate\n",
      "Column te_exh_cyl_out__0 has 73 NaN values\n",
      "Column te_exh_cyl_out__0 has 0.0006111906496203082 Missing_rate\n",
      "Column pd_air_ic__0 has 73 NaN values\n",
      "Column pd_air_ic__0 has 0.0006111906496203082 Missing_rate\n",
      "Column pr_exh_turb_out__0 has 119439 NaN values\n",
      "Column pr_exh_turb_out__0 has 1.0 Missing_rate\n",
      "Column te_air_ic_out__0 has 73 NaN values\n",
      "Column te_air_ic_out__0 has 0.0006111906496203082 Missing_rate\n",
      "Column te_seawater has 73 NaN values\n",
      "Column te_seawater has 0.0006111906496203082 Missing_rate\n",
      "Column te_air_comp_in_a__0 has 119439 NaN values\n",
      "Column te_air_comp_in_a__0 has 1.0 Missing_rate\n",
      "Column te_air_comp_in_b__0 has 119439 NaN values\n",
      "Column te_air_comp_in_b__0 has 1.0 Missing_rate\n",
      "Column fr_tc__0 has 119439 NaN values\n",
      "Column fr_tc__0 has 1.0 Missing_rate\n",
      "Column pr_baro has 73 NaN values\n",
      "Column pr_baro has 0.0006111906496203082 Missing_rate\n",
      "Column pd_air_ic__0_1 has 73 NaN values\n",
      "Column pd_air_ic__0_1 has 0.0006111906496203082 Missing_rate\n",
      "Column pr_exh_rec has 73 NaN values\n",
      "Column pr_exh_rec has 0.0006111906496203082 Missing_rate\n",
      "Column te_exh_turb_in__0 has 119439 NaN values\n",
      "Column te_exh_turb_in__0 has 1.0 Missing_rate\n",
      "Column te_exh_turb_out__0 has 119439 NaN values\n",
      "Column te_exh_turb_out__0 has 1.0 Missing_rate\n",
      "Column bo_aux_blower_running has 0 NaN values\n",
      "Column bo_aux_blower_running has 0.0 Missing_rate\n",
      "Column re_eng_load has 173 NaN values\n",
      "Column re_eng_load has 0.0014484381148536073 Missing_rate\n",
      "Column pr_air_scav_ecs has 0 NaN values\n",
      "Column pr_air_scav_ecs has 0.0 Missing_rate\n",
      "Column pr_air_scav has 0 NaN values\n",
      "Column pr_air_scav has 0.0 Missing_rate\n",
      "Column te_air_scav_rec has 73 NaN values\n",
      "Column te_air_scav_rec has 0.0006111906496203082 Missing_rate\n",
      "Column te_air_ic_out__0_1 has 73 NaN values\n",
      "Column te_air_ic_out__0_1 has 0.0006111906496203082 Missing_rate\n",
      "Column pr_cyl_comp__0 has 173 NaN values\n",
      "Column pr_cyl_comp__0 has 0.0014484381148536073 Missing_rate\n",
      "Column pr_cyl_max__0 has 173 NaN values\n",
      "Column pr_cyl_max__0 has 0.0014484381148536073 Missing_rate\n",
      "Column se_mip__0 has 173 NaN values\n",
      "Column se_mip__0 has 0.0014484381148536073 Missing_rate\n",
      "Column te_exh_cyl_out__0_1 has 73 NaN values\n",
      "Column te_exh_cyl_out__0_1 has 0.0006111906496203082 Missing_rate\n",
      "Column fr_eng_setpoint has 0 NaN values\n",
      "Column fr_eng_setpoint has 0.0 Missing_rate\n",
      "Column te_air_scav_rec_iso has 51235 NaN values\n",
      "Column te_air_scav_rec_iso has 0.4289637388122807 Missing_rate\n",
      "Column pr_cyl_max_mv_iso has 51385 NaN values\n",
      "Column pr_cyl_max_mv_iso has 0.4302196100101307 Missing_rate\n",
      "Column pr_cyl_comp_mv_iso has 51385 NaN values\n",
      "Column pr_cyl_comp_mv_iso has 0.4302196100101307 Missing_rate\n",
      "Column fr_eng_ecs has 0 NaN values\n",
      "Column fr_eng_ecs has 0.0 Missing_rate\n",
      "Column pr_air_scav_iso has 51385 NaN values\n",
      "Column pr_air_scav_iso has 0.4302196100101307 Missing_rate\n",
      "Column engine_type_G95ME-C10.5-GI-EGRTC has 0 NaN values\n",
      "Column engine_type_G95ME-C10.5-GI-EGRTC has 0.0 Missing_rate\n",
      "Original size:119439, Sampled size: 11944\n",
      "Reshaped data:(11944, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    },
    {
     "ename": "MlflowException",
     "evalue": "API request to http://localhost:5000/api/2.0/mlflow/experiments/get-by-name failed with exception HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /api/2.0/mlflow/experiments/get-by-name?experiment_name=SAITS (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f1b5015c760>: Failed to establish a new connection: [Errno 111] Connection refused'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connection.py:199\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 199\u001b[0m     sock \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[1;32m    200\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport),\n\u001b[1;32m    201\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout,\n\u001b[1;32m    202\u001b[0m         source_address\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msource_address,\n\u001b[1;32m    203\u001b[0m         socket_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msocket_options,\n\u001b[1;32m    204\u001b[0m     )\n\u001b[1;32m    205\u001b[0m \u001b[39mexcept\u001b[39;00m socket\u001b[39m.\u001b[39mgaierror \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     \u001b[39mraise\u001b[39;00m err\n\u001b[1;32m     86\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     \u001b[39m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m     sock\u001b[39m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m sock\u001b[39m.\u001b[39;49mconnect(sa)\n\u001b[1;32m     74\u001b[0m \u001b[39m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    790\u001b[0m     conn,\n\u001b[1;32m    791\u001b[0m     method,\n\u001b[1;32m    792\u001b[0m     url,\n\u001b[1;32m    793\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    794\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    795\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    796\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    797\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[1;32m    798\u001b[0m     response_conn\u001b[39m=\u001b[39;49mresponse_conn,\n\u001b[1;32m    799\u001b[0m     preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    800\u001b[0m     decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    801\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    802\u001b[0m )\n\u001b[1;32m    804\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:495\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 495\u001b[0m     conn\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    496\u001b[0m         method,\n\u001b[1;32m    497\u001b[0m         url,\n\u001b[1;32m    498\u001b[0m         body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    499\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    500\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    501\u001b[0m         preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    502\u001b[0m         decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    503\u001b[0m         enforce_content_length\u001b[39m=\u001b[39;49menforce_content_length,\n\u001b[1;32m    504\u001b[0m     )\n\u001b[1;32m    506\u001b[0m \u001b[39m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \u001b[39m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connection.py:441\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mputheader(header, value)\n\u001b[0;32m--> 441\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendheaders()\n\u001b[1;32m    443\u001b[0m \u001b[39m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/gdmake/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/http/client.py:1278\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[39mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1278\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_output(message_body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[0;32m~/SageMaker/gdmake/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/http/client.py:1038\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1038\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(msg)\n\u001b[1;32m   1040\u001b[0m \u001b[39mif\u001b[39;00m message_body \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1041\u001b[0m \n\u001b[1;32m   1042\u001b[0m     \u001b[39m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/gdmake/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/http/client.py:976\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_open:\n\u001b[0;32m--> 976\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m    977\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connection.py:279\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tunnel_host:\n\u001b[1;32m    281\u001b[0m         \u001b[39m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connection.py:214\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 214\u001b[0m     \u001b[39mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    215\u001b[0m         \u001b[39mself\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to establish a new connection: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    216\u001b[0m     ) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39me\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[39m# Audit hooks are only available in Python 3.8+\u001b[39;00m\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x7f1b5015c760>: Failed to establish a new connection: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    668\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    669\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    670\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    671\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    672\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    673\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    674\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    675\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    676\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    677\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    678\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    679\u001b[0m     )\n\u001b[1;32m    681\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:873\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    870\u001b[0m     log\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    871\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRetrying (\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m) after connection broken by \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, retries, err, url\n\u001b[1;32m    872\u001b[0m     )\n\u001b[0;32m--> 873\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    874\u001b[0m         method,\n\u001b[1;32m    875\u001b[0m         url,\n\u001b[1;32m    876\u001b[0m         body,\n\u001b[1;32m    877\u001b[0m         headers,\n\u001b[1;32m    878\u001b[0m         retries,\n\u001b[1;32m    879\u001b[0m         redirect,\n\u001b[1;32m    880\u001b[0m         assert_same_host,\n\u001b[1;32m    881\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    882\u001b[0m         pool_timeout\u001b[39m=\u001b[39;49mpool_timeout,\n\u001b[1;32m    883\u001b[0m         release_conn\u001b[39m=\u001b[39;49mrelease_conn,\n\u001b[1;32m    884\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    885\u001b[0m         body_pos\u001b[39m=\u001b[39;49mbody_pos,\n\u001b[1;32m    886\u001b[0m         preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    887\u001b[0m         decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    888\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    889\u001b[0m     )\n\u001b[1;32m    891\u001b[0m \u001b[39m# Handle redirect?\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:873\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    870\u001b[0m     log\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    871\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRetrying (\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m) after connection broken by \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, retries, err, url\n\u001b[1;32m    872\u001b[0m     )\n\u001b[0;32m--> 873\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    874\u001b[0m         method,\n\u001b[1;32m    875\u001b[0m         url,\n\u001b[1;32m    876\u001b[0m         body,\n\u001b[1;32m    877\u001b[0m         headers,\n\u001b[1;32m    878\u001b[0m         retries,\n\u001b[1;32m    879\u001b[0m         redirect,\n\u001b[1;32m    880\u001b[0m         assert_same_host,\n\u001b[1;32m    881\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    882\u001b[0m         pool_timeout\u001b[39m=\u001b[39;49mpool_timeout,\n\u001b[1;32m    883\u001b[0m         release_conn\u001b[39m=\u001b[39;49mrelease_conn,\n\u001b[1;32m    884\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    885\u001b[0m         body_pos\u001b[39m=\u001b[39;49mbody_pos,\n\u001b[1;32m    886\u001b[0m         preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    887\u001b[0m         decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    888\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    889\u001b[0m     )\n\u001b[1;32m    891\u001b[0m \u001b[39m# Handle redirect?\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: HTTPConnectionPool.urlopen at line 873 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:873\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    870\u001b[0m     log\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    871\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRetrying (\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m) after connection broken by \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, retries, err, url\n\u001b[1;32m    872\u001b[0m     )\n\u001b[0;32m--> 873\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    874\u001b[0m         method,\n\u001b[1;32m    875\u001b[0m         url,\n\u001b[1;32m    876\u001b[0m         body,\n\u001b[1;32m    877\u001b[0m         headers,\n\u001b[1;32m    878\u001b[0m         retries,\n\u001b[1;32m    879\u001b[0m         redirect,\n\u001b[1;32m    880\u001b[0m         assert_same_host,\n\u001b[1;32m    881\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    882\u001b[0m         pool_timeout\u001b[39m=\u001b[39;49mpool_timeout,\n\u001b[1;32m    883\u001b[0m         release_conn\u001b[39m=\u001b[39;49mrelease_conn,\n\u001b[1;32m    884\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    885\u001b[0m         body_pos\u001b[39m=\u001b[39;49mbody_pos,\n\u001b[1;32m    886\u001b[0m         preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    887\u001b[0m         decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    888\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    889\u001b[0m     )\n\u001b[1;32m    891\u001b[0m \u001b[39m# Handle redirect?\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:843\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    841\u001b[0m     new_e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, new_e)\n\u001b[0;32m--> 843\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[1;32m    844\u001b[0m     method, url, error\u001b[39m=\u001b[39;49mnew_e, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[1;32m    845\u001b[0m )\n\u001b[1;32m    846\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/util/retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    518\u001b[0m     reason \u001b[39m=\u001b[39m error \u001b[39mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 519\u001b[0m     \u001b[39mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mreason\u001b[39;00m  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    521\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mIncremented Retry for (url=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m): \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /api/2.0/mlflow/experiments/get-by-name?experiment_name=SAITS (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f1b5015c760>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/utils/rest_utils.py:189\u001b[0m, in \u001b[0;36mhttp_request\u001b[0;34m(host_creds, endpoint, method, max_retries, backoff_factor, backoff_jitter, extra_headers, retry_codes, timeout, raise_on_status, respect_retry_after_header, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 189\u001b[0m     \u001b[39mreturn\u001b[39;00m _get_http_response_with_retries(\n\u001b[1;32m    190\u001b[0m         method,\n\u001b[1;32m    191\u001b[0m         url,\n\u001b[1;32m    192\u001b[0m         max_retries,\n\u001b[1;32m    193\u001b[0m         backoff_factor,\n\u001b[1;32m    194\u001b[0m         backoff_jitter,\n\u001b[1;32m    195\u001b[0m         retry_codes,\n\u001b[1;32m    196\u001b[0m         raise_on_status,\n\u001b[1;32m    197\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    198\u001b[0m         verify\u001b[39m=\u001b[39;49mhost_creds\u001b[39m.\u001b[39;49mverify,\n\u001b[1;32m    199\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    200\u001b[0m         respect_retry_after_header\u001b[39m=\u001b[39;49mrespect_retry_after_header,\n\u001b[1;32m    201\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    202\u001b[0m     )\n\u001b[1;32m    203\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout \u001b[39mas\u001b[39;00m to:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/utils/request_utils.py:237\u001b[0m, in \u001b[0;36m_get_http_response_with_retries\u001b[0;34m(method, url, max_retries, backoff_factor, backoff_jitter, retry_codes, raise_on_status, allow_redirects, respect_retry_after_header, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m allow_redirects \u001b[39m=\u001b[39m env_value \u001b[39mif\u001b[39;00m allow_redirects \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m allow_redirects\n\u001b[0;32m--> 237\u001b[0m \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method, url, allow_redirects\u001b[39m=\u001b[39;49mallow_redirects, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/requests/adapters.py:700\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[39mraise\u001b[39;00m SSLError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[0;32m--> 700\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    702\u001b[0m \u001b[39mexcept\u001b[39;00m ClosedPoolError \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /api/2.0/mlflow/experiments/get-by-name?experiment_name=SAITS (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f1b5015c760>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMlflowException\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/Pypots_model_lighter.py:230\u001b[0m\n\u001b[1;32m    228\u001b[0m mlflow\u001b[39m.\u001b[39mset_tracking_uri(\u001b[39m\"\u001b[39m\u001b[39mhttp://localhost:5000\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    229\u001b[0m client \u001b[39m=\u001b[39m mlflow\u001b[39m.\u001b[39mtracking\u001b[39m.\u001b[39mMlflowClient()\n\u001b[0;32m--> 230\u001b[0m mlflow\u001b[39m.\u001b[39;49mset_experiment(\u001b[39m\"\u001b[39;49m\u001b[39mSAITS\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    231\u001b[0m SAITS_run_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mSAITS\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    233\u001b[0m \u001b[39mwith\u001b[39;00m mlflow\u001b[39m.\u001b[39mstart_run(run_name\u001b[39m=\u001b[39mSAITS_run_name) \u001b[39mas\u001b[39;00m run:\n\u001b[1;32m    234\u001b[0m \u001b[39m# initialize the model (from example)\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/tracking/fluent.py:157\u001b[0m, in \u001b[0;36mset_experiment\u001b[0;34m(experiment_name, experiment_id)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39mwith\u001b[39;00m _experiment_lock:\n\u001b[1;32m    156\u001b[0m     \u001b[39mif\u001b[39;00m experiment_id \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m         experiment \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mget_experiment_by_name(experiment_name)\n\u001b[1;32m    158\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m experiment:\n\u001b[1;32m    159\u001b[0m             \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/tracking/client.py:1257\u001b[0m, in \u001b[0;36mMlflowClient.get_experiment_by_name\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mget_experiment_by_name\u001b[39m(\u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Experiment]:\n\u001b[1;32m   1226\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Retrieve an experiment by experiment name from the backend store\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m \n\u001b[1;32m   1228\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1255\u001b[0m \u001b[39m        Lifecycle_stage: active\u001b[39;00m\n\u001b[1;32m   1256\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1257\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tracking_client\u001b[39m.\u001b[39;49mget_experiment_by_name(name)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py:502\u001b[0m, in \u001b[0;36mTrackingServiceClient.get_experiment_by_name\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mget_experiment_by_name\u001b[39m(\u001b[39mself\u001b[39m, name):\n\u001b[1;32m    495\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    497\u001b[0m \u001b[39m        name: The experiment name.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[39m        :py:class:`mlflow.entities.Experiment`\u001b[39;00m\n\u001b[1;32m    501\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 502\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstore\u001b[39m.\u001b[39;49mget_experiment_by_name(name)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/store/tracking/rest_store.py:522\u001b[0m, in \u001b[0;36mRestStore.get_experiment_by_name\u001b[0;34m(self, experiment_name)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    521\u001b[0m     req_body \u001b[39m=\u001b[39m message_to_json(GetExperimentByName(experiment_name\u001b[39m=\u001b[39mexperiment_name))\n\u001b[0;32m--> 522\u001b[0m     response_proto \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_endpoint(GetExperimentByName, req_body)\n\u001b[1;32m    523\u001b[0m     \u001b[39mreturn\u001b[39;00m Experiment\u001b[39m.\u001b[39mfrom_proto(response_proto\u001b[39m.\u001b[39mexperiment)\n\u001b[1;32m    524\u001b[0m \u001b[39mexcept\u001b[39;00m MlflowException \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/store/tracking/rest_store.py:82\u001b[0m, in \u001b[0;36mRestStore._call_endpoint\u001b[0;34m(self, api, json_body, endpoint)\u001b[0m\n\u001b[1;32m     80\u001b[0m     endpoint, method \u001b[39m=\u001b[39m _METHOD_TO_INFO[api]\n\u001b[1;32m     81\u001b[0m response_proto \u001b[39m=\u001b[39m api\u001b[39m.\u001b[39mResponse()\n\u001b[0;32m---> 82\u001b[0m \u001b[39mreturn\u001b[39;00m call_endpoint(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_host_creds(), endpoint, method, json_body, response_proto)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/utils/rest_utils.py:365\u001b[0m, in \u001b[0;36mcall_endpoint\u001b[0;34m(host_creds, endpoint, method, json_body, response_proto, extra_headers)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[39mif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mGET\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    364\u001b[0m     call_kwargs[\u001b[39m\"\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m json_body\n\u001b[0;32m--> 365\u001b[0m     response \u001b[39m=\u001b[39m http_request(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcall_kwargs)\n\u001b[1;32m    366\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    367\u001b[0m     call_kwargs[\u001b[39m\"\u001b[39m\u001b[39mjson\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m json_body\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/utils/rest_utils.py:212\u001b[0m, in \u001b[0;36mhttp_request\u001b[0;34m(host_creds, endpoint, method, max_retries, backoff_factor, backoff_jitter, extra_headers, retry_codes, timeout, raise_on_status, respect_retry_after_header, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[39mraise\u001b[39;00m InvalidUrlException(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid url: \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39miu\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 212\u001b[0m     \u001b[39mraise\u001b[39;00m MlflowException(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAPI request to \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m failed with exception \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mMlflowException\u001b[0m: API request to http://localhost:5000/api/2.0/mlflow/experiments/get-by-name failed with exception HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /api/2.0/mlflow/experiments/get-by-name?experiment_name=SAITS (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f1b5015c760>: Failed to establish a new connection: [Errno 111] Connection refused'))"
     ]
    }
   ],
   "source": [
    "#Import Pypots Library\n",
    "from pypots.optim import Adam\n",
    "from pypots.imputation import SAITS\n",
    "#from pypots.utils.metrics import calc_mae\n",
    "from pypots.nn.functional import calc_mae\n",
    "\n",
    "\n",
    "import argparse\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import data_insight\n",
    "from data_insight import setup_duckdb\n",
    "from duckdb import DuckDBPyConnection as DuckDB\n",
    "from duckdb import DuckDBPyRelation as Relation\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "from duckdb import DuckDBPyConnection as DuckDB\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import optuna \n",
    "from optuna.visualization import plot_optimization_history\n",
    "\n",
    "\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, Dataset\n",
    "from pygrinder.missing_completely_at_random import mcar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import sensor_imputation_thesis.shared.load_data as load\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "#PatchTST might be an ideal choise if SAITS is too slow \n",
    "\n",
    "##Drop columns with different indexes while loading data.. Or the mean values \n",
    "\n",
    "df=pd.read_parquet(\"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/Newdataframeforpypots.parquet\")\n",
    "\n",
    "len(df)\n",
    "\n",
    "#current length of the dataframe is 119439\n",
    "\n",
    "# Check nan values in each column\n",
    "for col in df.columns:\n",
    "    print(f\"Column {col} has {df[col].isna().sum()} NaN values\")\n",
    "    missing_rate=df[col].isna().sum()/len(df[col])\n",
    "    print(f\"Column {col} has {missing_rate} Missing_rate\")\n",
    "\n",
    "\n",
    "#Try with smaller dataset, size 4000\n",
    "##SAMPLE the percengtage of the dataset, df.sample (averagely pick samples)\n",
    "#not df.sample cuz it will randomly select \n",
    "original_size=len(df)\n",
    "desired_fraction=0.10\n",
    "step=int(1/desired_fraction) #step_size=10 (sample every 10th minute)\n",
    "\n",
    "#Systematic sampling: Start at a random offset to avoid bias \n",
    "start=np.random.randint(0,step) #Random start between 0-9\n",
    "df1=df.iloc[start::step].reset_index(drop=True)\n",
    "\n",
    "print(f\"Original size:{len(df)}, Sampled size: {len(df1)}\")\n",
    "\n",
    "\n",
    "#df1=df[:10000]\n",
    "\n",
    "\n",
    "# Custom Dataset class\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Data processing code\n",
    "sensor_cols = [col for col in df1.columns if col != \"time\"]\n",
    "data = df1[sensor_cols].values\n",
    "\n",
    "#Â¤get feature names for printing mae later \n",
    "feature_names=df1[sensor_cols].columns.tolist()\n",
    "\n",
    "## Convert data to 3D arrays of shape n_samples, n_timesteps, n_features, X_ori refers to the original data without missing values \n",
    "## Reconstruct all columns simultaneously  #num_features: 119\n",
    "n_features = data.shape[1]  # exclude the time column\n",
    "n_steps = 1 #60 (was 60 previously) #(TRY TO CHANGE HERE)  # # window length, 1440 steps = 24 hours of 1-minute data, but here is revised to 60 again\n",
    "#total_elements = data.shape[0] * data.shape[1]\n",
    "n_samples = data.shape[0] // n_steps \n",
    "\n",
    "\n",
    "\n",
    "# Reshape to (n_samples // n_steps, n_steps, n_features)\n",
    "#data_reshaped = data.reshape((n_samples, n_steps, n_features))\n",
    "data_reshaped=data[:n_samples*n_steps].reshape(n_samples,n_steps,n_features)\n",
    "print(f\"Reshaped data:{data.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "##Normalization is important because of the nature of mse calculation of saits, columns with large \n",
    "#values dominate the loss, making metrics meaningless. SAITS computes MSE/MAE column-wise and averages \n",
    "#them across all columns \n",
    "#  Apply minmax scaler here \n",
    "#normalize each feature independently\n",
    "scalers={}\n",
    "data_normalized=data.copy()\n",
    "\n",
    "data_normalized = np.zeros_like(data_reshaped)  # Initialize the normalized data array\n",
    "scalers = {}\n",
    "\n",
    "for i in range(data_reshaped.shape[2]):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    # Flatten timesteps and samples for scaling\n",
    "    data_normalized[:, :, i] = scaler.fit_transform(data_reshaped[:, :, i].reshape(-1, 1)).reshape(data_reshaped.shape[0], data_reshaped.shape[1])\n",
    "    scalers[i] = scaler  # Save scalers to inverse-transform later\n",
    "\n",
    "\n",
    "\n",
    "#Split into train, test, val\n",
    "\n",
    "train_size = int(0.6 * len(data_normalized))\n",
    "val_size = int(0.2 * len(data_normalized))\n",
    "test_size = len(data_normalized) - train_size - val_size\n",
    "\n",
    "train_data = data_normalized[:train_size]\n",
    "val_data = data_normalized[train_size:train_size + val_size]\n",
    "test_data= data_normalized[train_size + val_size:]\n",
    "\n",
    "#Optional: Artificially mask. Mask 20% of the data (MIT part)\n",
    "def mcar_f(X, mask_ratio=0.2):\n",
    "    \"\"\"Apply MCAR only to observed values.\"\"\"\n",
    "    observed_mask=~np.isnan(X) #find observed positions\n",
    "    artificial_mask=mcar(X,mask_ratio).astype(bool) #generate MCAR mask, cast to boolean\n",
    "    #combine masks \n",
    "    combined_mask=observed_mask & artificial_mask\n",
    "\n",
    "    #Apply masking\n",
    "    X_masked=X.copy()\n",
    "    X_masked[combined_mask]=np.nan\n",
    "    return X_masked,combined_mask\n",
    "\n",
    "\n",
    "#Use mcar on validation data \n",
    "val_X_masked, val_mask =mcar_f(val_data)\n",
    "val_X_ori=val_data.copy() #Ground truth\n",
    "\n",
    "test_X_masked, test_mask =mcar_f(test_data)\n",
    "test_X_ori=test_data.copy() #ground truth\n",
    "\n",
    "#?? Problem: Can't have the best input for testing\n",
    "#1.Create synthetic test_data cuz if I drop nan values for test set, there's basically nothing left\n",
    "#synthetic_data=np.random.randn(n_samples,n_steps,n_features)\n",
    "#test_X_masked,test_mask=mcar_f(synthetic_data)\n",
    "#test_X_ori=synthetic_data.copy() #Ground truth\n",
    "\n",
    "# 2, Ensure no NaN values in synthetic data\n",
    "#test_X_masked = np.nan_to_num(test_X_masked, nan=np.nanmean(test_X_masked))\n",
    "#test_X_ori = np.nan_to_num(test_X_ori, nan=np.nanmean(test_X_ori))\n",
    "\n",
    "\n",
    "#3. Set aside a part of dataset for test_set\n",
    "#df_test=df1[6000:8000]  #Even with 10000 rows of dataset fot testing, after dropping nan value in rows, there is none left \n",
    "#2. Drop nan rows \n",
    "#f_test=df_test.dropna(axis=0,how='all')\n",
    "#print(\"Size of the test set:\", len(df_test)) \n",
    "#data_test=df_test[sensor_cols].values\n",
    "\n",
    "\n",
    "\n",
    "#n_features = data_test.shape[1]  # exclude the time column\n",
    "#n_steps = 60 #(TRY TO CHANGE HERE)  # # window length, 1440 steps = 24 hours of 1-minute data, but here is revised to 60 again\n",
    "#total_elements = data.shape[0] * data.shape[1]\n",
    "#n_samples = data_test.shape[0] // n_steps \n",
    "#Reshape and apply mcar to the test set \n",
    "# #data_test=data_test[:n_samples*n_steps].reshape(n_samples,n_steps,n_features)\n",
    "\n",
    "#data_test_normalized=np.zeros_like(data_test)  # Initialize the normalized data array\n",
    "#for i in range(data_test.shape[2]):\n",
    "  # scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    # Flatten timesteps and samples for scaling\n",
    "  # data_test_normalized[:, :, i] = scaler.fit_transform(data_test[:, :, i].reshape(-1, 1)).reshape(data_test.shape[0], data_test.shape[1])\n",
    "  # scalers[i] = scaler  # Save scalers to inverse-transform later\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Config:\n",
    "    no_cuda = False\n",
    "    no_mps = False\n",
    "    seed = 1\n",
    "\n",
    "args=Config()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "use_mps = not args.no_mps and torch.backends.mps.is_available()\n",
    "\n",
    "if args.cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA\")\n",
    "elif use_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "\n",
    "#MLflow set up\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "mlflow.set_experiment(\"SAITS\")\n",
    "SAITS_run_name = \"SAITS\"\n",
    "\n",
    "with mlflow.start_run(run_name=SAITS_run_name) as run:\n",
    "# initialize the model (from example)\n",
    "    saits = SAITS(\n",
    "        n_steps=data_normalized.shape[1],\n",
    "        n_features=data_normalized.shape[2],\n",
    "        n_layers=2, #deep network(4) for long sequences, here is revised for 2 since it always shows cuda error \n",
    "        d_model=512,  #d_modle must equal n_heads * d_k\n",
    "        d_ffn=512,\n",
    "        n_heads=8,\n",
    "        d_k=64,\n",
    "        d_v=64,\n",
    "        dropout=0.1,\n",
    "        attn_dropout=0.1,\n",
    "        diagonal_attention_mask=True,  # otherwise the original self-attention mechanism will be applied\n",
    "        ORT_weight=1,  # you can adjust the weight values of arguments ORT_weight\n",
    "       # and MIT_weight to make the SAITS model focus more on one task. Usually you can just leave them to the default values, i.e. 1.\n",
    "        MIT_weight=1,\n",
    "        batch_size=5, #try with 5 to see if it works, was 4. \n",
    "        # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "        epochs=10, #try with 10 to see if it runs\n",
    "        # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "        # You can leave it to defualt as None to disable early stopping.\n",
    "        patience=4, #initially was 3, tried to increase to see more possibilities \n",
    "        # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "        # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "        optimizer=Adam(lr=1e-3),\n",
    "        # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "        # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "        # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "        num_workers=0,\n",
    "        # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "        # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "        device=device, \n",
    "        # set the path for saving tensorboard and trained model files \n",
    "        saving_path=\"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/best_model\",\n",
    "        # only save the best model after training finished.\n",
    "        # You can also set it as \"better\" to save models performing better ever during training.\n",
    "        model_saving_strategy=\"best\",\n",
    ")\n",
    "\n",
    "\n",
    "# Log model parameters\n",
    "mlflow.log_param(\"n_steps\", data_normalized.shape[1])\n",
    "mlflow.log_param(\"n_features\", data_normalized.shape[2])\n",
    "mlflow.log_param(\"batch_size\", 5)\n",
    "mlflow.log_param(\"epochs\", 10)\n",
    "mlflow.log_param(\"patience\", 4)\n",
    "\n",
    "\n",
    "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
    "#use original missigness for trainig, use the mcar masked points as ground truth\n",
    "saits.fit(train_set={\"X\": train_data}, val_set={\"X\": val_X_masked , \"X_ori\":val_X_ori })\n",
    "\n",
    "\n",
    "##drop null values in test set \n",
    "\n",
    "# Check for NaN values across the entire array\n",
    "#nan_mask = np.isnan(test_X).any(axis=(1, 2))\n",
    "\n",
    "# Filter out samples (rows) that contain NaN values\n",
    "#test_X_clean = test_X[~nan_mask]\n",
    "#test_X_ori = test_X_clean\n",
    "#test_X_masked, test_mask = mcar_f(test_X_clean)\n",
    "\n",
    "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
    "# Convert the numpy array to a dictionary\n",
    "test_set_dict = {\"X\": test_X_masked}\n",
    "test_imputation = saits.predict(test_set_dict)\n",
    "\n",
    "#change back to array for applying inverse scale later\n",
    "test_imputation_array = test_imputation[\"imputation\"]\n",
    "\n",
    "\n",
    "#clear cuda after prediction if on cuda\n",
    "\n",
    "# Ensure saits_imputation and test_X are numpy arrays\n",
    "#if not isinstance(saits_imputation, np.ndarray):\n",
    "    #saits_imputation = np.array(saits_imputation)\n",
    "#if not isinstance(test_X, np.ndarray):\n",
    "     #test_X = np.array(test_X)\n",
    "\n",
    "#Inverse Scale\n",
    "def inverse_scale(imputation, scalers):\n",
    "    n_features = imputation.shape[2]\n",
    "    imputation_denorm = np.empty_like(imputation)\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        imputation_denorm[:, :, i] = scalers[i].inverse_transform(imputation[:, :, i].reshape(-1, 1)).reshape(imputation.shape[0], imputation.shape[1])\n",
    "    \n",
    "    return imputation_denorm  # Move the return statement outside the loop\n",
    "\n",
    "    \n",
    "#Apply function to the dataset \n",
    "test_imputation_denorm=inverse_scale(test_imputation_array,scalers)\n",
    "test_X_ori_denorm=inverse_scale(test_X_ori,scalers)\n",
    "\n",
    "if args.cuda:\n",
    "    torch.cuda.empty_cache() \n",
    "\n",
    "\n",
    "#Calculate metrics per-feature: mean absolute error on the ground truth (artificially-missing values)\n",
    "mae_per_feature=[]\n",
    "for i in range(n_features):\n",
    "    #Extract imputation and ground truth for feature i\n",
    "    imputation_i=test_imputation_denorm[:,:,i]\n",
    "    ground_truth_i=test_X_ori_denorm[:,:,i]\n",
    "    mask_i=test_mask[:,:,i]\n",
    "    # Check for NaN values\n",
    "    if np.isnan(imputation_i).any() or np.isnan(ground_truth_i).any():\n",
    "        print(f\"NaN values detected in feature {i}\")\n",
    "        continue  # Skip this feature if NaN values are found\n",
    "    #Filter only artificially masked positions\n",
    "    mae_i=calc_mae(imputation_i,ground_truth_i,mask_i)\n",
    "    mae_per_feature.append(mae_i)\n",
    "    print(f\"MAE for {feature_names[i]}: {mae_i:.4f}\")\n",
    "    mlflow.log_metric(f\"MAE_{feature_names[i]}\", mae_i)\n",
    "\n",
    "\n",
    "# calculate average MAE \n",
    "avg_mae=np.mean(mae_per_feature)\n",
    "print(f\"Testing mean absolute error: {avg_mae:.4f}\")\n",
    "mlflow.log_metric(\"avg_mae\", avg_mae)\n",
    "\n",
    "\n",
    "##Current error is on the cuda \n",
    "#test memory\n",
    "#def test_memory(in_size=100, out_size=10,hidden_size=100,optimizer_type=torch.optim.Adam, batch_size=1, device=0):\n",
    "   # sample_input=torch.randn(batch_size,in_size)\n",
    "  #  model=saits,\n",
    "   # optimizer=optimizer_type(model.parameters(),lr=1e-3)\n",
    "   # print(\"Beginning mem:\",torch.cuda.memory_allocated(device))\n",
    "   # model.to(device)\n",
    "   # print()\n",
    "   # for i in range(3):\n",
    "   #     print(\"Iteration\",i)\n",
    "   #     a=torch.cuda.memory_allocated(device)\n",
    "   #     out=model(sample_input.to(device)).sum()\n",
    "   #     b=torch.cuda.memory_allocated(device)\n",
    "   #     print(\"2-After forward pass\", torch.cuda.memory_allocated(device))\n",
    "   #     print(\"2-Memory consumed by forward pass\", b-a)\n",
    "   #     out.backward()\n",
    "   #     print(\"3-After backward pass\", torch.cuda.memory_allocated(device))\n",
    "   #     optimizer.step()\n",
    "   #     print(\"4-After optimizer step\", torch.cuda.memory_allocated(device))\n",
    "\n",
    "##To do: log mlflow, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc1d983-05d3-4f51-9b46-838fd092630b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column time has 0 NaN values\n",
      "Column time has 0.0 Missing_rate\n",
      "Column fr_eng has 0 NaN values\n",
      "Column fr_eng has 0.0 Missing_rate\n",
      "Column te_exh_cyl_out__0 has 73 NaN values\n",
      "Column te_exh_cyl_out__0 has 0.0006111906496203082 Missing_rate\n",
      "Column pd_air_ic__0 has 73 NaN values\n",
      "Column pd_air_ic__0 has 0.0006111906496203082 Missing_rate\n",
      "Column pr_exh_turb_out__0 has 119439 NaN values\n",
      "Column pr_exh_turb_out__0 has 1.0 Missing_rate\n",
      "Column te_air_ic_out__0 has 73 NaN values\n",
      "Column te_air_ic_out__0 has 0.0006111906496203082 Missing_rate\n",
      "Column te_seawater has 73 NaN values\n",
      "Column te_seawater has 0.0006111906496203082 Missing_rate\n",
      "Column te_air_comp_in_a__0 has 119439 NaN values\n",
      "Column te_air_comp_in_a__0 has 1.0 Missing_rate\n",
      "Column te_air_comp_in_b__0 has 119439 NaN values\n",
      "Column te_air_comp_in_b__0 has 1.0 Missing_rate\n",
      "Column fr_tc__0 has 119439 NaN values\n",
      "Column fr_tc__0 has 1.0 Missing_rate\n",
      "Column pr_baro has 73 NaN values\n",
      "Column pr_baro has 0.0006111906496203082 Missing_rate\n",
      "Column pd_air_ic__0_1 has 73 NaN values\n",
      "Column pd_air_ic__0_1 has 0.0006111906496203082 Missing_rate\n",
      "Column pr_exh_rec has 73 NaN values\n",
      "Column pr_exh_rec has 0.0006111906496203082 Missing_rate\n",
      "Column te_exh_turb_in__0 has 119439 NaN values\n",
      "Column te_exh_turb_in__0 has 1.0 Missing_rate\n",
      "Column te_exh_turb_out__0 has 119439 NaN values\n",
      "Column te_exh_turb_out__0 has 1.0 Missing_rate\n",
      "Column bo_aux_blower_running has 0 NaN values\n",
      "Column bo_aux_blower_running has 0.0 Missing_rate\n",
      "Column re_eng_load has 173 NaN values\n",
      "Column re_eng_load has 0.0014484381148536073 Missing_rate\n",
      "Column pr_air_scav_ecs has 0 NaN values\n",
      "Column pr_air_scav_ecs has 0.0 Missing_rate\n",
      "Column pr_air_scav has 0 NaN values\n",
      "Column pr_air_scav has 0.0 Missing_rate\n",
      "Column te_air_scav_rec has 73 NaN values\n",
      "Column te_air_scav_rec has 0.0006111906496203082 Missing_rate\n",
      "Column te_air_ic_out__0_1 has 73 NaN values\n",
      "Column te_air_ic_out__0_1 has 0.0006111906496203082 Missing_rate\n",
      "Column pr_cyl_comp__0 has 173 NaN values\n",
      "Column pr_cyl_comp__0 has 0.0014484381148536073 Missing_rate\n",
      "Column pr_cyl_max__0 has 173 NaN values\n",
      "Column pr_cyl_max__0 has 0.0014484381148536073 Missing_rate\n",
      "Column se_mip__0 has 173 NaN values\n",
      "Column se_mip__0 has 0.0014484381148536073 Missing_rate\n",
      "Column te_exh_cyl_out__0_1 has 73 NaN values\n",
      "Column te_exh_cyl_out__0_1 has 0.0006111906496203082 Missing_rate\n",
      "Column fr_eng_setpoint has 0 NaN values\n",
      "Column fr_eng_setpoint has 0.0 Missing_rate\n",
      "Column te_air_scav_rec_iso has 51235 NaN values\n",
      "Column te_air_scav_rec_iso has 0.4289637388122807 Missing_rate\n",
      "Column pr_cyl_max_mv_iso has 51385 NaN values\n",
      "Column pr_cyl_max_mv_iso has 0.4302196100101307 Missing_rate\n",
      "Column pr_cyl_comp_mv_iso has 51385 NaN values\n",
      "Column pr_cyl_comp_mv_iso has 0.4302196100101307 Missing_rate\n",
      "Column fr_eng_ecs has 0 NaN values\n",
      "Column fr_eng_ecs has 0.0 Missing_rate\n",
      "Column pr_air_scav_iso has 51385 NaN values\n",
      "Column pr_air_scav_iso has 0.4302196100101307 Missing_rate\n",
      "Column engine_type_G95ME-C10.5-GI-EGRTC has 0 NaN values\n",
      "Column engine_type_G95ME-C10.5-GI-EGRTC has 0.0 Missing_rate\n",
      "Original size:119439, Sampled size: 11944\n",
      "Reshaped data:(11944, 31)\n",
      "Using CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "2025/05/13 21:29:26 INFO mlflow.tracking.fluent: Experiment with name 'SAITS' does not exist. Creating a new experiment.\n",
      "2025-05-13 21:29:27 [INFO]: Using the given device: cpu\n",
      "2025-05-13 21:29:27 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/best_model/20250513_T212927\n",
      "2025-05-13 21:29:27 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/best_model/20250513_T212927/tensorboard\n",
      "2025-05-13 21:29:27 [INFO]: Using customized MAE as the training loss function.\n",
      "2025-05-13 21:29:27 [INFO]: Using customized MSE as the validation metric function.\n",
      "2025-05-13 21:29:27 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 6,402,077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸƒ View run SAITS at: http://localhost:5000/#/experiments/1/runs/dd2bf2404745444c8c0e8b3b9e4c73f7\n",
      "ğŸ§ª View experiment at: http://localhost:5000/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 21:29:29 [ERROR]: âŒ Exception: `predictions` mustn't contain NaN values, but detected NaN in it\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Training got interrupted. Model was not trained. Please investigate the error printed above.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/pypots/base.py:737\u001b[0m, in \u001b[0;36mBaseNNModel._train_model\u001b[0;34m(self, train_dataloader, val_dataloader)\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> 737\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(inputs, calc_criterion\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    738\u001b[0m loss \u001b[39m=\u001b[39m results[\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39msum()\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/pypots/imputation/saits/core.py:114\u001b[0m, in \u001b[0;36m_SAITS.forward\u001b[0;34m(self, inputs, calc_criterion, diagonal_attention_mask)\u001b[0m\n\u001b[1;32m    113\u001b[0m ORT_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 114\u001b[0m ORT_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_loss(X_tilde_1, X, missing_mask)\n\u001b[1;32m    115\u001b[0m ORT_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_loss(X_tilde_2, X, missing_mask)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/pypots/nn/modules/loss.py:81\u001b[0m, in \u001b[0;36mMAE.forward\u001b[0;34m(self, logits, targets, masks)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mforward\u001b[39m(\n\u001b[1;32m     76\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     77\u001b[0m     logits: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m     78\u001b[0m     targets: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m     79\u001b[0m     masks: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     80\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m---> 81\u001b[0m     value \u001b[39m=\u001b[39m calc_mae(logits, targets, masks)\n\u001b[1;32m     82\u001b[0m     \u001b[39mreturn\u001b[39;00m value\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/pypots/nn/functional/error.py:98\u001b[0m, in \u001b[0;36mcalc_mae\u001b[0;34m(predictions, targets, masks)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39m# check shapes and values of inputs\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m lib \u001b[39m=\u001b[39m _check_inputs(predictions, targets, masks)\n\u001b[1;32m    100\u001b[0m \u001b[39mif\u001b[39;00m masks \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/pypots/nn/functional/error.py:34\u001b[0m, in \u001b[0;36m_check_inputs\u001b[0;34m(predictions, targets, masks, check_shape)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39m# check NaN\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m lib\u001b[39m.\u001b[39misnan(predictions)\u001b[39m.\u001b[39many(), \u001b[39m\"\u001b[39m\u001b[39m`predictions` mustn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt contain NaN values, but detected NaN in it\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     35\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m lib\u001b[39m.\u001b[39misnan(targets)\u001b[39m.\u001b[39many(), \u001b[39m\"\u001b[39m\u001b[39m`targets` mustn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt contain NaN values, but detected NaN in it\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: `predictions` mustn't contain NaN values, but detected NaN in it",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/Pypots_model_lighter.py:284\u001b[0m\n\u001b[1;32m    279\u001b[0m mlflow\u001b[39m.\u001b[39mlog_param(\u001b[39m\"\u001b[39m\u001b[39mpatience\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m4\u001b[39m)\n\u001b[1;32m    282\u001b[0m \u001b[39m# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[39m#use original missigness for trainig, use the mcar masked points as ground truth\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m saits\u001b[39m.\u001b[39;49mfit(train_set\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m: train_data}, val_set\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m: val_X_masked , \u001b[39m\"\u001b[39;49m\u001b[39mX_ori\u001b[39;49m\u001b[39m\"\u001b[39;49m:val_X_ori })\n\u001b[1;32m    287\u001b[0m \u001b[39m##drop null values in test set \u001b[39;00m\n\u001b[1;32m    288\u001b[0m \n\u001b[1;32m    289\u001b[0m \u001b[39m# Check for NaN values across the entire array\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[39m# the testing stage, impute the originally-missing values and artificially-missing values in the test set\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[39m# Convert the numpy array to a dictionary\u001b[39;00m\n\u001b[1;32m    299\u001b[0m test_set_dict \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m: test_X_masked}\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/pypots/imputation/saits/model.py:275\u001b[0m, in \u001b[0;36mSAITS.fit\u001b[0;34m(self, train_set, val_set, file_type)\u001b[0m\n\u001b[1;32m    267\u001b[0m     val_dataloader \u001b[39m=\u001b[39m DataLoader(\n\u001b[1;32m    268\u001b[0m         val_dataset,\n\u001b[1;32m    269\u001b[0m         batch_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size,\n\u001b[1;32m    270\u001b[0m         shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    271\u001b[0m         num_workers\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_workers,\n\u001b[1;32m    272\u001b[0m     )\n\u001b[1;32m    274\u001b[0m \u001b[39m# Step 2: train the model and freeze it\u001b[39;00m\n\u001b[0;32m--> 275\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_model(train_dataloader, val_dataloader)\n\u001b[1;32m    276\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mload_state_dict(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_model_dict)\n\u001b[1;32m    278\u001b[0m \u001b[39m# Step 3: save the model if necessary\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/pypots/base.py:814\u001b[0m, in \u001b[0;36mBaseNNModel._train_model\u001b[0;34m(self, train_dataloader, val_dataloader)\u001b[0m\n\u001b[1;32m    812\u001b[0m logger\u001b[39m.\u001b[39merror(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mâŒ Exception: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_model_dict \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# if no best model, raise error\u001b[39;00m\n\u001b[0;32m--> 814\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    815\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTraining got interrupted. Model was not trained. Please investigate the error printed above.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    816\u001b[0m     )\n\u001b[1;32m    817\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    818\u001b[0m     \u001b[39mRuntimeWarning\u001b[39;00m(\n\u001b[1;32m    819\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTraining got interrupted. Please investigate the error printed above.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    820\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mModel got trained and will load the best checkpoint so far for testing.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    821\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf you don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt want it, please try fit() again.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    822\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Training got interrupted. Model was not trained. Please investigate the error printed above."
     ]
    }
   ],
   "source": [
    "#Import Pypots Library\n",
    "from pypots.optim import Adam\n",
    "from pypots.imputation import SAITS\n",
    "#from pypots.utils.metrics import calc_mae\n",
    "from pypots.nn.functional import calc_mae\n",
    "\n",
    "\n",
    "import argparse\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import data_insight\n",
    "from data_insight import setup_duckdb\n",
    "from duckdb import DuckDBPyConnection as DuckDB\n",
    "from duckdb import DuckDBPyRelation as Relation\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "from duckdb import DuckDBPyConnection as DuckDB\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import optuna \n",
    "from optuna.visualization import plot_optimization_history\n",
    "\n",
    "\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, Dataset\n",
    "from pygrinder.missing_completely_at_random import mcar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import sensor_imputation_thesis.shared.load_data as load\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "#PatchTST might be an ideal choise if SAITS is too slow \n",
    "\n",
    "##Drop columns with different indexes while loading data.. Or the mean values \n",
    "\n",
    "df=pd.read_parquet(\"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/Newdataframeforpypots.parquet\")\n",
    "\n",
    "len(df)\n",
    "\n",
    "#current length of the dataframe is 119439\n",
    "\n",
    "# Check nan values in each column\n",
    "for col in df.columns:\n",
    "    print(f\"Column {col} has {df[col].isna().sum()} NaN values\")\n",
    "    missing_rate=df[col].isna().sum()/len(df[col])\n",
    "    print(f\"Column {col} has {missing_rate} Missing_rate\")\n",
    "\n",
    "\n",
    "#Try with smaller dataset, size 4000\n",
    "##SAMPLE the percengtage of the dataset, df.sample (averagely pick samples)\n",
    "#not df.sample cuz it will randomly select \n",
    "original_size=len(df)\n",
    "desired_fraction=0.10\n",
    "step=int(1/desired_fraction) #step_size=10 (sample every 10th minute)\n",
    "\n",
    "#Systematic sampling: Start at a random offset to avoid bias \n",
    "start=np.random.randint(0,step) #Random start between 0-9\n",
    "df1=df.iloc[start::step].reset_index(drop=True)\n",
    "\n",
    "print(f\"Original size:{len(df)}, Sampled size: {len(df1)}\")\n",
    "\n",
    "\n",
    "#df1=df[:10000]\n",
    "\n",
    "\n",
    "# Custom Dataset class\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Data processing code\n",
    "sensor_cols = [col for col in df1.columns if col != \"time\"]\n",
    "data = df1[sensor_cols].values\n",
    "\n",
    "#Â¤get feature names for printing mae later \n",
    "feature_names=df1[sensor_cols].columns.tolist()\n",
    "\n",
    "## Convert data to 3D arrays of shape n_samples, n_timesteps, n_features, X_ori refers to the original data without missing values \n",
    "## Reconstruct all columns simultaneously  #num_features: 119\n",
    "n_features = data.shape[1]  # exclude the time column\n",
    "n_steps = 1 #60 (was 60 previously) #(TRY TO CHANGE HERE)  # # window length, 1440 steps = 24 hours of 1-minute data, but here is revised to 60 again\n",
    "#total_elements = data.shape[0] * data.shape[1]\n",
    "n_samples = data.shape[0] // n_steps \n",
    "\n",
    "\n",
    "\n",
    "# Reshape to (n_samples // n_steps, n_steps, n_features)\n",
    "#data_reshaped = data.reshape((n_samples, n_steps, n_features))\n",
    "data_reshaped=data[:n_samples*n_steps].reshape(n_samples,n_steps,n_features)\n",
    "print(f\"Reshaped data:{data.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "##Normalization is important because of the nature of mse calculation of saits, columns with large \n",
    "#values dominate the loss, making metrics meaningless. SAITS computes MSE/MAE column-wise and averages \n",
    "#them across all columns \n",
    "#  Apply minmax scaler here \n",
    "#normalize each feature independently\n",
    "scalers={}\n",
    "data_normalized=data.copy()\n",
    "\n",
    "data_normalized = np.zeros_like(data_reshaped)  # Initialize the normalized data array\n",
    "scalers = {}\n",
    "\n",
    "for i in range(data_reshaped.shape[2]):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    # Flatten timesteps and samples for scaling\n",
    "    data_normalized[:, :, i] = scaler.fit_transform(data_reshaped[:, :, i].reshape(-1, 1)).reshape(data_reshaped.shape[0], data_reshaped.shape[1])\n",
    "    scalers[i] = scaler  # Save scalers to inverse-transform later\n",
    "\n",
    "\n",
    "\n",
    "#Split into train, test, val\n",
    "\n",
    "train_size = int(0.6 * len(data_normalized))\n",
    "val_size = int(0.2 * len(data_normalized))\n",
    "test_size = len(data_normalized) - train_size - val_size\n",
    "\n",
    "train_data = data_normalized[:train_size]\n",
    "val_data = data_normalized[train_size:train_size + val_size]\n",
    "test_data= data_normalized[train_size + val_size:]\n",
    "\n",
    "#Optional: Artificially mask. Mask 20% of the data (MIT part)\n",
    "def mcar_f(X, mask_ratio=0.2):\n",
    "    \"\"\"Apply MCAR only to observed values.\"\"\"\n",
    "    observed_mask=~np.isnan(X) #find observed positions\n",
    "    artificial_mask=mcar(X,mask_ratio).astype(bool) #generate MCAR mask, cast to boolean\n",
    "    #combine masks \n",
    "    combined_mask=observed_mask & artificial_mask\n",
    "\n",
    "    #Apply masking\n",
    "    X_masked=X.copy()\n",
    "    X_masked[combined_mask]=np.nan\n",
    "    return X_masked,combined_mask\n",
    "\n",
    "\n",
    "#Use mcar on validation data \n",
    "val_X_masked, val_mask =mcar_f(val_data)\n",
    "val_X_ori=val_data.copy() #Ground truth\n",
    "\n",
    "test_X_masked, test_mask =mcar_f(test_data)\n",
    "test_X_ori=test_data.copy() #ground truth\n",
    "\n",
    "#?? Problem: Can't have the best input for testing\n",
    "#1.Create synthetic test_data cuz if I drop nan values for test set, there's basically nothing left\n",
    "#synthetic_data=np.random.randn(n_samples,n_steps,n_features)\n",
    "#test_X_masked,test_mask=mcar_f(synthetic_data)\n",
    "#test_X_ori=synthetic_data.copy() #Ground truth\n",
    "\n",
    "# 2, Ensure no NaN values in synthetic data\n",
    "#test_X_masked = np.nan_to_num(test_X_masked, nan=np.nanmean(test_X_masked))\n",
    "#test_X_ori = np.nan_to_num(test_X_ori, nan=np.nanmean(test_X_ori))\n",
    "\n",
    "\n",
    "#3. Set aside a part of dataset for test_set\n",
    "#df_test=df1[6000:8000]  #Even with 10000 rows of dataset fot testing, after dropping nan value in rows, there is none left \n",
    "#2. Drop nan rows \n",
    "#f_test=df_test.dropna(axis=0,how='all')\n",
    "#print(\"Size of the test set:\", len(df_test)) \n",
    "#data_test=df_test[sensor_cols].values\n",
    "\n",
    "\n",
    "\n",
    "#n_features = data_test.shape[1]  # exclude the time column\n",
    "#n_steps = 60 #(TRY TO CHANGE HERE)  # # window length, 1440 steps = 24 hours of 1-minute data, but here is revised to 60 again\n",
    "#total_elements = data.shape[0] * data.shape[1]\n",
    "#n_samples = data_test.shape[0] // n_steps \n",
    "#Reshape and apply mcar to the test set \n",
    "# #data_test=data_test[:n_samples*n_steps].reshape(n_samples,n_steps,n_features)\n",
    "\n",
    "#data_test_normalized=np.zeros_like(data_test)  # Initialize the normalized data array\n",
    "#for i in range(data_test.shape[2]):\n",
    "  # scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    # Flatten timesteps and samples for scaling\n",
    "  # data_test_normalized[:, :, i] = scaler.fit_transform(data_test[:, :, i].reshape(-1, 1)).reshape(data_test.shape[0], data_test.shape[1])\n",
    "  # scalers[i] = scaler  # Save scalers to inverse-transform later\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Config:\n",
    "    no_cuda = False\n",
    "    no_mps = False\n",
    "    seed = 1\n",
    "\n",
    "args=Config()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "use_mps = not args.no_mps and torch.backends.mps.is_available()\n",
    "\n",
    "if args.cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA\")\n",
    "elif use_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "\n",
    "#MLflow set up\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "mlflow.set_experiment(\"SAITS\")\n",
    "SAITS_run_name = \"SAITS\"\n",
    "\n",
    "with mlflow.start_run(run_name=SAITS_run_name) as run:\n",
    "# initialize the model (from example)\n",
    "    saits = SAITS(\n",
    "        n_steps=data_normalized.shape[1],\n",
    "        n_features=data_normalized.shape[2],\n",
    "        n_layers=2, #deep network(4) for long sequences, here is revised for 2 since it always shows cuda error \n",
    "        d_model=512,  #d_modle must equal n_heads * d_k\n",
    "        d_ffn=512,\n",
    "        n_heads=8,\n",
    "        d_k=64,\n",
    "        d_v=64,\n",
    "        dropout=0.1,\n",
    "        attn_dropout=0.1,\n",
    "        diagonal_attention_mask=True,  # otherwise the original self-attention mechanism will be applied\n",
    "        ORT_weight=1,  # you can adjust the weight values of arguments ORT_weight\n",
    "       # and MIT_weight to make the SAITS model focus more on one task. Usually you can just leave them to the default values, i.e. 1.\n",
    "        MIT_weight=1,\n",
    "        batch_size=5, #try with 5 to see if it works, was 4. \n",
    "        # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "        epochs=10, #try with 10 to see if it runs\n",
    "        # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "        # You can leave it to defualt as None to disable early stopping.\n",
    "        patience=4, #initially was 3, tried to increase to see more possibilities \n",
    "        # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "        # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "        optimizer=Adam(lr=1e-3),\n",
    "        # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "        # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "        # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "        num_workers=0,\n",
    "        # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "        # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "        device=device, \n",
    "        # set the path for saving tensorboard and trained model files \n",
    "        saving_path=\"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/best_model\",\n",
    "        # only save the best model after training finished.\n",
    "        # You can also set it as \"better\" to save models performing better ever during training.\n",
    "        model_saving_strategy=\"best\",\n",
    ")\n",
    "\n",
    "\n",
    "# Log model parameters\n",
    "mlflow.log_param(\"n_steps\", data_normalized.shape[1])\n",
    "mlflow.log_param(\"n_features\", data_normalized.shape[2])\n",
    "mlflow.log_param(\"batch_size\", 5)\n",
    "mlflow.log_param(\"epochs\", 10)\n",
    "mlflow.log_param(\"patience\", 4)\n",
    "\n",
    "\n",
    "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
    "#use original missigness for trainig, use the mcar masked points as ground truth\n",
    "saits.fit(train_set={\"X\": train_data}, val_set={\"X\": val_X_masked , \"X_ori\":val_X_ori })\n",
    "\n",
    "\n",
    "##drop null values in test set \n",
    "\n",
    "# Check for NaN values across the entire array\n",
    "#nan_mask = np.isnan(test_X).any(axis=(1, 2))\n",
    "\n",
    "# Filter out samples (rows) that contain NaN values\n",
    "#test_X_clean = test_X[~nan_mask]\n",
    "#test_X_ori = test_X_clean\n",
    "#test_X_masked, test_mask = mcar_f(test_X_clean)\n",
    "\n",
    "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
    "# Convert the numpy array to a dictionary\n",
    "test_set_dict = {\"X\": test_X_masked}\n",
    "test_imputation = saits.predict(test_set_dict)\n",
    "\n",
    "#change back to array for applying inverse scale later\n",
    "test_imputation_array = test_imputation[\"imputation\"]\n",
    "\n",
    "\n",
    "#clear cuda after prediction if on cuda\n",
    "\n",
    "# Ensure saits_imputation and test_X are numpy arrays\n",
    "#if not isinstance(saits_imputation, np.ndarray):\n",
    "    #saits_imputation = np.array(saits_imputation)\n",
    "#if not isinstance(test_X, np.ndarray):\n",
    "     #test_X = np.array(test_X)\n",
    "\n",
    "#Inverse Scale\n",
    "def inverse_scale(imputation, scalers):\n",
    "    n_features = imputation.shape[2]\n",
    "    imputation_denorm = np.empty_like(imputation)\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        imputation_denorm[:, :, i] = scalers[i].inverse_transform(imputation[:, :, i].reshape(-1, 1)).reshape(imputation.shape[0], imputation.shape[1])\n",
    "    \n",
    "    return imputation_denorm  # Move the return statement outside the loop\n",
    "\n",
    "    \n",
    "#Apply function to the dataset \n",
    "test_imputation_denorm=inverse_scale(test_imputation_array,scalers)\n",
    "test_X_ori_denorm=inverse_scale(test_X_ori,scalers)\n",
    "\n",
    "if args.cuda:\n",
    "    torch.cuda.empty_cache() \n",
    "\n",
    "\n",
    "#Calculate metrics per-feature: mean absolute error on the ground truth (artificially-missing values)\n",
    "mae_per_feature=[]\n",
    "for i in range(n_features):\n",
    "    #Extract imputation and ground truth for feature i\n",
    "    imputation_i=test_imputation_denorm[:,:,i]\n",
    "    ground_truth_i=test_X_ori_denorm[:,:,i]\n",
    "    mask_i=test_mask[:,:,i]\n",
    "    # Check for NaN values\n",
    "    if np.isnan(imputation_i).any() or np.isnan(ground_truth_i).any():\n",
    "        print(f\"NaN values detected in feature {i}\")\n",
    "        continue  # Skip this feature if NaN values are found\n",
    "    #Filter only artificially masked positions\n",
    "    mae_i=calc_mae(imputation_i,ground_truth_i,mask_i)\n",
    "    mae_per_feature.append(mae_i)\n",
    "    print(f\"MAE for {feature_names[i]}: {mae_i:.4f}\")\n",
    "    mlflow.log_metric(f\"MAE_{feature_names[i]}\", mae_i)\n",
    "\n",
    "\n",
    "# calculate average MAE \n",
    "avg_mae=np.mean(mae_per_feature)\n",
    "print(f\"Testing mean absolute error: {avg_mae:.4f}\")\n",
    "mlflow.log_metric(\"avg_mae\", avg_mae)\n",
    "\n",
    "\n",
    "##Current error is on the cuda \n",
    "#test memory\n",
    "#def test_memory(in_size=100, out_size=10,hidden_size=100,optimizer_type=torch.optim.Adam, batch_size=1, device=0):\n",
    "   # sample_input=torch.randn(batch_size,in_size)\n",
    "  #  model=saits,\n",
    "   # optimizer=optimizer_type(model.parameters(),lr=1e-3)\n",
    "   # print(\"Beginning mem:\",torch.cuda.memory_allocated(device))\n",
    "   # model.to(device)\n",
    "   # print()\n",
    "   # for i in range(3):\n",
    "   #     print(\"Iteration\",i)\n",
    "   #     a=torch.cuda.memory_allocated(device)\n",
    "   #     out=model(sample_input.to(device)).sum()\n",
    "   #     b=torch.cuda.memory_allocated(device)\n",
    "   #     print(\"2-After forward pass\", torch.cuda.memory_allocated(device))\n",
    "   #     print(\"2-Memory consumed by forward pass\", b-a)\n",
    "   #     out.backward()\n",
    "   #     print(\"3-After backward pass\", torch.cuda.memory_allocated(device))\n",
    "   #     optimizer.step()\n",
    "   #     print(\"4-After optimizer step\", torch.cuda.memory_allocated(device))\n",
    "\n",
    "##To do: log mlflow, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid non-printable character U+00A0 (<ipython-input-4-491befc0bc6f>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    raise ValueError(\"NaN values detected in training data\")\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid non-printable character U+00A0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if np.isnan(train_data).any():\n",
    "Â Â Â  raise ValueError(\"NaN values detected in training data\")\n",
    "\n",
    "# Check for NaN values in validation data\n",
    "if np.isnan(val_X_masked).any() or np.isnan(val_X_ori).any():\n",
    "Â Â Â  raise ValueError(\"NaN values detected in validation data\")\n",
    "\n",
    "# Check for NaN values in test data\n",
    "if np.isnan(test_X_masked).any() or np.isnan(test_X_ori).any():\n",
    "Â Â Â  raise ValueError(\"NaN values detected in test data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid non-printable character U+00A0 (<ipython-input-5-975a705d4f20>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    raise ValueError(\"NaN values detected in training data\")\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid non-printable character U+00A0\n"
     ]
    }
   ],
   "source": [
    "if np.isnan(train_data).any():\n",
    "Â Â Â  raise ValueError(\"NaN values detected in training data\")\n",
    "\n",
    "# Check for NaN values in validation data\n",
    "if np.isnan(val_X_masked).any() or np.isnan(val_X_ori).any():\n",
    "Â Â Â  raise ValueError(\"NaN values detected in validation data\")\n",
    "\n",
    "# Check for NaN values in test data\n",
    "if np.isnan(test_X_masked).any() or np.isnan(test_X_ori).any():\n",
    "Â Â Â  raise ValueError(\"NaN values detected in test data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid non-printable character U+00A0 (<ipython-input-6-491befc0bc6f>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    raise ValueError(\"NaN values detected in training data\")\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid non-printable character U+00A0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if np.isnan(train_data).any():\n",
    "Â Â Â  raise ValueError(\"NaN values detected in training data\")\n",
    "\n",
    "# Check for NaN values in validation data\n",
    "if np.isnan(val_X_masked).any() or np.isnan(val_X_ori).any():\n",
    "Â Â Â  raise ValueError(\"NaN values detected in validation data\")\n",
    "\n",
    "# Check for NaN values in test data\n",
    "if np.isnan(test_X_masked).any() or np.isnan(test_X_ori).any():\n",
    "Â Â Â  raise ValueError(\"NaN values detected in test data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(train_data).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     time    fr_eng  te_exh_cyl_out__0  pd_air_ic__0  \\\n",
      "0     2023-10-01 00:05:00  0.734716             510.15         450.0   \n",
      "1     2023-10-01 00:15:00  0.733886             510.15         430.0   \n",
      "2     2023-10-01 00:25:00  0.733876             511.15         460.0   \n",
      "3     2023-10-01 00:35:00  0.732363             511.15         430.0   \n",
      "4     2023-10-01 00:45:00  0.734891             511.15         480.0   \n",
      "...                   ...       ...                ...           ...   \n",
      "11939 2024-01-31 05:46:00  0.536374             412.15         450.0   \n",
      "11940 2024-01-31 05:56:00  0.529955             455.15         460.0   \n",
      "11941 2024-01-31 06:06:00  0.348162             417.15         300.0   \n",
      "11942 2024-01-31 06:16:00  0.350018             406.15         280.0   \n",
      "11943 2024-01-31 06:26:00  0.349860             400.15         270.0   \n",
      "\n",
      "       pr_exh_turb_out__0  te_air_ic_out__0  te_seawater  te_air_comp_in_a__0  \\\n",
      "0                     NaN            308.65       273.15                  NaN   \n",
      "1                     NaN            308.65       273.15                  NaN   \n",
      "2                     NaN            308.65       273.15                  NaN   \n",
      "3                     NaN            308.65       273.15                  NaN   \n",
      "4                     NaN            308.65       273.15                  NaN   \n",
      "...                   ...               ...          ...                  ...   \n",
      "11939                 NaN            301.35       273.15                  NaN   \n",
      "11940                 NaN            302.55       273.15                  NaN   \n",
      "11941                 NaN            302.55       273.15                  NaN   \n",
      "11942                 NaN            302.15       273.15                  NaN   \n",
      "11943                 NaN            301.35       273.15                  NaN   \n",
      "\n",
      "       te_air_comp_in_b__0  fr_tc__0  ...  pr_cyl_max__0      se_mip__0  \\\n",
      "0                      NaN       NaN  ...   9.309164e+06  563658.046722   \n",
      "1                      NaN       NaN  ...   9.450893e+06  579276.371002   \n",
      "2                      NaN       NaN  ...   9.396848e+06  570795.631409   \n",
      "3                      NaN       NaN  ...   9.456684e+06  576502.227783   \n",
      "4                      NaN       NaN  ...   9.739590e+06  617160.320282   \n",
      "...                    ...       ...  ...            ...            ...   \n",
      "11939                  NaN       NaN  ...   8.184983e+06  507889.509201   \n",
      "11940                  NaN       NaN  ...   8.336363e+06  440497.159958   \n",
      "11941                  NaN       NaN  ...   7.230381e+06  224525.761604   \n",
      "11942                  NaN       NaN  ...   7.244995e+06  227659.106255   \n",
      "11943                  NaN       NaN  ...   7.226797e+06  226403.594017   \n",
      "\n",
      "       te_exh_cyl_out__0_1  fr_eng_setpoint  te_air_scav_rec_iso  \\\n",
      "0                   510.15         0.733310                  NaN   \n",
      "1                   510.15         0.733310                  NaN   \n",
      "2                   511.15         0.733310                  NaN   \n",
      "3                   511.15         0.733310                  NaN   \n",
      "4                   511.15         0.733310                  NaN   \n",
      "...                    ...              ...                  ...   \n",
      "11939               412.15         0.533316           307.137445   \n",
      "11940               455.15         0.533316           307.156283   \n",
      "11941               417.15         0.349989           307.164837   \n",
      "11942               406.15         0.349989           307.164837   \n",
      "11943               400.15         0.349989           307.164837   \n",
      "\n",
      "       pr_cyl_max_mv_iso  pr_cyl_comp_mv_iso  fr_eng_ecs  pr_air_scav_iso  \\\n",
      "0                    NaN                 NaN    0.734716              NaN   \n",
      "1                    NaN                 NaN    0.733886              NaN   \n",
      "2                    NaN                 NaN    0.733876              NaN   \n",
      "3                    NaN                 NaN    0.732363              NaN   \n",
      "4                    NaN                 NaN    0.734891              NaN   \n",
      "...                  ...                 ...         ...              ...   \n",
      "11939       8.054274e+06        5.711586e+06    0.536374     13251.732170   \n",
      "11940       8.117281e+06        5.818509e+06    0.529955     15660.573974   \n",
      "11941       7.098394e+06        5.457687e+06    0.348162      5219.571966   \n",
      "11942       7.089159e+06        5.445148e+06    0.350018      5045.522236   \n",
      "11943       7.059574e+06        5.435775e+06    0.349860      4531.211502   \n",
      "\n",
      "       engine_type_G95ME-C10.5-GI-EGRTC  \n",
      "0                                   1.0  \n",
      "1                                   1.0  \n",
      "2                                   1.0  \n",
      "3                                   1.0  \n",
      "4                                   1.0  \n",
      "...                                 ...  \n",
      "11939                               1.0  \n",
      "11940                               1.0  \n",
      "11941                               1.0  \n",
      "11942                               1.0  \n",
      "11943                               1.0  \n",
      "\n",
      "[11944 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d7ff71-19e6-4def-8469-02a15dc6c7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column time has 0 NaN values\n",
      "Column time has 0.0 Missing_rate\n",
      "Column fr_eng has 0 NaN values\n",
      "Column fr_eng has 0.0 Missing_rate\n",
      "Column te_exh_cyl_out__0 has 73 NaN values\n",
      "Column te_exh_cyl_out__0 has 0.0006111906496203082 Missing_rate\n",
      "Column pd_air_ic__0 has 73 NaN values\n",
      "Column pd_air_ic__0 has 0.0006111906496203082 Missing_rate\n",
      "Column pr_exh_turb_out__0 has 119439 NaN values\n",
      "Column pr_exh_turb_out__0 has 1.0 Missing_rate\n",
      "Column te_air_ic_out__0 has 73 NaN values\n",
      "Column te_air_ic_out__0 has 0.0006111906496203082 Missing_rate\n",
      "Column te_seawater has 73 NaN values\n",
      "Column te_seawater has 0.0006111906496203082 Missing_rate\n",
      "Column te_air_comp_in_a__0 has 119439 NaN values\n",
      "Column te_air_comp_in_a__0 has 1.0 Missing_rate\n",
      "Column te_air_comp_in_b__0 has 119439 NaN values\n",
      "Column te_air_comp_in_b__0 has 1.0 Missing_rate\n",
      "Column fr_tc__0 has 119439 NaN values\n",
      "Column fr_tc__0 has 1.0 Missing_rate\n",
      "Column pr_baro has 73 NaN values\n",
      "Column pr_baro has 0.0006111906496203082 Missing_rate\n",
      "Column pd_air_ic__0_1 has 73 NaN values\n",
      "Column pd_air_ic__0_1 has 0.0006111906496203082 Missing_rate\n",
      "Column pr_exh_rec has 73 NaN values\n",
      "Column pr_exh_rec has 0.0006111906496203082 Missing_rate\n",
      "Column te_exh_turb_in__0 has 119439 NaN values\n",
      "Column te_exh_turb_in__0 has 1.0 Missing_rate\n",
      "Column te_exh_turb_out__0 has 119439 NaN values\n",
      "Column te_exh_turb_out__0 has 1.0 Missing_rate\n",
      "Column bo_aux_blower_running has 0 NaN values\n",
      "Column bo_aux_blower_running has 0.0 Missing_rate\n",
      "Column re_eng_load has 173 NaN values\n",
      "Column re_eng_load has 0.0014484381148536073 Missing_rate\n",
      "Column pr_air_scav_ecs has 0 NaN values\n",
      "Column pr_air_scav_ecs has 0.0 Missing_rate\n",
      "Column pr_air_scav has 0 NaN values\n",
      "Column pr_air_scav has 0.0 Missing_rate\n",
      "Column te_air_scav_rec has 73 NaN values\n",
      "Column te_air_scav_rec has 0.0006111906496203082 Missing_rate\n",
      "Column te_air_ic_out__0_1 has 73 NaN values\n",
      "Column te_air_ic_out__0_1 has 0.0006111906496203082 Missing_rate\n",
      "Column pr_cyl_comp__0 has 173 NaN values\n",
      "Column pr_cyl_comp__0 has 0.0014484381148536073 Missing_rate\n",
      "Column pr_cyl_max__0 has 173 NaN values\n",
      "Column pr_cyl_max__0 has 0.0014484381148536073 Missing_rate\n",
      "Column se_mip__0 has 173 NaN values\n",
      "Column se_mip__0 has 0.0014484381148536073 Missing_rate\n",
      "Column te_exh_cyl_out__0_1 has 73 NaN values\n",
      "Column te_exh_cyl_out__0_1 has 0.0006111906496203082 Missing_rate\n",
      "Column fr_eng_setpoint has 0 NaN values\n",
      "Column fr_eng_setpoint has 0.0 Missing_rate\n",
      "Column te_air_scav_rec_iso has 51235 NaN values\n",
      "Column te_air_scav_rec_iso has 0.4289637388122807 Missing_rate\n",
      "Column pr_cyl_max_mv_iso has 51385 NaN values\n",
      "Column pr_cyl_max_mv_iso has 0.4302196100101307 Missing_rate\n",
      "Column pr_cyl_comp_mv_iso has 51385 NaN values\n",
      "Column pr_cyl_comp_mv_iso has 0.4302196100101307 Missing_rate\n",
      "Column fr_eng_ecs has 0 NaN values\n",
      "Column fr_eng_ecs has 0.0 Missing_rate\n",
      "Column pr_air_scav_iso has 51385 NaN values\n",
      "Column pr_air_scav_iso has 0.4302196100101307 Missing_rate\n",
      "Column engine_type_G95ME-C10.5-GI-EGRTC has 0 NaN values\n",
      "Column engine_type_G95ME-C10.5-GI-EGRTC has 0.0 Missing_rate\n",
      "Original size:119439, Sampled size: 11944\n",
      "Reshaped data:(11944, 31)\n",
      "Using CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Run with UUID a5285036106e4611b162d9ad8f532b5a is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/Pypots_model_lighter.py:233\u001b[0m\n\u001b[1;32m    230\u001b[0m mlflow\u001b[39m.\u001b[39mset_experiment(\u001b[39m\"\u001b[39m\u001b[39mSAITS\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    231\u001b[0m SAITS_run_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mSAITS\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 233\u001b[0m \u001b[39mwith\u001b[39;00m mlflow\u001b[39m.\u001b[39;49mstart_run(run_name\u001b[39m=\u001b[39;49mSAITS_run_name) \u001b[39mas\u001b[39;00m run:\n\u001b[1;32m    234\u001b[0m \u001b[39m# initialize the model (from example)\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     saits \u001b[39m=\u001b[39m SAITS(\n\u001b[1;32m    236\u001b[0m         n_steps\u001b[39m=\u001b[39mdata_normalized\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m],\n\u001b[1;32m    237\u001b[0m         n_features\u001b[39m=\u001b[39mdata_normalized\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    270\u001b[0m         model_saving_strategy\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbest\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    271\u001b[0m )\n\u001b[1;32m    274\u001b[0m \u001b[39m# Log model parameters\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/tracking/fluent.py:351\u001b[0m, in \u001b[0;36mstart_run\u001b[0;34m(run_id, experiment_id, run_name, nested, parent_run_id, tags, description, log_system_metrics)\u001b[0m\n\u001b[1;32m    349\u001b[0m experiment_id \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(experiment_id) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(experiment_id, \u001b[39mint\u001b[39m) \u001b[39melse\u001b[39;00m experiment_id\n\u001b[1;32m    350\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(active_run_stack) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m nested:\n\u001b[0;32m--> 351\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\n\u001b[1;32m    352\u001b[0m         (\n\u001b[1;32m    353\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mRun with UUID \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m is already active. To start a new run, first end the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    354\u001b[0m             \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcurrent run with mlflow.end_run(). To start a nested \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    355\u001b[0m             \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrun, call start_run with nested=True\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    356\u001b[0m         )\u001b[39m.\u001b[39mformat(active_run_stack[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mrun_id)\n\u001b[1;32m    357\u001b[0m     )\n\u001b[1;32m    358\u001b[0m client \u001b[39m=\u001b[39m MlflowClient()\n\u001b[1;32m    359\u001b[0m \u001b[39mif\u001b[39;00m run_id:\n",
      "\u001b[0;31mException\u001b[0m: Run with UUID a5285036106e4611b162d9ad8f532b5a is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True"
     ]
    }
   ],
   "source": [
    "#Import Pypots Library\n",
    "from pypots.optim import Adam\n",
    "from pypots.imputation import SAITS\n",
    "#from pypots.utils.metrics import calc_mae\n",
    "from pypots.nn.functional import calc_mae\n",
    "\n",
    "\n",
    "import argparse\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import data_insight\n",
    "from data_insight import setup_duckdb\n",
    "from duckdb import DuckDBPyConnection as DuckDB\n",
    "from duckdb import DuckDBPyRelation as Relation\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "from duckdb import DuckDBPyConnection as DuckDB\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import optuna \n",
    "from optuna.visualization import plot_optimization_history\n",
    "\n",
    "\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, Dataset\n",
    "from pygrinder.missing_completely_at_random import mcar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import sensor_imputation_thesis.shared.load_data as load\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "#PatchTST might be an ideal choise if SAITS is too slow \n",
    "\n",
    "##Drop columns with different indexes while loading data.. Or the mean values \n",
    "\n",
    "df=pd.read_parquet(\"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/Newdataframeforpypots.parquet\")\n",
    "\n",
    "len(df)\n",
    "\n",
    "#current length of the dataframe is 119439\n",
    "\n",
    "# Check nan values in each column\n",
    "for col in df.columns:\n",
    "    print(f\"Column {col} has {df[col].isna().sum()} NaN values\")\n",
    "    missing_rate=df[col].isna().sum()/len(df[col])\n",
    "    print(f\"Column {col} has {missing_rate} Missing_rate\")\n",
    "\n",
    "\n",
    "#Try with smaller dataset, size 4000\n",
    "##SAMPLE the percengtage of the dataset, df.sample (averagely pick samples)\n",
    "#not df.sample cuz it will randomly select \n",
    "original_size=len(df)\n",
    "desired_fraction=0.10\n",
    "step=int(1/desired_fraction) #step_size=10 (sample every 10th minute)\n",
    "\n",
    "#Systematic sampling: Start at a random offset to avoid bias \n",
    "start=np.random.randint(0,step) #Random start between 0-9\n",
    "df1=df.iloc[start::step].reset_index(drop=True)\n",
    "\n",
    "print(f\"Original size:{len(df)}, Sampled size: {len(df1)}\")\n",
    "\n",
    "\n",
    "#df1=df[:10000]\n",
    "\n",
    "\n",
    "# Custom Dataset class\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Data processing code\n",
    "sensor_cols = [col for col in df1.columns if col != \"time\"]\n",
    "data = df1[sensor_cols].values\n",
    "\n",
    "#Â¤get feature names for printing mae later \n",
    "feature_names=df1[sensor_cols].columns.tolist()\n",
    "\n",
    "## Convert data to 3D arrays of shape n_samples, n_timesteps, n_features, X_ori refers to the original data without missing values \n",
    "## Reconstruct all columns simultaneously  #num_features: 119\n",
    "n_features = data.shape[1]  # exclude the time column\n",
    "n_steps = 30 #60 (was 60 previously) #(TRY TO CHANGE HERE)  # # window length, 1440 steps = 24 hours of 1-minute data, but here is revised to 60 again\n",
    "#total_elements = data.shape[0] * data.shape[1]\n",
    "n_samples = data.shape[0] // n_steps \n",
    "\n",
    "\n",
    "\n",
    "# Reshape to (n_samples // n_steps, n_steps, n_features)\n",
    "#data_reshaped = data.reshape((n_samples, n_steps, n_features))\n",
    "data_reshaped=data[:n_samples*n_steps].reshape(n_samples,n_steps,n_features)\n",
    "print(f\"Reshaped data:{data.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "##Normalization is important because of the nature of mse calculation of saits, columns with large \n",
    "#values dominate the loss, making metrics meaningless. SAITS computes MSE/MAE column-wise and averages \n",
    "#them across all columns \n",
    "#  Apply minmax scaler here \n",
    "#normalize each feature independently\n",
    "scalers={}\n",
    "data_normalized=data.copy()\n",
    "\n",
    "data_normalized = np.zeros_like(data_reshaped)  # Initialize the normalized data array\n",
    "scalers = {}\n",
    "\n",
    "for i in range(data_reshaped.shape[2]):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    # Flatten timesteps and samples for scaling\n",
    "    data_normalized[:, :, i] = scaler.fit_transform(data_reshaped[:, :, i].reshape(-1, 1)).reshape(data_reshaped.shape[0], data_reshaped.shape[1])\n",
    "    scalers[i] = scaler  # Save scalers to inverse-transform later\n",
    "\n",
    "\n",
    "\n",
    "#Split into train, test, val\n",
    "\n",
    "train_size = int(0.6 * len(data_normalized))\n",
    "val_size = int(0.2 * len(data_normalized))\n",
    "test_size = len(data_normalized) - train_size - val_size\n",
    "\n",
    "train_data = data_normalized[:train_size]\n",
    "val_data = data_normalized[train_size:train_size + val_size]\n",
    "test_data= data_normalized[train_size + val_size:]\n",
    "\n",
    "#Optional: Artificially mask. Mask 20% of the data (MIT part)\n",
    "def mcar_f(X, mask_ratio=0.2):\n",
    "    \"\"\"Apply MCAR only to observed values.\"\"\"\n",
    "    observed_mask=~np.isnan(X) #find observed positions\n",
    "    artificial_mask=mcar(X,mask_ratio).astype(bool) #generate MCAR mask, cast to boolean\n",
    "    #combine masks \n",
    "    combined_mask=observed_mask & artificial_mask\n",
    "\n",
    "    #Apply masking\n",
    "    X_masked=X.copy()\n",
    "    X_masked[combined_mask]=np.nan\n",
    "    return X_masked,combined_mask\n",
    "\n",
    "\n",
    "#Use mcar on validation data \n",
    "val_X_masked, val_mask =mcar_f(val_data)\n",
    "val_X_ori=val_data.copy() #Ground truth\n",
    "\n",
    "test_X_masked, test_mask =mcar_f(test_data)\n",
    "test_X_ori=test_data.copy() #ground truth\n",
    "\n",
    "#?? Problem: Can't have the best input for testing\n",
    "#1.Create synthetic test_data cuz if I drop nan values for test set, there's basically nothing left\n",
    "#synthetic_data=np.random.randn(n_samples,n_steps,n_features)\n",
    "#test_X_masked,test_mask=mcar_f(synthetic_data)\n",
    "#test_X_ori=synthetic_data.copy() #Ground truth\n",
    "\n",
    "# 2, Ensure no NaN values in synthetic data\n",
    "#test_X_masked = np.nan_to_num(test_X_masked, nan=np.nanmean(test_X_masked))\n",
    "#test_X_ori = np.nan_to_num(test_X_ori, nan=np.nanmean(test_X_ori))\n",
    "\n",
    "\n",
    "#3. Set aside a part of dataset for test_set\n",
    "#df_test=df1[6000:8000]  #Even with 10000 rows of dataset fot testing, after dropping nan value in rows, there is none left \n",
    "#2. Drop nan rows \n",
    "#f_test=df_test.dropna(axis=0,how='all')\n",
    "#print(\"Size of the test set:\", len(df_test)) \n",
    "#data_test=df_test[sensor_cols].values\n",
    "\n",
    "\n",
    "\n",
    "#n_features = data_test.shape[1]  # exclude the time column\n",
    "#n_steps = 60 #(TRY TO CHANGE HERE)  # # window length, 1440 steps = 24 hours of 1-minute data, but here is revised to 60 again\n",
    "#total_elements = data.shape[0] * data.shape[1]\n",
    "#n_samples = data_test.shape[0] // n_steps \n",
    "#Reshape and apply mcar to the test set \n",
    "# #data_test=data_test[:n_samples*n_steps].reshape(n_samples,n_steps,n_features)\n",
    "\n",
    "#data_test_normalized=np.zeros_like(data_test)  # Initialize the normalized data array\n",
    "#for i in range(data_test.shape[2]):\n",
    "  # scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    # Flatten timesteps and samples for scaling\n",
    "  # data_test_normalized[:, :, i] = scaler.fit_transform(data_test[:, :, i].reshape(-1, 1)).reshape(data_test.shape[0], data_test.shape[1])\n",
    "  # scalers[i] = scaler  # Save scalers to inverse-transform later\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Config:\n",
    "    no_cuda = False\n",
    "    no_mps = False\n",
    "    seed = 1\n",
    "\n",
    "args=Config()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "use_mps = not args.no_mps and torch.backends.mps.is_available()\n",
    "\n",
    "if args.cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA\")\n",
    "elif use_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "\n",
    "#MLflow set up\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "mlflow.set_experiment(\"SAITS\")\n",
    "SAITS_run_name = \"SAITS\"\n",
    "\n",
    "with mlflow.start_run(run_name=SAITS_run_name) as run:\n",
    "# initialize the model (from example)\n",
    "    saits = SAITS(\n",
    "        n_steps=data_normalized.shape[1],\n",
    "        n_features=data_normalized.shape[2],\n",
    "        n_layers=2, #deep network(4) for long sequences, here is revised for 2 since it always shows cuda error \n",
    "        d_model=512,  #d_modle must equal n_heads * d_k\n",
    "        d_ffn=512,\n",
    "        n_heads=8,\n",
    "        d_k=64,\n",
    "        d_v=64,\n",
    "        dropout=0.1,\n",
    "        attn_dropout=0.1,\n",
    "        diagonal_attention_mask=True,  # otherwise the original self-attention mechanism will be applied\n",
    "        ORT_weight=1,  # you can adjust the weight values of arguments ORT_weight\n",
    "       # and MIT_weight to make the SAITS model focus more on one task. Usually you can just leave them to the default values, i.e. 1.\n",
    "        MIT_weight=1,\n",
    "        batch_size=5, #try with 5 to see if it works, was 4. \n",
    "        # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "        epochs=10, #try with 10 to see if it runs\n",
    "        # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "        # You can leave it to defualt as None to disable early stopping.\n",
    "        patience=4, #initially was 3, tried to increase to see more possibilities \n",
    "        # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "        # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "        optimizer=Adam(lr=1e-3),\n",
    "        # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "        # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "        # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "        num_workers=0,\n",
    "        # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "        # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "        device=device, \n",
    "        # set the path for saving tensorboard and trained model files \n",
    "        saving_path=\"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/best_model\",\n",
    "        # only save the best model after training finished.\n",
    "        # You can also set it as \"better\" to save models performing better ever during training.\n",
    "        model_saving_strategy=\"best\",\n",
    ")\n",
    "\n",
    "\n",
    "# Log model parameters\n",
    "mlflow.log_param(\"n_steps\", data_normalized.shape[1])\n",
    "mlflow.log_param(\"n_features\", data_normalized.shape[2])\n",
    "mlflow.log_param(\"batch_size\", 5)\n",
    "mlflow.log_param(\"epochs\", 10)\n",
    "mlflow.log_param(\"patience\", 4)\n",
    "\n",
    "\n",
    "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
    "#use original missigness for trainig, use the mcar masked points as ground truth\n",
    "saits.fit(train_set={\"X\": train_data}, val_set={\"X\": val_X_masked , \"X_ori\":val_X_ori })\n",
    "\n",
    "\n",
    "##drop null values in test set \n",
    "\n",
    "# Check for NaN values across the entire array\n",
    "#nan_mask = np.isnan(test_X).any(axis=(1, 2))\n",
    "\n",
    "# Filter out samples (rows) that contain NaN values\n",
    "#test_X_clean = test_X[~nan_mask]\n",
    "#test_X_ori = test_X_clean\n",
    "#test_X_masked, test_mask = mcar_f(test_X_clean)\n",
    "\n",
    "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
    "# Convert the numpy array to a dictionary\n",
    "test_set_dict = {\"X\": test_X_masked}\n",
    "test_imputation = saits.predict(test_set_dict)\n",
    "\n",
    "#change back to array for applying inverse scale later\n",
    "test_imputation_array = test_imputation[\"imputation\"]\n",
    "\n",
    "\n",
    "#clear cuda after prediction if on cuda\n",
    "\n",
    "# Ensure saits_imputation and test_X are numpy arrays\n",
    "#if not isinstance(saits_imputation, np.ndarray):\n",
    "    #saits_imputation = np.array(saits_imputation)\n",
    "#if not isinstance(test_X, np.ndarray):\n",
    "     #test_X = np.array(test_X)\n",
    "\n",
    "#Inverse Scale\n",
    "def inverse_scale(imputation, scalers):\n",
    "    n_features = imputation.shape[2]\n",
    "    imputation_denorm = np.empty_like(imputation)\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        imputation_denorm[:, :, i] = scalers[i].inverse_transform(imputation[:, :, i].reshape(-1, 1)).reshape(imputation.shape[0], imputation.shape[1])\n",
    "    \n",
    "    return imputation_denorm  # Move the return statement outside the loop\n",
    "\n",
    "    \n",
    "#Apply function to the dataset \n",
    "test_imputation_denorm=inverse_scale(test_imputation_array,scalers)\n",
    "test_X_ori_denorm=inverse_scale(test_X_ori,scalers)\n",
    "\n",
    "if args.cuda:\n",
    "    torch.cuda.empty_cache() \n",
    "\n",
    "\n",
    "#Calculate metrics per-feature: mean absolute error on the ground truth (artificially-missing values)\n",
    "mae_per_feature=[]\n",
    "for i in range(n_features):\n",
    "    #Extract imputation and ground truth for feature i\n",
    "    imputation_i=test_imputation_denorm[:,:,i]\n",
    "    ground_truth_i=test_X_ori_denorm[:,:,i]\n",
    "    mask_i=test_mask[:,:,i]\n",
    "    # Check for NaN values\n",
    "    if np.isnan(imputation_i).any() or np.isnan(ground_truth_i).any():\n",
    "        print(f\"NaN values detected in feature {i}\")\n",
    "        continue  # Skip this feature if NaN values are found\n",
    "    #Filter only artificially masked positions\n",
    "    mae_i=calc_mae(imputation_i,ground_truth_i,mask_i)\n",
    "    mae_per_feature.append(mae_i)\n",
    "    print(f\"MAE for {feature_names[i]}: {mae_i:.4f}\")\n",
    "    mlflow.log_metric(f\"MAE_{feature_names[i]}\", mae_i)\n",
    "\n",
    "\n",
    "# calculate average MAE \n",
    "avg_mae=np.mean(mae_per_feature)\n",
    "print(f\"Testing mean absolute error: {avg_mae:.4f}\")\n",
    "mlflow.log_metric(\"avg_mae\", avg_mae)\n",
    "\n",
    "\n",
    "##Current error is on the cuda \n",
    "#test memory\n",
    "#def test_memory(in_size=100, out_size=10,hidden_size=100,optimizer_type=torch.optim.Adam, batch_size=1, device=0):\n",
    "   # sample_input=torch.randn(batch_size,in_size)\n",
    "  #  model=saits,\n",
    "   # optimizer=optimizer_type(model.parameters(),lr=1e-3)\n",
    "   # print(\"Beginning mem:\",torch.cuda.memory_allocated(device))\n",
    "   # model.to(device)\n",
    "   # print()\n",
    "   # for i in range(3):\n",
    "   #     print(\"Iteration\",i)\n",
    "   #     a=torch.cuda.memory_allocated(device)\n",
    "   #     out=model(sample_input.to(device)).sum()\n",
    "   #     b=torch.cuda.memory_allocated(device)\n",
    "   #     print(\"2-After forward pass\", torch.cuda.memory_allocated(device))\n",
    "   #     print(\"2-Memory consumed by forward pass\", b-a)\n",
    "   #     out.backward()\n",
    "   #     print(\"3-After backward pass\", torch.cuda.memory_allocated(device))\n",
    "   #     optimizer.step()\n",
    "   #     print(\"4-After optimizer step\", torch.cuda.memory_allocated(device))\n",
    "\n",
    "##To do: log mlflow, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d1ebb3-70cf-4220-80bf-e7222580551d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column time has 0 NaN values\n",
      "Column time has 0.0 Missing_rate\n",
      "Column fr_eng has 0 NaN values\n",
      "Column fr_eng has 0.0 Missing_rate\n",
      "Column te_exh_cyl_out__0 has 73 NaN values\n",
      "Column te_exh_cyl_out__0 has 0.0006111906496203082 Missing_rate\n",
      "Column pd_air_ic__0 has 73 NaN values\n",
      "Column pd_air_ic__0 has 0.0006111906496203082 Missing_rate\n",
      "Column pr_exh_turb_out__0 has 119439 NaN values\n",
      "Column pr_exh_turb_out__0 has 1.0 Missing_rate\n",
      "Column te_air_ic_out__0 has 73 NaN values\n",
      "Column te_air_ic_out__0 has 0.0006111906496203082 Missing_rate\n",
      "Column te_seawater has 73 NaN values\n",
      "Column te_seawater has 0.0006111906496203082 Missing_rate\n",
      "Column te_air_comp_in_a__0 has 119439 NaN values\n",
      "Column te_air_comp_in_a__0 has 1.0 Missing_rate\n",
      "Column te_air_comp_in_b__0 has 119439 NaN values\n",
      "Column te_air_comp_in_b__0 has 1.0 Missing_rate\n",
      "Column fr_tc__0 has 119439 NaN values\n",
      "Column fr_tc__0 has 1.0 Missing_rate\n",
      "Column pr_baro has 73 NaN values\n",
      "Column pr_baro has 0.0006111906496203082 Missing_rate\n",
      "Column pd_air_ic__0_1 has 73 NaN values\n",
      "Column pd_air_ic__0_1 has 0.0006111906496203082 Missing_rate\n",
      "Column pr_exh_rec has 73 NaN values\n",
      "Column pr_exh_rec has 0.0006111906496203082 Missing_rate\n",
      "Column te_exh_turb_in__0 has 119439 NaN values\n",
      "Column te_exh_turb_in__0 has 1.0 Missing_rate\n",
      "Column te_exh_turb_out__0 has 119439 NaN values\n",
      "Column te_exh_turb_out__0 has 1.0 Missing_rate\n",
      "Column bo_aux_blower_running has 0 NaN values\n",
      "Column bo_aux_blower_running has 0.0 Missing_rate\n",
      "Column re_eng_load has 173 NaN values\n",
      "Column re_eng_load has 0.0014484381148536073 Missing_rate\n",
      "Column pr_air_scav_ecs has 0 NaN values\n",
      "Column pr_air_scav_ecs has 0.0 Missing_rate\n",
      "Column pr_air_scav has 0 NaN values\n",
      "Column pr_air_scav has 0.0 Missing_rate\n",
      "Column te_air_scav_rec has 73 NaN values\n",
      "Column te_air_scav_rec has 0.0006111906496203082 Missing_rate\n",
      "Column te_air_ic_out__0_1 has 73 NaN values\n",
      "Column te_air_ic_out__0_1 has 0.0006111906496203082 Missing_rate\n",
      "Column pr_cyl_comp__0 has 173 NaN values\n",
      "Column pr_cyl_comp__0 has 0.0014484381148536073 Missing_rate\n",
      "Column pr_cyl_max__0 has 173 NaN values\n",
      "Column pr_cyl_max__0 has 0.0014484381148536073 Missing_rate\n",
      "Column se_mip__0 has 173 NaN values\n",
      "Column se_mip__0 has 0.0014484381148536073 Missing_rate\n",
      "Column te_exh_cyl_out__0_1 has 73 NaN values\n",
      "Column te_exh_cyl_out__0_1 has 0.0006111906496203082 Missing_rate\n",
      "Column fr_eng_setpoint has 0 NaN values\n",
      "Column fr_eng_setpoint has 0.0 Missing_rate\n",
      "Column te_air_scav_rec_iso has 51235 NaN values\n",
      "Column te_air_scav_rec_iso has 0.4289637388122807 Missing_rate\n",
      "Column pr_cyl_max_mv_iso has 51385 NaN values\n",
      "Column pr_cyl_max_mv_iso has 0.4302196100101307 Missing_rate\n",
      "Column pr_cyl_comp_mv_iso has 51385 NaN values\n",
      "Column pr_cyl_comp_mv_iso has 0.4302196100101307 Missing_rate\n",
      "Column fr_eng_ecs has 0 NaN values\n",
      "Column fr_eng_ecs has 0.0 Missing_rate\n",
      "Column pr_air_scav_iso has 51385 NaN values\n",
      "Column pr_air_scav_iso has 0.4302196100101307 Missing_rate\n",
      "Column engine_type_G95ME-C10.5-GI-EGRTC has 0 NaN values\n",
      "Column engine_type_G95ME-C10.5-GI-EGRTC has 0.0 Missing_rate\n",
      "Original size:119439, Sampled size: 11944\n",
      "Reshaped data:(11944, 31)\n",
      "Using CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Run with UUID a5285036106e4611b162d9ad8f532b5a is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/Pypots_model_lighter.py:233\u001b[0m\n\u001b[1;32m    230\u001b[0m mlflow\u001b[39m.\u001b[39mset_experiment(\u001b[39m\"\u001b[39m\u001b[39mSAITS\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    231\u001b[0m SAITS_run_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mSAITS\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 233\u001b[0m \u001b[39mwith\u001b[39;00m mlflow\u001b[39m.\u001b[39;49mstart_run(run_name\u001b[39m=\u001b[39;49mSAITS_run_name) \u001b[39mas\u001b[39;00m run:\n\u001b[1;32m    234\u001b[0m \u001b[39m# initialize the model (from example)\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     saits \u001b[39m=\u001b[39m SAITS(\n\u001b[1;32m    236\u001b[0m         n_steps\u001b[39m=\u001b[39mdata_normalized\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m],\n\u001b[1;32m    237\u001b[0m         n_features\u001b[39m=\u001b[39mdata_normalized\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    270\u001b[0m         model_saving_strategy\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbest\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    271\u001b[0m )\n\u001b[1;32m    275\u001b[0m \u001b[39m# Log model parameters\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/tracking/fluent.py:351\u001b[0m, in \u001b[0;36mstart_run\u001b[0;34m(run_id, experiment_id, run_name, nested, parent_run_id, tags, description, log_system_metrics)\u001b[0m\n\u001b[1;32m    349\u001b[0m experiment_id \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(experiment_id) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(experiment_id, \u001b[39mint\u001b[39m) \u001b[39melse\u001b[39;00m experiment_id\n\u001b[1;32m    350\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(active_run_stack) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m nested:\n\u001b[0;32m--> 351\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\n\u001b[1;32m    352\u001b[0m         (\n\u001b[1;32m    353\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mRun with UUID \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m is already active. To start a new run, first end the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    354\u001b[0m             \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcurrent run with mlflow.end_run(). To start a nested \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    355\u001b[0m             \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrun, call start_run with nested=True\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    356\u001b[0m         )\u001b[39m.\u001b[39mformat(active_run_stack[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mrun_id)\n\u001b[1;32m    357\u001b[0m     )\n\u001b[1;32m    358\u001b[0m client \u001b[39m=\u001b[39m MlflowClient()\n\u001b[1;32m    359\u001b[0m \u001b[39mif\u001b[39;00m run_id:\n",
      "\u001b[0;31mException\u001b[0m: Run with UUID a5285036106e4611b162d9ad8f532b5a is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True"
     ]
    }
   ],
   "source": [
    "#Import Pypots Library\n",
    "from pypots.optim import Adam\n",
    "from pypots.imputation import SAITS\n",
    "#from pypots.utils.metrics import calc_mae\n",
    "from pypots.nn.functional import calc_mae\n",
    "\n",
    "\n",
    "import argparse\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import data_insight\n",
    "from data_insight import setup_duckdb\n",
    "from duckdb import DuckDBPyConnection as DuckDB\n",
    "from duckdb import DuckDBPyRelation as Relation\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "from duckdb import DuckDBPyConnection as DuckDB\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import optuna \n",
    "from optuna.visualization import plot_optimization_history\n",
    "\n",
    "\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, Dataset\n",
    "from pygrinder.missing_completely_at_random import mcar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import sensor_imputation_thesis.shared.load_data as load\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "#PatchTST might be an ideal choise if SAITS is too slow \n",
    "\n",
    "##Drop columns with different indexes while loading data.. Or the mean values \n",
    "\n",
    "df=pd.read_parquet(\"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/Newdataframeforpypots.parquet\")\n",
    "\n",
    "len(df)\n",
    "\n",
    "#current length of the dataframe is 119439\n",
    "\n",
    "# Check nan values in each column\n",
    "for col in df.columns:\n",
    "    print(f\"Column {col} has {df[col].isna().sum()} NaN values\")\n",
    "    missing_rate=df[col].isna().sum()/len(df[col])\n",
    "    print(f\"Column {col} has {missing_rate} Missing_rate\")\n",
    "\n",
    "\n",
    "#Try with smaller dataset, size 4000\n",
    "##SAMPLE the percengtage of the dataset, df.sample (averagely pick samples)\n",
    "#not df.sample cuz it will randomly select \n",
    "original_size=len(df)\n",
    "desired_fraction=0.10\n",
    "step=int(1/desired_fraction) #step_size=10 (sample every 10th minute)\n",
    "\n",
    "#Systematic sampling: Start at a random offset to avoid bias \n",
    "start=np.random.randint(0,step) #Random start between 0-9\n",
    "df1=df.iloc[start::step].reset_index(drop=True)\n",
    "\n",
    "print(f\"Original size:{len(df)}, Sampled size: {len(df1)}\")\n",
    "\n",
    "\n",
    "#df1=df[:10000]\n",
    "\n",
    "\n",
    "# Custom Dataset class\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Data processing code\n",
    "sensor_cols = [col for col in df1.columns if col != \"time\"]\n",
    "data = df1[sensor_cols].values\n",
    "\n",
    "#Â¤get feature names for printing mae later \n",
    "feature_names=df1[sensor_cols].columns.tolist()\n",
    "\n",
    "## Convert data to 3D arrays of shape n_samples, n_timesteps, n_features, X_ori refers to the original data without missing values \n",
    "## Reconstruct all columns simultaneously  #num_features: 119\n",
    "n_features = data.shape[1]  # exclude the time column\n",
    "n_steps = 30 #60 (was 60 previously) #(TRY TO CHANGE HERE)  # # window length, 1440 steps = 24 hours of 1-minute data, but here is revised to 60 again\n",
    "#total_elements = data.shape[0] * data.shape[1]\n",
    "n_samples = data.shape[0] // n_steps \n",
    "\n",
    "\n",
    "\n",
    "# Reshape to (n_samples // n_steps, n_steps, n_features)\n",
    "#data_reshaped = data.reshape((n_samples, n_steps, n_features))\n",
    "data_reshaped=data[:n_samples*n_steps].reshape(n_samples,n_steps,n_features)\n",
    "print(f\"Reshaped data:{data.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "##Normalization is important because of the nature of mse calculation of saits, columns with large \n",
    "#values dominate the loss, making metrics meaningless. SAITS computes MSE/MAE column-wise and averages \n",
    "#them across all columns \n",
    "#  Apply minmax scaler here \n",
    "#normalize each feature independently\n",
    "scalers={}\n",
    "data_normalized=data.copy()\n",
    "\n",
    "data_normalized = np.zeros_like(data_reshaped)  # Initialize the normalized data array\n",
    "scalers = {}\n",
    "\n",
    "for i in range(data_reshaped.shape[2]):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    # Flatten timesteps and samples for scaling\n",
    "    data_normalized[:, :, i] = scaler.fit_transform(data_reshaped[:, :, i].reshape(-1, 1)).reshape(data_reshaped.shape[0], data_reshaped.shape[1])\n",
    "    scalers[i] = scaler  # Save scalers to inverse-transform later\n",
    "\n",
    "\n",
    "\n",
    "#Split into train, test, val\n",
    "\n",
    "train_size = int(0.6 * len(data_normalized))\n",
    "val_size = int(0.2 * len(data_normalized))\n",
    "test_size = len(data_normalized) - train_size - val_size\n",
    "\n",
    "train_data = data_normalized[:train_size]\n",
    "val_data = data_normalized[train_size:train_size + val_size]\n",
    "test_data= data_normalized[train_size + val_size:]\n",
    "\n",
    "#Optional: Artificially mask. Mask 20% of the data (MIT part)\n",
    "def mcar_f(X, mask_ratio=0.2):\n",
    "    \"\"\"Apply MCAR only to observed values.\"\"\"\n",
    "    observed_mask=~np.isnan(X) #find observed positions\n",
    "    artificial_mask=mcar(X,mask_ratio).astype(bool) #generate MCAR mask, cast to boolean\n",
    "    #combine masks \n",
    "    combined_mask=observed_mask & artificial_mask\n",
    "\n",
    "    #Apply masking\n",
    "    X_masked=X.copy()\n",
    "    X_masked[combined_mask]=np.nan\n",
    "    return X_masked,combined_mask\n",
    "\n",
    "\n",
    "#Use mcar on validation data \n",
    "val_X_masked, val_mask =mcar_f(val_data)\n",
    "val_X_ori=val_data.copy() #Ground truth\n",
    "\n",
    "test_X_masked, test_mask =mcar_f(test_data)\n",
    "test_X_ori=test_data.copy() #ground truth\n",
    "\n",
    "#?? Problem: Can't have the best input for testing\n",
    "#1.Create synthetic test_data cuz if I drop nan values for test set, there's basically nothing left\n",
    "#synthetic_data=np.random.randn(n_samples,n_steps,n_features)\n",
    "#test_X_masked,test_mask=mcar_f(synthetic_data)\n",
    "#test_X_ori=synthetic_data.copy() #Ground truth\n",
    "\n",
    "# 2, Ensure no NaN values in synthetic data\n",
    "#test_X_masked = np.nan_to_num(test_X_masked, nan=np.nanmean(test_X_masked))\n",
    "#test_X_ori = np.nan_to_num(test_X_ori, nan=np.nanmean(test_X_ori))\n",
    "\n",
    "\n",
    "#3. Set aside a part of dataset for test_set\n",
    "#df_test=df1[6000:8000]  #Even with 10000 rows of dataset fot testing, after dropping nan value in rows, there is none left \n",
    "#2. Drop nan rows \n",
    "#f_test=df_test.dropna(axis=0,how='all')\n",
    "#print(\"Size of the test set:\", len(df_test)) \n",
    "#data_test=df_test[sensor_cols].values\n",
    "\n",
    "\n",
    "\n",
    "#n_features = data_test.shape[1]  # exclude the time column\n",
    "#n_steps = 60 #(TRY TO CHANGE HERE)  # # window length, 1440 steps = 24 hours of 1-minute data, but here is revised to 60 again\n",
    "#total_elements = data.shape[0] * data.shape[1]\n",
    "#n_samples = data_test.shape[0] // n_steps \n",
    "#Reshape and apply mcar to the test set \n",
    "# #data_test=data_test[:n_samples*n_steps].reshape(n_samples,n_steps,n_features)\n",
    "\n",
    "#data_test_normalized=np.zeros_like(data_test)  # Initialize the normalized data array\n",
    "#for i in range(data_test.shape[2]):\n",
    "  # scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    # Flatten timesteps and samples for scaling\n",
    "  # data_test_normalized[:, :, i] = scaler.fit_transform(data_test[:, :, i].reshape(-1, 1)).reshape(data_test.shape[0], data_test.shape[1])\n",
    "  # scalers[i] = scaler  # Save scalers to inverse-transform later\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Config:\n",
    "    no_cuda = False\n",
    "    no_mps = False\n",
    "    seed = 1\n",
    "\n",
    "args=Config()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "use_mps = not args.no_mps and torch.backends.mps.is_available()\n",
    "\n",
    "if args.cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA\")\n",
    "elif use_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "\n",
    "#MLflow set up\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "mlflow.set_experiment(\"SAITS\")\n",
    "SAITS_run_name = \"SAITS\"\n",
    "\n",
    "with mlflow.start_run(run_name=SAITS_run_name) as run:\n",
    "# initialize the model (from example)\n",
    "    saits = SAITS(\n",
    "        n_steps=data_normalized.shape[1],\n",
    "        n_features=data_normalized.shape[2],\n",
    "        n_layers=2, #deep network(4) for long sequences, here is revised for 2 since it always shows cuda error \n",
    "        d_model=512,  #d_modle must equal n_heads * d_k\n",
    "        d_ffn=512,\n",
    "        n_heads=8,\n",
    "        d_k=64,\n",
    "        d_v=64,\n",
    "        dropout=0.1,\n",
    "        attn_dropout=0.1,\n",
    "        diagonal_attention_mask=True,  # otherwise the original self-attention mechanism will be applied\n",
    "        ORT_weight=1,  # you can adjust the weight values of arguments ORT_weight\n",
    "       # and MIT_weight to make the SAITS model focus more on one task. Usually you can just leave them to the default values, i.e. 1.\n",
    "        MIT_weight=1,\n",
    "        batch_size=5, #try with 5 to see if it works, was 4. \n",
    "        # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "        epochs=10, #try with 10 to see if it runs\n",
    "        # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "        # You can leave it to defualt as None to disable early stopping.\n",
    "        patience=4, #initially was 3, tried to increase to see more possibilities \n",
    "        # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "        # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "        optimizer=Adam(lr=1e-3),\n",
    "        # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "        # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "        # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "        num_workers=0,\n",
    "        # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "        # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "        device=device, \n",
    "        # set the path for saving tensorboard and trained model files \n",
    "        saving_path=\"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/best_model\",\n",
    "        # only save the best model after training finished.\n",
    "        # You can also set it as \"better\" to save models performing better ever during training.\n",
    "        model_saving_strategy=\"best\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Log model parameters\n",
    "mlflow.log_param(\"n_steps\", data_normalized.shape[1])\n",
    "mlflow.log_param(\"n_features\", data_normalized.shape[2])\n",
    "mlflow.log_param(\"batch_size\", 5)\n",
    "mlflow.log_param(\"epochs\", 10)\n",
    "mlflow.log_param(\"patience\", 4)\n",
    "\n",
    "\n",
    "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
    "#use original missigness for trainig, use the mcar masked points as ground truth\n",
    "saits.fit(train_set={\"X\": train_data}, val_set={\"X\": val_X_masked , \"X_ori\":val_X_ori })\n",
    "\n",
    "\n",
    "##drop null values in test set \n",
    "\n",
    "# Check for NaN values across the entire array\n",
    "#nan_mask = np.isnan(test_X).any(axis=(1, 2))\n",
    "\n",
    "# Filter out samples (rows) that contain NaN values\n",
    "#test_X_clean = test_X[~nan_mask]\n",
    "#test_X_ori = test_X_clean\n",
    "#test_X_masked, test_mask = mcar_f(test_X_clean)\n",
    "\n",
    "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
    "# Convert the numpy array to a dictionary\n",
    "test_set_dict = {\"X\": test_X_masked}\n",
    "test_imputation = saits.predict(test_set_dict)\n",
    "\n",
    "#change back to array for applying inverse scale later\n",
    "test_imputation_array = test_imputation[\"imputation\"]\n",
    "\n",
    "\n",
    "#clear cuda after prediction if on cuda\n",
    "\n",
    "# Ensure saits_imputation and test_X are numpy arrays\n",
    "#if not isinstance(saits_imputation, np.ndarray):\n",
    "    #saits_imputation = np.array(saits_imputation)\n",
    "#if not isinstance(test_X, np.ndarray):\n",
    "     #test_X = np.array(test_X)\n",
    "\n",
    "#Inverse Scale\n",
    "def inverse_scale(imputation, scalers):\n",
    "    n_features = imputation.shape[2]\n",
    "    imputation_denorm = np.empty_like(imputation)\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        imputation_denorm[:, :, i] = scalers[i].inverse_transform(imputation[:, :, i].reshape(-1, 1)).reshape(imputation.shape[0], imputation.shape[1])\n",
    "    \n",
    "    return imputation_denorm  # Move the return statement outside the loop\n",
    "\n",
    "    \n",
    "#Apply function to the dataset \n",
    "test_imputation_denorm=inverse_scale(test_imputation_array,scalers)\n",
    "test_X_ori_denorm=inverse_scale(test_X_ori,scalers)\n",
    "\n",
    "if args.cuda:\n",
    "    torch.cuda.empty_cache() \n",
    "\n",
    "\n",
    "#Calculate metrics per-feature: mean absolute error on the ground truth (artificially-missing values)\n",
    "mae_per_feature=[]\n",
    "for i in range(n_features):\n",
    "    #Extract imputation and ground truth for feature i\n",
    "    imputation_i=test_imputation_denorm[:,:,i]\n",
    "    ground_truth_i=test_X_ori_denorm[:,:,i]\n",
    "    mask_i=test_mask[:,:,i]\n",
    "    # Check for NaN values\n",
    "    if np.isnan(imputation_i).any() or np.isnan(ground_truth_i).any():\n",
    "        print(f\"NaN values detected in feature {i}\")\n",
    "        continue  # Skip this feature if NaN values are found\n",
    "    #Filter only artificially masked positions\n",
    "    mae_i=calc_mae(imputation_i,ground_truth_i,mask_i)\n",
    "    mae_per_feature.append(mae_i)\n",
    "    print(f\"MAE for {feature_names[i]}: {mae_i:.4f}\")\n",
    "    mlflow.log_metric(f\"MAE_{feature_names[i]}\", mae_i)\n",
    "\n",
    "\n",
    "# calculate average MAE \n",
    "avg_mae=np.mean(mae_per_feature)\n",
    "print(f\"Testing mean absolute error: {avg_mae:.4f}\")\n",
    "mlflow.log_metric(\"avg_mae\", avg_mae)\n",
    "\n",
    "mlflow.end_run()\n",
    "\n",
    "##Current error is on the cuda \n",
    "#test memory\n",
    "#def test_memory(in_size=100, out_size=10,hidden_size=100,optimizer_type=torch.optim.Adam, batch_size=1, device=0):\n",
    "   # sample_input=torch.randn(batch_size,in_size)\n",
    "  #  model=saits,\n",
    "   # optimizer=optimizer_type(model.parameters(),lr=1e-3)\n",
    "   # print(\"Beginning mem:\",torch.cuda.memory_allocated(device))\n",
    "   # model.to(device)\n",
    "   # print()\n",
    "   # for i in range(3):\n",
    "   #     print(\"Iteration\",i)\n",
    "   #     a=torch.cuda.memory_allocated(device)\n",
    "   #     out=model(sample_input.to(device)).sum()\n",
    "   #     b=torch.cuda.memory_allocated(device)\n",
    "   #     print(\"2-After forward pass\", torch.cuda.memory_allocated(device))\n",
    "   #     print(\"2-Memory consumed by forward pass\", b-a)\n",
    "   #     out.backward()\n",
    "   #     print(\"3-After backward pass\", torch.cuda.memory_allocated(device))\n",
    "   #     optimizer.step()\n",
    "   #     print(\"4-After optimizer step\", torch.cuda.memory_allocated(device))\n",
    "\n",
    "##To do: log mlflow, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸƒ View run intelligent-worm-859 at: http://localhost:5000/#/experiments/1/runs/a5285036106e4611b162d9ad8f532b5a\n",
      "ğŸ§ª View experiment at: http://localhost:5000/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61efdebb-640f-4e87-9cb5-4b378f697287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "2025-05-13 21:45:25 [INFO]: Using the given device: cpu\n",
      "2025-05-13 21:45:25 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/best_model/20250513_T214525\n",
      "2025-05-13 21:45:25 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/best_model/20250513_T214525/tensorboard\n",
      "2025-05-13 21:45:25 [INFO]: Using customized MAE as the training loss function.\n",
      "2025-05-13 21:45:25 [INFO]: Using customized MSE as the validation metric function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column time has 0 NaN values\n",
      "Column time has 0.0 Missing_rate\n",
      "Column fr_eng has 0 NaN values\n",
      "Column fr_eng has 0.0 Missing_rate\n",
      "Column te_exh_cyl_out__0 has 73 NaN values\n",
      "Column te_exh_cyl_out__0 has 0.0006111906496203082 Missing_rate\n",
      "Column pd_air_ic__0 has 73 NaN values\n",
      "Column pd_air_ic__0 has 0.0006111906496203082 Missing_rate\n",
      "Column pr_exh_turb_out__0 has 119439 NaN values\n",
      "Column pr_exh_turb_out__0 has 1.0 Missing_rate\n",
      "Column te_air_ic_out__0 has 73 NaN values\n",
      "Column te_air_ic_out__0 has 0.0006111906496203082 Missing_rate\n",
      "Column te_seawater has 73 NaN values\n",
      "Column te_seawater has 0.0006111906496203082 Missing_rate\n",
      "Column te_air_comp_in_a__0 has 119439 NaN values\n",
      "Column te_air_comp_in_a__0 has 1.0 Missing_rate\n",
      "Column te_air_comp_in_b__0 has 119439 NaN values\n",
      "Column te_air_comp_in_b__0 has 1.0 Missing_rate\n",
      "Column fr_tc__0 has 119439 NaN values\n",
      "Column fr_tc__0 has 1.0 Missing_rate\n",
      "Column pr_baro has 73 NaN values\n",
      "Column pr_baro has 0.0006111906496203082 Missing_rate\n",
      "Column pd_air_ic__0_1 has 73 NaN values\n",
      "Column pd_air_ic__0_1 has 0.0006111906496203082 Missing_rate\n",
      "Column pr_exh_rec has 73 NaN values\n",
      "Column pr_exh_rec has 0.0006111906496203082 Missing_rate\n",
      "Column te_exh_turb_in__0 has 119439 NaN values\n",
      "Column te_exh_turb_in__0 has 1.0 Missing_rate\n",
      "Column te_exh_turb_out__0 has 119439 NaN values\n",
      "Column te_exh_turb_out__0 has 1.0 Missing_rate\n",
      "Column bo_aux_blower_running has 0 NaN values\n",
      "Column bo_aux_blower_running has 0.0 Missing_rate\n",
      "Column re_eng_load has 173 NaN values\n",
      "Column re_eng_load has 0.0014484381148536073 Missing_rate\n",
      "Column pr_air_scav_ecs has 0 NaN values\n",
      "Column pr_air_scav_ecs has 0.0 Missing_rate\n",
      "Column pr_air_scav has 0 NaN values\n",
      "Column pr_air_scav has 0.0 Missing_rate\n",
      "Column te_air_scav_rec has 73 NaN values\n",
      "Column te_air_scav_rec has 0.0006111906496203082 Missing_rate\n",
      "Column te_air_ic_out__0_1 has 73 NaN values\n",
      "Column te_air_ic_out__0_1 has 0.0006111906496203082 Missing_rate\n",
      "Column pr_cyl_comp__0 has 173 NaN values\n",
      "Column pr_cyl_comp__0 has 0.0014484381148536073 Missing_rate\n",
      "Column pr_cyl_max__0 has 173 NaN values\n",
      "Column pr_cyl_max__0 has 0.0014484381148536073 Missing_rate\n",
      "Column se_mip__0 has 173 NaN values\n",
      "Column se_mip__0 has 0.0014484381148536073 Missing_rate\n",
      "Column te_exh_cyl_out__0_1 has 73 NaN values\n",
      "Column te_exh_cyl_out__0_1 has 0.0006111906496203082 Missing_rate\n",
      "Column fr_eng_setpoint has 0 NaN values\n",
      "Column fr_eng_setpoint has 0.0 Missing_rate\n",
      "Column te_air_scav_rec_iso has 51235 NaN values\n",
      "Column te_air_scav_rec_iso has 0.4289637388122807 Missing_rate\n",
      "Column pr_cyl_max_mv_iso has 51385 NaN values\n",
      "Column pr_cyl_max_mv_iso has 0.4302196100101307 Missing_rate\n",
      "Column pr_cyl_comp_mv_iso has 51385 NaN values\n",
      "Column pr_cyl_comp_mv_iso has 0.4302196100101307 Missing_rate\n",
      "Column fr_eng_ecs has 0 NaN values\n",
      "Column fr_eng_ecs has 0.0 Missing_rate\n",
      "Column pr_air_scav_iso has 51385 NaN values\n",
      "Column pr_air_scav_iso has 0.4302196100101307 Missing_rate\n",
      "Column engine_type_G95ME-C10.5-GI-EGRTC has 0 NaN values\n",
      "Column engine_type_G95ME-C10.5-GI-EGRTC has 0.0 Missing_rate\n",
      "Original size:119439, Sampled size: 11944\n",
      "Reshaped data:(11944, 31)\n",
      "Using CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 21:45:25 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 6,402,976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸƒ View run SAITS at: http://localhost:5000/#/experiments/1/runs/91558134e43f4cb691a4e2c055ce0621\n",
      "ğŸ§ª View experiment at: http://localhost:5000/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 21:45:32 [INFO]: Epoch 001 - training loss (MAE): 0.4964, validation MSE: 0.0687\n",
      "2025-05-13 21:45:39 [INFO]: Epoch 002 - training loss (MAE): 0.2534, validation MSE: 0.0921\n",
      "2025-05-13 21:45:45 [INFO]: Epoch 003 - training loss (MAE): 0.2250, validation MSE: 0.1182\n",
      "2025-05-13 21:45:52 [INFO]: Epoch 004 - training loss (MAE): 0.1784, validation MSE: 0.0729\n",
      "2025-05-13 21:45:59 [INFO]: Epoch 005 - training loss (MAE): 0.1823, validation MSE: 0.0942\n",
      "2025-05-13 21:45:59 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-05-13 21:45:59 [INFO]: Finished training. The best model is from epoch#1.\n",
      "2025-05-13 21:45:59 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/best_model/20250513_T214525/SAITS.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for fr_eng: 0.1380\n",
      "NaN values detected in feature 1\n",
      "NaN values detected in feature 2\n",
      "NaN values detected in feature 3\n",
      "NaN values detected in feature 4\n",
      "NaN values detected in feature 5\n",
      "NaN values detected in feature 6\n",
      "NaN values detected in feature 7\n",
      "NaN values detected in feature 8\n",
      "NaN values detected in feature 9\n",
      "NaN values detected in feature 10\n",
      "NaN values detected in feature 11\n",
      "NaN values detected in feature 12\n",
      "NaN values detected in feature 13\n",
      "MAE for bo_aux_blower_running: 0.4142\n",
      "NaN values detected in feature 15\n",
      "MAE for pr_air_scav_ecs: 60613.2628\n",
      "MAE for pr_air_scav: 35925.2965\n",
      "NaN values detected in feature 18\n",
      "NaN values detected in feature 19\n",
      "NaN values detected in feature 20\n",
      "NaN values detected in feature 21\n",
      "NaN values detected in feature 22\n",
      "NaN values detected in feature 23\n",
      "MAE for fr_eng_setpoint: 0.2241\n",
      "NaN values detected in feature 25\n",
      "NaN values detected in feature 26\n",
      "NaN values detected in feature 27\n",
      "MAE for fr_eng_ecs: 0.1385\n",
      "NaN values detected in feature 29\n",
      "MAE for engine_type_G95ME-C10.5-GI-EGRTC: 0.0124\n",
      "Testing mean absolute error: 13791.3552\n",
      "ğŸƒ View run lyrical-auk-966 at: http://localhost:5000/#/experiments/1/runs/e0feebc2310f48ec8fef16b122be6ab1\n",
      "ğŸ§ª View experiment at: http://localhost:5000/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "#Import Pypots Library\n",
    "from pypots.optim import Adam\n",
    "from pypots.imputation import SAITS\n",
    "#from pypots.utils.metrics import calc_mae\n",
    "from pypots.nn.functional import calc_mae\n",
    "\n",
    "\n",
    "import argparse\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import data_insight\n",
    "from data_insight import setup_duckdb\n",
    "from duckdb import DuckDBPyConnection as DuckDB\n",
    "from duckdb import DuckDBPyRelation as Relation\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "from duckdb import DuckDBPyConnection as DuckDB\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import optuna \n",
    "from optuna.visualization import plot_optimization_history\n",
    "\n",
    "\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, Dataset\n",
    "from pygrinder.missing_completely_at_random import mcar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import sensor_imputation_thesis.shared.load_data as load\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "#PatchTST might be an ideal choise if SAITS is too slow \n",
    "\n",
    "##Drop columns with different indexes while loading data.. Or the mean values \n",
    "\n",
    "df=pd.read_parquet(\"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/Newdataframeforpypots.parquet\")\n",
    "\n",
    "len(df)\n",
    "\n",
    "#current length of the dataframe is 119439\n",
    "\n",
    "# Check nan values in each column\n",
    "for col in df.columns:\n",
    "    print(f\"Column {col} has {df[col].isna().sum()} NaN values\")\n",
    "    missing_rate=df[col].isna().sum()/len(df[col])\n",
    "    print(f\"Column {col} has {missing_rate} Missing_rate\")\n",
    "\n",
    "\n",
    "#Try with smaller dataset, size 4000\n",
    "##SAMPLE the percengtage of the dataset, df.sample (averagely pick samples)\n",
    "#not df.sample cuz it will randomly select \n",
    "original_size=len(df)\n",
    "desired_fraction=0.10\n",
    "step=int(1/desired_fraction) #step_size=10 (sample every 10th minute)\n",
    "\n",
    "#Systematic sampling: Start at a random offset to avoid bias \n",
    "start=np.random.randint(0,step) #Random start between 0-9\n",
    "df1=df.iloc[start::step].reset_index(drop=True)\n",
    "\n",
    "print(f\"Original size:{len(df)}, Sampled size: {len(df1)}\")\n",
    "\n",
    "\n",
    "#df1=df[:10000]\n",
    "\n",
    "\n",
    "# Custom Dataset class\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Data processing code\n",
    "sensor_cols = [col for col in df1.columns if col != \"time\"]\n",
    "data = df1[sensor_cols].values\n",
    "\n",
    "#Â¤get feature names for printing mae later \n",
    "feature_names=df1[sensor_cols].columns.tolist()\n",
    "\n",
    "## Convert data to 3D arrays of shape n_samples, n_timesteps, n_features, X_ori refers to the original data without missing values \n",
    "## Reconstruct all columns simultaneously  #num_features: 119\n",
    "n_features = data.shape[1]  # exclude the time column\n",
    "n_steps = 30 #60 (was 60 previously) #(TRY TO CHANGE HERE)  # # window length, 1440 steps = 24 hours of 1-minute data, but here is revised to 60 again\n",
    "#total_elements = data.shape[0] * data.shape[1]\n",
    "n_samples = data.shape[0] // n_steps \n",
    "\n",
    "\n",
    "\n",
    "# Reshape to (n_samples // n_steps, n_steps, n_features)\n",
    "#data_reshaped = data.reshape((n_samples, n_steps, n_features))\n",
    "data_reshaped=data[:n_samples*n_steps].reshape(n_samples,n_steps,n_features)\n",
    "print(f\"Reshaped data:{data.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "##Normalization is important because of the nature of mse calculation of saits, columns with large \n",
    "#values dominate the loss, making metrics meaningless. SAITS computes MSE/MAE column-wise and averages \n",
    "#them across all columns \n",
    "#  Apply minmax scaler here \n",
    "#normalize each feature independently\n",
    "scalers={}\n",
    "data_normalized=data.copy()\n",
    "\n",
    "data_normalized = np.zeros_like(data_reshaped)  # Initialize the normalized data array\n",
    "scalers = {}\n",
    "\n",
    "for i in range(data_reshaped.shape[2]):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    # Flatten timesteps and samples for scaling\n",
    "    data_normalized[:, :, i] = scaler.fit_transform(data_reshaped[:, :, i].reshape(-1, 1)).reshape(data_reshaped.shape[0], data_reshaped.shape[1])\n",
    "    scalers[i] = scaler  # Save scalers to inverse-transform later\n",
    "\n",
    "\n",
    "\n",
    "#Split into train, test, val\n",
    "\n",
    "train_size = int(0.6 * len(data_normalized))\n",
    "val_size = int(0.2 * len(data_normalized))\n",
    "test_size = len(data_normalized) - train_size - val_size\n",
    "\n",
    "train_data = data_normalized[:train_size]\n",
    "val_data = data_normalized[train_size:train_size + val_size]\n",
    "test_data= data_normalized[train_size + val_size:]\n",
    "\n",
    "#Optional: Artificially mask. Mask 20% of the data (MIT part)\n",
    "def mcar_f(X, mask_ratio=0.2):\n",
    "    \"\"\"Apply MCAR only to observed values.\"\"\"\n",
    "    observed_mask=~np.isnan(X) #find observed positions\n",
    "    artificial_mask=mcar(X,mask_ratio).astype(bool) #generate MCAR mask, cast to boolean\n",
    "    #combine masks \n",
    "    combined_mask=observed_mask & artificial_mask\n",
    "\n",
    "    #Apply masking\n",
    "    X_masked=X.copy()\n",
    "    X_masked[combined_mask]=np.nan\n",
    "    return X_masked,combined_mask\n",
    "\n",
    "\n",
    "#Use mcar on validation data \n",
    "val_X_masked, val_mask =mcar_f(val_data)\n",
    "val_X_ori=val_data.copy() #Ground truth\n",
    "\n",
    "test_X_masked, test_mask =mcar_f(test_data)\n",
    "test_X_ori=test_data.copy() #ground truth\n",
    "\n",
    "#?? Problem: Can't have the best input for testing\n",
    "#1.Create synthetic test_data cuz if I drop nan values for test set, there's basically nothing left\n",
    "#synthetic_data=np.random.randn(n_samples,n_steps,n_features)\n",
    "#test_X_masked,test_mask=mcar_f(synthetic_data)\n",
    "#test_X_ori=synthetic_data.copy() #Ground truth\n",
    "\n",
    "# 2, Ensure no NaN values in synthetic data\n",
    "#test_X_masked = np.nan_to_num(test_X_masked, nan=np.nanmean(test_X_masked))\n",
    "#test_X_ori = np.nan_to_num(test_X_ori, nan=np.nanmean(test_X_ori))\n",
    "\n",
    "\n",
    "#3. Set aside a part of dataset for test_set\n",
    "#df_test=df1[6000:8000]  #Even with 10000 rows of dataset fot testing, after dropping nan value in rows, there is none left \n",
    "#2. Drop nan rows \n",
    "#f_test=df_test.dropna(axis=0,how='all')\n",
    "#print(\"Size of the test set:\", len(df_test)) \n",
    "#data_test=df_test[sensor_cols].values\n",
    "\n",
    "\n",
    "\n",
    "#n_features = data_test.shape[1]  # exclude the time column\n",
    "#n_steps = 60 #(TRY TO CHANGE HERE)  # # window length, 1440 steps = 24 hours of 1-minute data, but here is revised to 60 again\n",
    "#total_elements = data.shape[0] * data.shape[1]\n",
    "#n_samples = data_test.shape[0] // n_steps \n",
    "#Reshape and apply mcar to the test set \n",
    "# #data_test=data_test[:n_samples*n_steps].reshape(n_samples,n_steps,n_features)\n",
    "\n",
    "#data_test_normalized=np.zeros_like(data_test)  # Initialize the normalized data array\n",
    "#for i in range(data_test.shape[2]):\n",
    "  # scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    # Flatten timesteps and samples for scaling\n",
    "  # data_test_normalized[:, :, i] = scaler.fit_transform(data_test[:, :, i].reshape(-1, 1)).reshape(data_test.shape[0], data_test.shape[1])\n",
    "  # scalers[i] = scaler  # Save scalers to inverse-transform later\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Config:\n",
    "    no_cuda = False\n",
    "    no_mps = False\n",
    "    seed = 1\n",
    "\n",
    "args=Config()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "use_mps = not args.no_mps and torch.backends.mps.is_available()\n",
    "\n",
    "if args.cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA\")\n",
    "elif use_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "\n",
    "#MLflow set up\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "mlflow.set_experiment(\"SAITS\")\n",
    "SAITS_run_name = \"SAITS\"\n",
    "\n",
    "with mlflow.start_run(run_name=SAITS_run_name) as run:\n",
    "# initialize the model (from example)\n",
    "    saits = SAITS(\n",
    "        n_steps=data_normalized.shape[1],\n",
    "        n_features=data_normalized.shape[2],\n",
    "        n_layers=2, #deep network(4) for long sequences, here is revised for 2 since it always shows cuda error \n",
    "        d_model=512,  #d_modle must equal n_heads * d_k\n",
    "        d_ffn=512,\n",
    "        n_heads=8,\n",
    "        d_k=64,\n",
    "        d_v=64,\n",
    "        dropout=0.1,\n",
    "        attn_dropout=0.1,\n",
    "        diagonal_attention_mask=True,  # otherwise the original self-attention mechanism will be applied\n",
    "        ORT_weight=1,  # you can adjust the weight values of arguments ORT_weight\n",
    "       # and MIT_weight to make the SAITS model focus more on one task. Usually you can just leave them to the default values, i.e. 1.\n",
    "        MIT_weight=1,\n",
    "        batch_size=5, #try with 5 to see if it works, was 4. \n",
    "        # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "        epochs=10, #try with 10 to see if it runs\n",
    "        # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "        # You can leave it to defualt as None to disable early stopping.\n",
    "        patience=4, #initially was 3, tried to increase to see more possibilities \n",
    "        # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "        # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "        optimizer=Adam(lr=1e-3),\n",
    "        # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "        # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "        # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "        num_workers=0,\n",
    "        # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "        # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "        device=device, \n",
    "        # set the path for saving tensorboard and trained model files \n",
    "        saving_path=\"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/best_model\",\n",
    "        # only save the best model after training finished.\n",
    "        # You can also set it as \"better\" to save models performing better ever during training.\n",
    "        model_saving_strategy=\"best\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Log model parameters\n",
    "mlflow.log_param(\"n_steps\", data_normalized.shape[1])\n",
    "mlflow.log_param(\"n_features\", data_normalized.shape[2])\n",
    "mlflow.log_param(\"batch_size\", 5)\n",
    "mlflow.log_param(\"epochs\", 10)\n",
    "mlflow.log_param(\"patience\", 4)\n",
    "\n",
    "\n",
    "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
    "#use original missigness for trainig, use the mcar masked points as ground truth\n",
    "saits.fit(train_set={\"X\": train_data}, val_set={\"X\": val_X_masked , \"X_ori\":val_X_ori })\n",
    "\n",
    "\n",
    "##drop null values in test set \n",
    "\n",
    "# Check for NaN values across the entire array\n",
    "#nan_mask = np.isnan(test_X).any(axis=(1, 2))\n",
    "\n",
    "# Filter out samples (rows) that contain NaN values\n",
    "#test_X_clean = test_X[~nan_mask]\n",
    "#test_X_ori = test_X_clean\n",
    "#test_X_masked, test_mask = mcar_f(test_X_clean)\n",
    "\n",
    "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
    "# Convert the numpy array to a dictionary\n",
    "test_set_dict = {\"X\": test_X_masked}\n",
    "test_imputation = saits.predict(test_set_dict)\n",
    "\n",
    "#change back to array for applying inverse scale later\n",
    "test_imputation_array = test_imputation[\"imputation\"]\n",
    "\n",
    "\n",
    "#clear cuda after prediction if on cuda\n",
    "\n",
    "# Ensure saits_imputation and test_X are numpy arrays\n",
    "#if not isinstance(saits_imputation, np.ndarray):\n",
    "    #saits_imputation = np.array(saits_imputation)\n",
    "#if not isinstance(test_X, np.ndarray):\n",
    "     #test_X = np.array(test_X)\n",
    "\n",
    "#Inverse Scale\n",
    "def inverse_scale(imputation, scalers):\n",
    "    n_features = imputation.shape[2]\n",
    "    imputation_denorm = np.empty_like(imputation)\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        imputation_denorm[:, :, i] = scalers[i].inverse_transform(imputation[:, :, i].reshape(-1, 1)).reshape(imputation.shape[0], imputation.shape[1])\n",
    "    \n",
    "    return imputation_denorm  # Move the return statement outside the loop\n",
    "\n",
    "    \n",
    "#Apply function to the dataset \n",
    "test_imputation_denorm=inverse_scale(test_imputation_array,scalers)\n",
    "test_X_ori_denorm=inverse_scale(test_X_ori,scalers)\n",
    "\n",
    "if args.cuda:\n",
    "    torch.cuda.empty_cache() \n",
    "\n",
    "\n",
    "#Calculate metrics per-feature: mean absolute error on the ground truth (artificially-missing values)\n",
    "mae_per_feature=[]\n",
    "for i in range(n_features):\n",
    "    #Extract imputation and ground truth for feature i\n",
    "    imputation_i=test_imputation_denorm[:,:,i]\n",
    "    ground_truth_i=test_X_ori_denorm[:,:,i]\n",
    "    mask_i=test_mask[:,:,i]\n",
    "    # Check for NaN values\n",
    "    if np.isnan(imputation_i).any() or np.isnan(ground_truth_i).any():\n",
    "        print(f\"NaN values detected in feature {i}\")\n",
    "        continue  # Skip this feature if NaN values are found\n",
    "    #Filter only artificially masked positions\n",
    "    mae_i=calc_mae(imputation_i,ground_truth_i,mask_i)\n",
    "    mae_per_feature.append(mae_i)\n",
    "    print(f\"MAE for {feature_names[i]}: {mae_i:.4f}\")\n",
    "    mlflow.log_metric(f\"MAE_{feature_names[i]}\", mae_i)\n",
    "\n",
    "\n",
    "# calculate average MAE \n",
    "avg_mae=np.mean(mae_per_feature)\n",
    "print(f\"Testing mean absolute error: {avg_mae:.4f}\")\n",
    "mlflow.log_metric(\"avg_mae\", avg_mae)\n",
    "\n",
    "mlflow.end_run()\n",
    "\n",
    "##Current error is on the cuda \n",
    "#test memory\n",
    "#def test_memory(in_size=100, out_size=10,hidden_size=100,optimizer_type=torch.optim.Adam, batch_size=1, device=0):\n",
    "   # sample_input=torch.randn(batch_size,in_size)\n",
    "  #  model=saits,\n",
    "   # optimizer=optimizer_type(model.parameters(),lr=1e-3)\n",
    "   # print(\"Beginning mem:\",torch.cuda.memory_allocated(device))\n",
    "   # model.to(device)\n",
    "   # print()\n",
    "   # for i in range(3):\n",
    "   #     print(\"Iteration\",i)\n",
    "   #     a=torch.cuda.memory_allocated(device)\n",
    "   #     out=model(sample_input.to(device)).sum()\n",
    "   #     b=torch.cuda.memory_allocated(device)\n",
    "   #     print(\"2-After forward pass\", torch.cuda.memory_allocated(device))\n",
    "   #     print(\"2-Memory consumed by forward pass\", b-a)\n",
    "   #     out.backward()\n",
    "   #     print(\"3-After backward pass\", torch.cuda.memory_allocated(device))\n",
    "   #     optimizer.step()\n",
    "   #     print(\"4-After optimizer step\", torch.cuda.memory_allocated(device))\n",
    "\n",
    "##To do: log mlflow, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64122.002728967054"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[\"pr_air_scav_ecs\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0230871705516225"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[\"te_air_scav_rec\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2105815625902985"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[\"fr_eng_ecs\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3acf5b-5e03-4c4f-a09c-2ed345f5ad38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "2025-05-13 22:35:20 [INFO]: Using the given device: cpu\n",
      "2025-05-13 22:35:20 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/best_model/20250513_T223520\n",
      "2025-05-13 22:35:20 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/best_model/20250513_T223520/tensorboard\n",
      "2025-05-13 22:35:20 [INFO]: Using customized MAE as the training loss function.\n",
      "2025-05-13 22:35:20 [INFO]: Using customized MSE as the validation metric function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column time has 0 NaN values\n",
      "Column time has 0.0 Missing_rate\n",
      "Column fr_eng has 0 NaN values\n",
      "Column fr_eng has 0.0 Missing_rate\n",
      "Column te_exh_cyl_out__0 has 73 NaN values\n",
      "Column te_exh_cyl_out__0 has 0.0006111906496203082 Missing_rate\n",
      "Column pd_air_ic__0 has 73 NaN values\n",
      "Column pd_air_ic__0 has 0.0006111906496203082 Missing_rate\n",
      "Column pr_exh_turb_out__0 has 119439 NaN values\n",
      "Column pr_exh_turb_out__0 has 1.0 Missing_rate\n",
      "Column te_air_ic_out__0 has 73 NaN values\n",
      "Column te_air_ic_out__0 has 0.0006111906496203082 Missing_rate\n",
      "Column te_seawater has 73 NaN values\n",
      "Column te_seawater has 0.0006111906496203082 Missing_rate\n",
      "Column te_air_comp_in_a__0 has 119439 NaN values\n",
      "Column te_air_comp_in_a__0 has 1.0 Missing_rate\n",
      "Column te_air_comp_in_b__0 has 119439 NaN values\n",
      "Column te_air_comp_in_b__0 has 1.0 Missing_rate\n",
      "Column fr_tc__0 has 119439 NaN values\n",
      "Column fr_tc__0 has 1.0 Missing_rate\n",
      "Column pr_baro has 73 NaN values\n",
      "Column pr_baro has 0.0006111906496203082 Missing_rate\n",
      "Column pd_air_ic__0_1 has 73 NaN values\n",
      "Column pd_air_ic__0_1 has 0.0006111906496203082 Missing_rate\n",
      "Column pr_exh_rec has 73 NaN values\n",
      "Column pr_exh_rec has 0.0006111906496203082 Missing_rate\n",
      "Column te_exh_turb_in__0 has 119439 NaN values\n",
      "Column te_exh_turb_in__0 has 1.0 Missing_rate\n",
      "Column te_exh_turb_out__0 has 119439 NaN values\n",
      "Column te_exh_turb_out__0 has 1.0 Missing_rate\n",
      "Column bo_aux_blower_running has 0 NaN values\n",
      "Column bo_aux_blower_running has 0.0 Missing_rate\n",
      "Column re_eng_load has 173 NaN values\n",
      "Column re_eng_load has 0.0014484381148536073 Missing_rate\n",
      "Column pr_air_scav_ecs has 0 NaN values\n",
      "Column pr_air_scav_ecs has 0.0 Missing_rate\n",
      "Column pr_air_scav has 0 NaN values\n",
      "Column pr_air_scav has 0.0 Missing_rate\n",
      "Column te_air_scav_rec has 73 NaN values\n",
      "Column te_air_scav_rec has 0.0006111906496203082 Missing_rate\n",
      "Column te_air_ic_out__0_1 has 73 NaN values\n",
      "Column te_air_ic_out__0_1 has 0.0006111906496203082 Missing_rate\n",
      "Column pr_cyl_comp__0 has 173 NaN values\n",
      "Column pr_cyl_comp__0 has 0.0014484381148536073 Missing_rate\n",
      "Column pr_cyl_max__0 has 173 NaN values\n",
      "Column pr_cyl_max__0 has 0.0014484381148536073 Missing_rate\n",
      "Column se_mip__0 has 173 NaN values\n",
      "Column se_mip__0 has 0.0014484381148536073 Missing_rate\n",
      "Column te_exh_cyl_out__0_1 has 73 NaN values\n",
      "Column te_exh_cyl_out__0_1 has 0.0006111906496203082 Missing_rate\n",
      "Column fr_eng_setpoint has 0 NaN values\n",
      "Column fr_eng_setpoint has 0.0 Missing_rate\n",
      "Column te_air_scav_rec_iso has 51235 NaN values\n",
      "Column te_air_scav_rec_iso has 0.4289637388122807 Missing_rate\n",
      "Column pr_cyl_max_mv_iso has 51385 NaN values\n",
      "Column pr_cyl_max_mv_iso has 0.4302196100101307 Missing_rate\n",
      "Column pr_cyl_comp_mv_iso has 51385 NaN values\n",
      "Column pr_cyl_comp_mv_iso has 0.4302196100101307 Missing_rate\n",
      "Column fr_eng_ecs has 0 NaN values\n",
      "Column fr_eng_ecs has 0.0 Missing_rate\n",
      "Column pr_air_scav_iso has 51385 NaN values\n",
      "Column pr_air_scav_iso has 0.4302196100101307 Missing_rate\n",
      "Column engine_type_G95ME-C10.5-GI-EGRTC has 0 NaN values\n",
      "Column engine_type_G95ME-C10.5-GI-EGRTC has 0.0 Missing_rate\n",
      "Original size:119439, Sampled size: 39813\n",
      "Reshaped data:(39813, 31)\n",
      "Using CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 22:35:21 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 6,402,976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸƒ View run SAITS at: http://localhost:5000/#/experiments/1/runs/f1af00359cf949f5ae1d71fc286cc84d\n",
      "ğŸ§ª View experiment at: http://localhost:5000/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 22:36:06 [INFO]: Epoch 001 - training loss (MAE): 0.5610, validation MSE: 0.0698\n",
      "2025-05-13 22:36:53 [INFO]: Epoch 002 - training loss (MAE): 0.3486, validation MSE: 0.0634\n",
      "2025-05-13 22:37:42 [INFO]: Epoch 003 - training loss (MAE): 0.3429, validation MSE: 0.0599\n",
      "2025-05-13 22:38:30 [INFO]: Epoch 004 - training loss (MAE): 0.3448, validation MSE: 0.0667\n",
      "2025-05-13 22:39:25 [INFO]: Epoch 005 - training loss (MAE): 0.3420, validation MSE: 0.0675\n",
      "2025-05-13 22:40:22 [INFO]: Epoch 006 - training loss (MAE): 0.3421, validation MSE: 0.0633\n",
      "2025-05-13 22:41:17 [INFO]: Epoch 007 - training loss (MAE): 0.3421, validation MSE: 0.0640\n",
      "2025-05-13 22:42:14 [INFO]: Epoch 008 - training loss (MAE): 0.3426, validation MSE: 0.0634\n",
      "2025-05-13 22:43:15 [INFO]: Epoch 009 - training loss (MAE): 0.3398, validation MSE: 0.0681\n",
      "2025-05-13 22:43:15 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-05-13 22:43:15 [INFO]: Finished training. The best model is from epoch#3.\n",
      "2025-05-13 22:43:15 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/best_model/20250513_T223520/SAITS.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for fr_eng: 0.1329\n",
      "NaN values detected in feature 1\n",
      "NaN values detected in feature 2\n",
      "NaN values detected in feature 3\n",
      "NaN values detected in feature 4\n",
      "NaN values detected in feature 5\n",
      "NaN values detected in feature 6\n",
      "NaN values detected in feature 7\n",
      "NaN values detected in feature 8\n",
      "NaN values detected in feature 9\n",
      "NaN values detected in feature 10\n",
      "NaN values detected in feature 11\n",
      "NaN values detected in feature 12\n",
      "NaN values detected in feature 13\n",
      "MAE for bo_aux_blower_running: 0.5469\n",
      "NaN values detected in feature 15\n",
      "MAE for pr_air_scav_ecs: 36343.4720\n",
      "MAE for pr_air_scav: 36151.9926\n",
      "NaN values detected in feature 18\n",
      "NaN values detected in feature 19\n",
      "NaN values detected in feature 20\n",
      "NaN values detected in feature 21\n",
      "NaN values detected in feature 22\n",
      "NaN values detected in feature 23\n",
      "MAE for fr_eng_setpoint: 0.1322\n",
      "NaN values detected in feature 25\n",
      "NaN values detected in feature 26\n",
      "NaN values detected in feature 27\n",
      "MAE for fr_eng_ecs: 0.1466\n",
      "NaN values detected in feature 29\n",
      "MAE for engine_type_G95ME-C10.5-GI-EGRTC: 0.0034\n",
      "Testing mean absolute error: 10356.6324\n",
      "ğŸƒ View run chill-eel-176 at: http://localhost:5000/#/experiments/1/runs/94a2f8fad3144b81b43dfd5448600ddd\n",
      "ğŸ§ª View experiment at: http://localhost:5000/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "#Import Pypots Library\n",
    "from pypots.optim import Adam\n",
    "from pypots.imputation import SAITS\n",
    "#from pypots.utils.metrics import calc_mae\n",
    "from pypots.nn.functional import calc_mae\n",
    "\n",
    "\n",
    "import argparse\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import data_insight\n",
    "from data_insight import setup_duckdb\n",
    "from duckdb import DuckDBPyConnection as DuckDB\n",
    "from duckdb import DuckDBPyRelation as Relation\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "from duckdb import DuckDBPyConnection as DuckDB\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import optuna \n",
    "from optuna.visualization import plot_optimization_history\n",
    "\n",
    "\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, Dataset\n",
    "from pygrinder.missing_completely_at_random import mcar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import sensor_imputation_thesis.shared.load_data as load\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "#PatchTST might be an ideal choise if SAITS is too slow \n",
    "\n",
    "##Drop columns with different indexes while loading data.. Or the mean values \n",
    "\n",
    "df=pd.read_parquet(\"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/Newdataframeforpypots.parquet\")\n",
    "\n",
    "len(df)\n",
    "\n",
    "#current length of the dataframe is 119439\n",
    "\n",
    "# Check nan values in each column\n",
    "for col in df.columns:\n",
    "    print(f\"Column {col} has {df[col].isna().sum()} NaN values\")\n",
    "    missing_rate=df[col].isna().sum()/len(df[col])\n",
    "    print(f\"Column {col} has {missing_rate} Missing_rate\")\n",
    "\n",
    "\n",
    "#Try with smaller dataset, size 4000\n",
    "##SAMPLE the percengtage of the dataset, df.sample (averagely pick samples)\n",
    "#not df.sample cuz it will randomly select \n",
    "original_size=len(df)\n",
    "desired_fraction=0.3\n",
    "step=int(1/desired_fraction) #step_size=10 (sample every 10th (3/10) minute)\n",
    "\n",
    "#Systematic sampling: Start at a random offset to avoid bias \n",
    "start=np.random.randint(0,step) #Random start between 0-9\n",
    "df1=df.iloc[start::step].reset_index(drop=True)\n",
    "\n",
    "print(f\"Original size:{len(df)}, Sampled size: {len(df1)}\")\n",
    "\n",
    "\n",
    "#df1=df[:10000]\n",
    "\n",
    "\n",
    "# Custom Dataset class\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Data processing code\n",
    "sensor_cols = [col for col in df1.columns if col != \"time\"]\n",
    "data = df1[sensor_cols].values\n",
    "\n",
    "#Â¤get feature names for printing mae later \n",
    "feature_names=df1[sensor_cols].columns.tolist()\n",
    "\n",
    "## Convert data to 3D arrays of shape n_samples, n_timesteps, n_features, X_ori refers to the original data without missing values \n",
    "## Reconstruct all columns simultaneously  #num_features: 119\n",
    "n_features = data.shape[1]  # exclude the time column\n",
    "n_steps = 30 #60 (was 60 previously) #(TRY TO CHANGE HERE)  # # window length, 1440 steps = 24 hours of 1-minute data, but here is revised to 60 again\n",
    "#total_elements = data.shape[0] * data.shape[1]\n",
    "n_samples = data.shape[0] // n_steps \n",
    "\n",
    "\n",
    "\n",
    "# Reshape to (n_samples // n_steps, n_steps, n_features)\n",
    "#data_reshaped = data.reshape((n_samples, n_steps, n_features))\n",
    "data_reshaped=data[:n_samples*n_steps].reshape(n_samples,n_steps,n_features)\n",
    "print(f\"Reshaped data:{data.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "##Normalization is important because of the nature of mse calculation of saits, columns with large \n",
    "#values dominate the loss, making metrics meaningless. SAITS computes MSE/MAE column-wise and averages \n",
    "#them across all columns \n",
    "#  Apply minmax scaler here \n",
    "#normalize each feature independently\n",
    "scalers={}\n",
    "data_normalized=data.copy()\n",
    "\n",
    "data_normalized = np.zeros_like(data_reshaped)  # Initialize the normalized data array\n",
    "scalers = {}\n",
    "\n",
    "for i in range(data_reshaped.shape[2]):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    # Flatten timesteps and samples for scaling\n",
    "    data_normalized[:, :, i] = scaler.fit_transform(data_reshaped[:, :, i].reshape(-1, 1)).reshape(data_reshaped.shape[0], data_reshaped.shape[1])\n",
    "    scalers[i] = scaler  # Save scalers to inverse-transform later\n",
    "\n",
    "\n",
    "\n",
    "#Split into train, test, val\n",
    "\n",
    "train_size = int(0.6 * len(data_normalized))\n",
    "val_size = int(0.2 * len(data_normalized))\n",
    "test_size = len(data_normalized) - train_size - val_size\n",
    "\n",
    "train_data = data_normalized[:train_size]\n",
    "val_data = data_normalized[train_size:train_size + val_size]\n",
    "test_data= data_normalized[train_size + val_size:]\n",
    "\n",
    "#Optional: Artificially mask. Mask 20% of the data (MIT part)\n",
    "def mcar_f(X, mask_ratio=0.2):\n",
    "    \"\"\"Apply MCAR only to observed values.\"\"\"\n",
    "    observed_mask=~np.isnan(X) #find observed positions\n",
    "    artificial_mask=mcar(X,mask_ratio).astype(bool) #generate MCAR mask, cast to boolean\n",
    "    #combine masks \n",
    "    combined_mask=observed_mask & artificial_mask\n",
    "\n",
    "    #Apply masking\n",
    "    X_masked=X.copy()\n",
    "    X_masked[combined_mask]=np.nan\n",
    "    return X_masked,combined_mask\n",
    "\n",
    "\n",
    "#Use mcar on validation data \n",
    "val_X_masked, val_mask =mcar_f(val_data)\n",
    "val_X_ori=val_data.copy() #Ground truth\n",
    "\n",
    "test_X_masked, test_mask =mcar_f(test_data)\n",
    "test_X_ori=test_data.copy() #ground truth\n",
    "\n",
    "#?? Problem: Can't have the best input for testing\n",
    "#1.Create synthetic test_data cuz if I drop nan values for test set, there's basically nothing left\n",
    "#synthetic_data=np.random.randn(n_samples,n_steps,n_features)\n",
    "#test_X_masked,test_mask=mcar_f(synthetic_data)\n",
    "#test_X_ori=synthetic_data.copy() #Ground truth\n",
    "\n",
    "# 2, Ensure no NaN values in synthetic data\n",
    "#test_X_masked = np.nan_to_num(test_X_masked, nan=np.nanmean(test_X_masked))\n",
    "#test_X_ori = np.nan_to_num(test_X_ori, nan=np.nanmean(test_X_ori))\n",
    "\n",
    "\n",
    "#3. Set aside a part of dataset for test_set\n",
    "#df_test=df1[6000:8000]  #Even with 10000 rows of dataset fot testing, after dropping nan value in rows, there is none left \n",
    "#2. Drop nan rows \n",
    "#f_test=df_test.dropna(axis=0,how='all')\n",
    "#print(\"Size of the test set:\", len(df_test)) \n",
    "#data_test=df_test[sensor_cols].values\n",
    "\n",
    "\n",
    "\n",
    "#n_features = data_test.shape[1]  # exclude the time column\n",
    "#n_steps = 60 #(TRY TO CHANGE HERE)  # # window length, 1440 steps = 24 hours of 1-minute data, but here is revised to 60 again\n",
    "#total_elements = data.shape[0] * data.shape[1]\n",
    "#n_samples = data_test.shape[0] // n_steps \n",
    "#Reshape and apply mcar to the test set \n",
    "# #data_test=data_test[:n_samples*n_steps].reshape(n_samples,n_steps,n_features)\n",
    "\n",
    "#data_test_normalized=np.zeros_like(data_test)  # Initialize the normalized data array\n",
    "#for i in range(data_test.shape[2]):\n",
    "  # scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    # Flatten timesteps and samples for scaling\n",
    "  # data_test_normalized[:, :, i] = scaler.fit_transform(data_test[:, :, i].reshape(-1, 1)).reshape(data_test.shape[0], data_test.shape[1])\n",
    "  # scalers[i] = scaler  # Save scalers to inverse-transform later\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Config:\n",
    "    no_cuda = False\n",
    "    no_mps = False\n",
    "    seed = 1\n",
    "\n",
    "args=Config()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "use_mps = not args.no_mps and torch.backends.mps.is_available()\n",
    "\n",
    "if args.cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA\")\n",
    "elif use_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "\n",
    "#MLflow set up\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "mlflow.set_experiment(\"SAITS\")\n",
    "SAITS_run_name = \"SAITS\"\n",
    "\n",
    "with mlflow.start_run(run_name=SAITS_run_name) as run:\n",
    "# initialize the model (from example)\n",
    "    saits = SAITS(\n",
    "        n_steps=data_normalized.shape[1],\n",
    "        n_features=data_normalized.shape[2],\n",
    "        n_layers=2, #deep network(4) for long sequences, here is revised for 2 since it always shows cuda error \n",
    "        d_model=512,  #d_modle must equal n_heads * d_k\n",
    "        d_ffn=512,\n",
    "        n_heads=8,\n",
    "        d_k=64,\n",
    "        d_v=64,\n",
    "        dropout=0.1,\n",
    "        attn_dropout=0.1,\n",
    "        diagonal_attention_mask=True,  # otherwise the original self-attention mechanism will be applied\n",
    "        ORT_weight=1,  # you can adjust the weight values of arguments ORT_weight\n",
    "       # and MIT_weight to make the SAITS model focus more on one task. Usually you can just leave them to the default values, i.e. 1.\n",
    "        MIT_weight=1,\n",
    "        batch_size=5, #try with 5 to see if it works, was 4. \n",
    "        # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "        epochs=10, #try with 10 to see if it runs\n",
    "        # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "        # You can leave it to defualt as None to disable early stopping.\n",
    "        patience=6, #initially was 3, tried to increase to see more possibilities \n",
    "        # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "        # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "        optimizer=Adam(lr=1e-2),\n",
    "        # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "        # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "        # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "        num_workers=0,\n",
    "        # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "        # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "        device=device, \n",
    "        # set the path for saving tensorboard and trained model files \n",
    "        saving_path=\"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/best_model\",\n",
    "        # only save the best model after training finished.\n",
    "        # You can also set it as \"better\" to save models performing better ever during training.\n",
    "        model_saving_strategy=\"best\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Log model parameters\n",
    "mlflow.log_param(\"n_steps\", data_normalized.shape[1])\n",
    "mlflow.log_param(\"n_features\", data_normalized.shape[2])\n",
    "mlflow.log_param(\"batch_size\", 5)\n",
    "mlflow.log_param(\"epochs\", 10)\n",
    "mlflow.log_param(\"patience\", 4)\n",
    "\n",
    "\n",
    "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
    "#use original missigness for trainig, use the mcar masked points as ground truth\n",
    "saits.fit(train_set={\"X\": train_data}, val_set={\"X\": val_X_masked , \"X_ori\":val_X_ori })\n",
    "\n",
    "\n",
    "##drop null values in test set \n",
    "\n",
    "# Check for NaN values across the entire array\n",
    "#nan_mask = np.isnan(test_X).any(axis=(1, 2))\n",
    "\n",
    "# Filter out samples (rows) that contain NaN values\n",
    "#test_X_clean = test_X[~nan_mask]\n",
    "#test_X_ori = test_X_clean\n",
    "#test_X_masked, test_mask = mcar_f(test_X_clean)\n",
    "\n",
    "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
    "# Convert the numpy array to a dictionary\n",
    "test_set_dict = {\"X\": test_X_masked}\n",
    "test_imputation = saits.predict(test_set_dict)\n",
    "\n",
    "#change back to array for applying inverse scale later\n",
    "test_imputation_array = test_imputation[\"imputation\"]\n",
    "\n",
    "\n",
    "#clear cuda after prediction if on cuda\n",
    "\n",
    "# Ensure saits_imputation and test_X are numpy arrays\n",
    "#if not isinstance(saits_imputation, np.ndarray):\n",
    "    #saits_imputation = np.array(saits_imputation)\n",
    "#if not isinstance(test_X, np.ndarray):\n",
    "     #test_X = np.array(test_X)\n",
    "\n",
    "#Inverse Scale\n",
    "def inverse_scale(imputation, scalers):\n",
    "    n_features = imputation.shape[2]\n",
    "    imputation_denorm = np.empty_like(imputation)\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        imputation_denorm[:, :, i] = scalers[i].inverse_transform(imputation[:, :, i].reshape(-1, 1)).reshape(imputation.shape[0], imputation.shape[1])\n",
    "    \n",
    "    return imputation_denorm  # Move the return statement outside the loop\n",
    "\n",
    "    \n",
    "#Apply function to the dataset \n",
    "test_imputation_denorm=inverse_scale(test_imputation_array,scalers)\n",
    "test_X_ori_denorm=inverse_scale(test_X_ori,scalers)\n",
    "\n",
    "if args.cuda:\n",
    "    torch.cuda.empty_cache() \n",
    "\n",
    "\n",
    "#Calculate metrics per-feature: mean absolute error on the ground truth (artificially-missing values)\n",
    "mae_per_feature=[]\n",
    "for i in range(n_features):\n",
    "    #Extract imputation and ground truth for feature i\n",
    "    imputation_i=test_imputation_denorm[:,:,i]\n",
    "    ground_truth_i=test_X_ori_denorm[:,:,i]\n",
    "    mask_i=test_mask[:,:,i]\n",
    "    # Check for NaN values\n",
    "    if np.isnan(imputation_i).any() or np.isnan(ground_truth_i).any():\n",
    "        print(f\"NaN values detected in feature {i}\")\n",
    "        continue  # Skip this feature if NaN values are found\n",
    "    #Filter only artificially masked positions\n",
    "    mae_i=calc_mae(imputation_i,ground_truth_i,mask_i)\n",
    "    mae_per_feature.append(mae_i)\n",
    "    print(f\"MAE for {feature_names[i]}: {mae_i:.4f}\")\n",
    "    mlflow.log_metric(f\"MAE_{feature_names[i]}\", mae_i)\n",
    "\n",
    "\n",
    "# calculate average MAE \n",
    "avg_mae=np.mean(mae_per_feature)\n",
    "print(f\"Testing mean absolute error: {avg_mae:.4f}\")\n",
    "mlflow.log_metric(\"avg_mae\", avg_mae)\n",
    "\n",
    "mlflow.end_run()\n",
    "\n",
    "##Current error is on the cuda \n",
    "#test memory\n",
    "#def test_memory(in_size=100, out_size=10,hidden_size=100,optimizer_type=torch.optim.Adam, batch_size=1, device=0):\n",
    "   # sample_input=torch.randn(batch_size,in_size)\n",
    "  #  model=saits,\n",
    "   # optimizer=optimizer_type(model.parameters(),lr=1e-3)\n",
    "   # print(\"Beginning mem:\",torch.cuda.memory_allocated(device))\n",
    "   # model.to(device)\n",
    "   # print()\n",
    "   # for i in range(3):\n",
    "   #     print(\"Iteration\",i)\n",
    "   #     a=torch.cuda.memory_allocated(device)\n",
    "   #     out=model(sample_input.to(device)).sum()\n",
    "   #     b=torch.cuda.memory_allocated(device)\n",
    "   #     print(\"2-After forward pass\", torch.cuda.memory_allocated(device))\n",
    "   #     print(\"2-Memory consumed by forward pass\", b-a)\n",
    "   #     out.backward()\n",
    "   #     print(\"3-After backward pass\", torch.cuda.memory_allocated(device))\n",
    "   #     optimizer.step()\n",
    "   #     print(\"4-After optimizer step\", torch.cuda.memory_allocated(device))\n",
    "\n",
    "##To do: log mlflow, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     time    fr_eng  te_exh_cyl_out__0  pd_air_ic__0  \\\n",
      "0     2023-10-01 00:01:00  0.732878             510.15         420.0   \n",
      "1     2023-10-01 00:04:00  0.732730             510.15         430.0   \n",
      "2     2023-10-01 00:07:00  0.734522             510.15         460.0   \n",
      "3     2023-10-01 00:10:00  0.733247             510.15         430.0   \n",
      "4     2023-10-01 00:13:00  0.733089             510.15         430.0   \n",
      "...                   ...       ...                ...           ...   \n",
      "39808 2024-01-31 06:16:00  0.350018             406.15         280.0   \n",
      "39809 2024-01-31 06:19:00  0.350902             405.15         300.0   \n",
      "39810 2024-01-31 06:22:00  0.350278             402.15         300.0   \n",
      "39811 2024-01-31 06:25:00  0.348935             401.15         280.0   \n",
      "39812 2024-01-31 06:28:00  0.186159             399.15         300.0   \n",
      "\n",
      "       pr_exh_turb_out__0  te_air_ic_out__0  te_seawater  te_air_comp_in_a__0  \\\n",
      "0                     NaN            308.95       273.15                  NaN   \n",
      "1                     NaN            308.75       273.15                  NaN   \n",
      "2                     NaN            308.65       273.15                  NaN   \n",
      "3                     NaN            308.65       273.15                  NaN   \n",
      "4                     NaN            308.65       273.15                  NaN   \n",
      "...                   ...               ...          ...                  ...   \n",
      "39808                 NaN            302.15       273.15                  NaN   \n",
      "39809                 NaN            301.85       273.15                  NaN   \n",
      "39810                 NaN            301.65       273.15                  NaN   \n",
      "39811                 NaN            301.45       273.15                  NaN   \n",
      "39812                 NaN            301.25       273.15                  NaN   \n",
      "\n",
      "       te_air_comp_in_b__0  fr_tc__0  ...  pr_cyl_max__0      se_mip__0  \\\n",
      "0                      NaN       NaN  ...   9.274421e+06  560138.845444   \n",
      "1                      NaN       NaN  ...   9.308612e+06  560386.896133   \n",
      "2                      NaN       NaN  ...   9.535269e+06  579738.473892   \n",
      "3                      NaN       NaN  ...   9.219550e+06  554740.571976   \n",
      "4                      NaN       NaN  ...   9.413393e+06  570139.646530   \n",
      "...                    ...       ...  ...            ...            ...   \n",
      "39808                  NaN       NaN  ...   7.244995e+06  227659.106255   \n",
      "39809                  NaN       NaN  ...   7.332955e+06  232482.528687   \n",
      "39810                  NaN       NaN  ...   7.255197e+06  226882.481575   \n",
      "39811                  NaN       NaN  ...   7.217973e+06  227513.623238   \n",
      "39812                  NaN       NaN  ...   7.343158e+06  236072.421074   \n",
      "\n",
      "       te_exh_cyl_out__0_1  fr_eng_setpoint  te_air_scav_rec_iso  \\\n",
      "0                   510.15         0.733310                  NaN   \n",
      "1                   510.15         0.733310                  NaN   \n",
      "2                   510.15         0.733310                  NaN   \n",
      "3                   510.15         0.733310                  NaN   \n",
      "4                   510.15         0.733310                  NaN   \n",
      "...                    ...              ...                  ...   \n",
      "39808               406.15         0.349989           307.164837   \n",
      "39809               405.15         0.349989           307.164837   \n",
      "39810               402.15         0.349989           307.164837   \n",
      "39811               401.15         0.349989           307.164837   \n",
      "39812               399.15         0.000000           307.164837   \n",
      "\n",
      "       pr_cyl_max_mv_iso  pr_cyl_comp_mv_iso  fr_eng_ecs  pr_air_scav_iso  \\\n",
      "0                    NaN                 NaN    0.732878              NaN   \n",
      "1                    NaN                 NaN    0.732730              NaN   \n",
      "2                    NaN                 NaN    0.734522              NaN   \n",
      "3                    NaN                 NaN    0.733247              NaN   \n",
      "4                    NaN                 NaN    0.733089              NaN   \n",
      "...                  ...                 ...         ...              ...   \n",
      "39808       7.089159e+06        5.445148e+06    0.350018      5045.522236   \n",
      "39809       7.136279e+06        5.446973e+06    0.350902      4926.913585   \n",
      "39810       7.073738e+06        5.434143e+06    0.350278      4483.703945   \n",
      "39811       7.052718e+06        5.430939e+06    0.348935      4576.780253   \n",
      "39812       7.149100e+06        5.433145e+06    0.186159      4342.364566   \n",
      "\n",
      "       engine_type_G95ME-C10.5-GI-EGRTC  \n",
      "0                                   1.0  \n",
      "1                                   1.0  \n",
      "2                                   1.0  \n",
      "3                                   1.0  \n",
      "4                                   1.0  \n",
      "...                                 ...  \n",
      "39808                               1.0  \n",
      "39809                               1.0  \n",
      "39810                               1.0  \n",
      "39811                               1.0  \n",
      "39812                               1.0  \n",
      "\n",
      "[39813 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for fr_eng: 0.1329\n",
      "MAE for te_exh_cyl_out__0: 20.2169\n",
      "MAE for pd_air_ic__0: 260.4531\n",
      "MAE for pr_exh_turb_out__0: nan\n",
      "MAE for te_air_ic_out__0: 1.9398\n",
      "MAE for te_seawater: 0.0001\n",
      "MAE for te_air_comp_in_a__0: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-6a8f46eb636a>:8: RuntimeWarning: Mean of empty slice.\n",
      "  mae = np.abs(imputation[valid_mask] - ground_truth[valid_mask]).mean()\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for te_air_comp_in_b__0: nan\n",
      "MAE for fr_tc__0: nan\n",
      "MAE for pr_baro: 826.0051\n",
      "MAE for pd_air_ic__0_1: 267.2695\n",
      "MAE for pr_exh_rec: 32717.8773\n",
      "MAE for te_exh_turb_in__0: nan\n",
      "MAE for te_exh_turb_out__0: nan\n",
      "MAE for bo_aux_blower_running: 0.5469\n",
      "MAE for re_eng_load: 0.1117\n",
      "MAE for pr_air_scav_ecs: 36343.4720\n",
      "MAE for pr_air_scav: 36151.9926\n",
      "MAE for te_air_scav_rec: 4.0500\n",
      "MAE for te_air_ic_out__0_1: 1.9919\n",
      "MAE for pr_cyl_comp__0: 2055017.6472\n",
      "MAE for pr_cyl_max__0: 2345898.8432\n",
      "MAE for se_mip__0: 179047.5526\n",
      "MAE for te_exh_cyl_out__0_1: 18.8740\n",
      "MAE for fr_eng_setpoint: 0.1322\n",
      "MAE for te_air_scav_rec_iso: 2.1861\n",
      "MAE for pr_cyl_max_mv_iso: 2484755.9879\n",
      "MAE for pr_cyl_comp_mv_iso: 2116437.2242\n",
      "MAE for fr_eng_ecs: 0.1466\n",
      "MAE for pr_air_scav_iso: 28296.0174\n",
      "MAE for engine_type_G95ME-C10.5-GI-EGRTC: 0.0034\n",
      "Testing mean absolute error: nan\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import mlflow\n",
    "\n",
    "# Function to calculate MAE while ignoring NaNs\n",
    "def calc_mae_ignore_nan(imputation, ground_truth, mask):\n",
    "    # Apply mask to ignore NaNs\n",
    "    valid_mask = ~np.isnan(imputation) & ~np.isnan(ground_truth) & mask\n",
    "    mae = np.abs(imputation[valid_mask] - ground_truth[valid_mask]).mean()\n",
    "    return mae\n",
    "\n",
    "# Calculate metrics per-feature: mean absolute error on the ground truth (artificially-missing values)\n",
    "mae_per_feature = []\n",
    "for i in range(n_features):\n",
    "    # Extract imputation and ground truth for feature i\n",
    "    imputation_i = test_imputation_denorm[:, :, i]\n",
    "    ground_truth_i = test_X_ori_denorm[:, :, i]\n",
    "    mask_i = test_mask[:, :, i]\n",
    "    \n",
    "    # Calculate MAE while ignoring NaNs\n",
    "    mae_i = calc_mae_ignore_nan(imputation_i, ground_truth_i, mask_i)\n",
    "    mae_per_feature.append(mae_i)\n",
    "    print(f\"MAE for {feature_names[i]}: {mae_i:.4f}\")\n",
    "    mlflow.log_metric(f\"MAE_{feature_names[i]}\", mae_i)\n",
    "\n",
    "# Calculate average MAE\n",
    "avg_mae = np.mean(mae_per_feature)\n",
    "print(f\"Testing mean absolute error: {avg_mae:.4f}\")\n",
    "mlflow.log_metric(\"avg_mae\", avg_mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.023644989747263"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[\"te_air_scav_rec\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "902.3217968207963"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[\"pr_baro\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for fr_eng: 0.1329\n",
      "Performance Ratio for fr_eng: 0.7238\n",
      "MAE for te_exh_cyl_out__0: 20.2169\n",
      "Performance Ratio for te_exh_cyl_out__0: 0.6155\n",
      "MAE for pd_air_ic__0: 260.4531\n",
      "Performance Ratio for pd_air_ic__0: 0.9337\n",
      "MAE for pr_exh_turb_out__0: nan\n",
      "Performance Ratio for pr_exh_turb_out__0: nan\n",
      "MAE for te_air_ic_out__0: 1.9398\n",
      "Performance Ratio for te_air_ic_out__0: 0.7705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-528c8751e9ad>:5: RuntimeWarning: Mean of empty slice.\n",
      "  mae = np.abs(imputation[valid_mask] - ground_truth[valid_mask]).mean()\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "<ipython-input-21-528c8751e9ad>:25: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  performance_ratio_i = mae_i / std_dev_i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for te_seawater: 0.0001\n",
      "Performance Ratio for te_seawater: inf\n",
      "MAE for te_air_comp_in_a__0: nan\n",
      "Performance Ratio for te_air_comp_in_a__0: nan\n",
      "MAE for te_air_comp_in_b__0: nan\n",
      "Performance Ratio for te_air_comp_in_b__0: nan\n",
      "MAE for fr_tc__0: nan\n",
      "Performance Ratio for fr_tc__0: nan\n",
      "MAE for pr_baro: 826.0051\n",
      "Performance Ratio for pr_baro: 1.1394\n",
      "MAE for pd_air_ic__0_1: 267.2695\n",
      "Performance Ratio for pd_air_ic__0_1: 0.9582\n",
      "MAE for pr_exh_rec: 32717.8773\n",
      "Performance Ratio for pr_exh_rec: 0.7650\n",
      "MAE for te_exh_turb_in__0: nan\n",
      "Performance Ratio for te_exh_turb_in__0: nan\n",
      "MAE for te_exh_turb_out__0: nan\n",
      "Performance Ratio for te_exh_turb_out__0: nan\n",
      "MAE for bo_aux_blower_running: 0.5469\n",
      "Performance Ratio for bo_aux_blower_running: 1.2748\n",
      "MAE for re_eng_load: 0.1117\n",
      "Performance Ratio for re_eng_load: 0.7356\n",
      "MAE for pr_air_scav_ecs: 36343.4720\n",
      "Performance Ratio for pr_air_scav_ecs: 0.7623\n",
      "MAE for pr_air_scav: 36151.9926\n",
      "Performance Ratio for pr_air_scav: 0.7582\n",
      "MAE for te_air_scav_rec: 4.0500\n",
      "Performance Ratio for te_air_scav_rec: 1.3167\n",
      "MAE for te_air_ic_out__0_1: 1.9919\n",
      "Performance Ratio for te_air_ic_out__0_1: 0.7912\n",
      "MAE for pr_cyl_comp__0: 2055017.6472\n",
      "Performance Ratio for pr_cyl_comp__0: 0.7813\n",
      "MAE for pr_cyl_max__0: 2345898.8432\n",
      "Performance Ratio for pr_cyl_max__0: 0.7720\n",
      "MAE for se_mip__0: 179047.5526\n",
      "Performance Ratio for se_mip__0: 0.7012\n",
      "MAE for te_exh_cyl_out__0_1: 18.8740\n",
      "Performance Ratio for te_exh_cyl_out__0_1: 0.5746\n",
      "MAE for fr_eng_setpoint: 0.1322\n",
      "Performance Ratio for fr_eng_setpoint: 0.7119\n",
      "MAE for te_air_scav_rec_iso: 2.1861\n",
      "Performance Ratio for te_air_scav_rec_iso: 0.7101\n",
      "MAE for pr_cyl_max_mv_iso: 2484755.9879\n",
      "Performance Ratio for pr_cyl_max_mv_iso: 1.0029\n",
      "MAE for pr_cyl_comp_mv_iso: 2116437.2242\n",
      "Performance Ratio for pr_cyl_comp_mv_iso: 1.1054\n",
      "MAE for fr_eng_ecs: 0.1466\n",
      "Performance Ratio for fr_eng_ecs: 0.7984\n",
      "MAE for pr_air_scav_iso: 28296.0174\n",
      "Performance Ratio for pr_air_scav_iso: 0.9745\n",
      "MAE for engine_type_G95ME-C10.5-GI-EGRTC: 0.0034\n",
      "Performance Ratio for engine_type_G95ME-C10.5-GI-EGRTC: inf\n",
      "Testing mean absolute error: nan\n",
      "Average Performance Ratio: nan\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Function to calculate MAE while ignoring NaNs\n",
    "def calc_mae_ignore_nan(imputation, ground_truth, mask):\n",
    "    # Apply mask to ignore NaNs\n",
    "    valid_mask = ~np.isnan(imputation) & ~np.isnan(ground_truth) & mask\n",
    "    mae = np.abs(imputation[valid_mask] - ground_truth[valid_mask]).mean()\n",
    "    return mae\n",
    "\n",
    "# Calculate metrics per-feature: mean absolute error on the ground truth (artificially-missing values)\n",
    "mae_per_feature = []\n",
    "performance_ratios = []\n",
    "for i in range(n_features):\n",
    "    # Extract imputation and ground truth for feature i\n",
    "    imputation_i = test_imputation_denorm[:, :, i]\n",
    "    ground_truth_i = test_X_ori_denorm[:, :, i]\n",
    "    mask_i = test_mask[:, :, i]\n",
    "    \n",
    "    # Calculate MAE while ignoring NaNs\n",
    "    mae_i = calc_mae_ignore_nan(imputation_i, ground_truth_i, mask_i)\n",
    "    mae_per_feature.append(mae_i)\n",
    "    \n",
    "    # Calculate standard deviation of the original feature\n",
    "    std_dev_i = np.nanstd(ground_truth_i)\n",
    "    \n",
    "    # Calculate performance ratio (MAE / Standard Deviation)\n",
    "    performance_ratio_i = mae_i / std_dev_i\n",
    "    performance_ratios.append(performance_ratio_i)\n",
    "    \n",
    "    print(f\"MAE for {feature_names[i]}: {mae_i:.4f}\")\n",
    "    print(f\"Performance Ratio for {feature_names[i]}: {performance_ratio_i:.4f}\")\n",
    "    \n",
    "    mlflow.log_metric(f\"MAE_{feature_names[i]}\", mae_i)\n",
    "    mlflow.log_metric(f\"Performance_Ratio_{feature_names[i]}\", performance_ratio_i)\n",
    "\n",
    "# Calculate average MAE and average performance ratio\n",
    "avg_mae = np.mean(mae_per_feature)\n",
    "avg_performance_ratio = np.mean(performance_ratios)\n",
    "\n",
    "print(f\"Testing mean absolute error: {avg_mae:.4f}\")\n",
    "print(f\"Average Performance Ratio: {avg_performance_ratio:.4f}\")\n",
    "\n",
    "mlflow.log_metric(\"avg_mae\", avg_mae)\n",
    "mlflow.log_metric(\"avg_performance_ratio\", avg_performance_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try hyperparameter tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 23:23:00 [INFO]: Using the given device: cpu\n",
      "2025-05-13 23:23:00 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/best_model/20250513_T232300\n",
      "2025-05-13 23:23:00 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/best_model/20250513_T232300/tensorboard\n",
      "2025-05-13 23:23:00 [INFO]: Using customized MAE as the training loss function.\n",
      "2025-05-13 23:23:00 [INFO]: Using customized MSE as the validation metric function.\n",
      "2025-05-13 23:23:00 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 6,402,976\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "If no scoring is specified, the estimator passed should have a 'score' method. The estimator <pypots.imputation.saits.model.SAITS object at 0x7f19f1ee8130> does not.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39m# Perform grid search\u001b[39;00m\n\u001b[1;32m     40\u001b[0m grid_search \u001b[39m=\u001b[39m GridSearchCV(estimator\u001b[39m=\u001b[39msaits, param_grid\u001b[39m=\u001b[39mparam_grid, cv\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m grid_search\u001b[39m.\u001b[39;49mfit(train_data, val_data)\n\u001b[1;32m     43\u001b[0m \u001b[39m# Get the best parameters\u001b[39;00m\n\u001b[1;32m     44\u001b[0m best_params \u001b[39m=\u001b[39m grid_search\u001b[39m.\u001b[39mbest_params_\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/model_selection/_search.py:926\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Run fit with all sets of parameters.\u001b[39;00m\n\u001b[1;32m    893\u001b[0m \n\u001b[1;32m    894\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[39m    Instance of fitted estimator.\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    925\u001b[0m estimator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimator\n\u001b[0;32m--> 926\u001b[0m scorers, refit_metric \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_scorers()\n\u001b[1;32m    928\u001b[0m X, y \u001b[39m=\u001b[39m indexable(X, y)\n\u001b[1;32m    929\u001b[0m params \u001b[39m=\u001b[39m _check_method_params(X, params\u001b[39m=\u001b[39mparams)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/model_selection/_search.py:858\u001b[0m, in \u001b[0;36mBaseSearchCV._get_scorers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    856\u001b[0m     scorers \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscoring\n\u001b[1;32m    857\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscoring \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscoring, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 858\u001b[0m     scorers \u001b[39m=\u001b[39m check_scoring(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mestimator, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscoring)\n\u001b[1;32m    859\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    860\u001b[0m     scorers \u001b[39m=\u001b[39m _check_multimetric_scoring(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscoring)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    217\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    218\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    223\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    226\u001b[0m     )\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py:1022\u001b[0m, in \u001b[0;36mcheck_scoring\u001b[0;34m(estimator, scoring, allow_none, raise_exc)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1021\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1022\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m   1023\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf no scoring is specified, the estimator passed should \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1024\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhave a \u001b[39m\u001b[39m'\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m'\u001b[39m\u001b[39m method. The estimator \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m does not.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m estimator\n\u001b[1;32m   1025\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: If no scoring is specified, the estimator passed should have a 'score' method. The estimator <pypots.imputation.saits.model.SAITS object at 0x7f19f1ee8130> does not."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_layers': [2, 3, 4],\n",
    "    'd_model': [256, 512, 1024],\n",
    "    'dropout': [0.1, 0.2, 0.3],\n",
    "    'learning_rate': [1e-4, 1e-3, 1e-2]\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "saits = SAITS(\n",
    "    n_steps=data_normalized.shape[1],\n",
    "    n_features=data_normalized.shape[2],\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    d_ffn=512,\n",
    "    n_heads=8,\n",
    "    d_k=64,\n",
    "    d_v=64,\n",
    "    dropout=0.1,\n",
    "    attn_dropout=0.1,\n",
    "    diagonal_attention_mask=True,\n",
    "    ORT_weight=1,\n",
    "    MIT_weight=1,\n",
    "    batch_size=5,\n",
    "    epochs=10, \n",
    "    patience=6,\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    num_workers=0,\n",
    "    device=device, \n",
    "    saving_path=\"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/best_model\",\n",
    "    model_saving_strategy=\"best\",\n",
    "\n",
    "\n",
    "    \n",
    ")\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=saits, param_grid=param_grid, cv=3)\n",
    "grid_search.fit(train_data, val_data)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best parameters: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 23:24:33 [INFO]: Using the given device: cpu\n",
      "2025-05-13 23:24:33 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/best_model/20250513_T232433\n",
      "2025-05-13 23:24:33 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/best_model/20250513_T232433/tensorboard\n",
      "2025-05-13 23:24:33 [INFO]: Using customized MAE as the training loss function.\n",
      "2025-05-13 23:24:33 [INFO]: Using customized MSE as the validation metric function.\n",
      "2025-05-13 23:24:33 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 6,402,976\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "If no scoring is specified, the estimator passed should have a 'score' method. The estimator <pypots.imputation.saits.model.SAITS object at 0x7f19f1ee86d0> does not.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39m# Perform grid search\u001b[39;00m\n\u001b[1;32m     42\u001b[0m grid_search \u001b[39m=\u001b[39m GridSearchCV(estimator\u001b[39m=\u001b[39msaits, param_grid\u001b[39m=\u001b[39mparam_grid, cv\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[0;32m---> 43\u001b[0m grid_search\u001b[39m.\u001b[39;49mfit(train_data, val_data)\n\u001b[1;32m     45\u001b[0m \u001b[39m# Get the best parameters\u001b[39;00m\n\u001b[1;32m     46\u001b[0m best_params \u001b[39m=\u001b[39m grid_search\u001b[39m.\u001b[39mbest_params_\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/model_selection/_search.py:926\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Run fit with all sets of parameters.\u001b[39;00m\n\u001b[1;32m    893\u001b[0m \n\u001b[1;32m    894\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[39m    Instance of fitted estimator.\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    925\u001b[0m estimator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimator\n\u001b[0;32m--> 926\u001b[0m scorers, refit_metric \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_scorers()\n\u001b[1;32m    928\u001b[0m X, y \u001b[39m=\u001b[39m indexable(X, y)\n\u001b[1;32m    929\u001b[0m params \u001b[39m=\u001b[39m _check_method_params(X, params\u001b[39m=\u001b[39mparams)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/model_selection/_search.py:858\u001b[0m, in \u001b[0;36mBaseSearchCV._get_scorers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    856\u001b[0m     scorers \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscoring\n\u001b[1;32m    857\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscoring \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscoring, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 858\u001b[0m     scorers \u001b[39m=\u001b[39m check_scoring(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mestimator, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscoring)\n\u001b[1;32m    859\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    860\u001b[0m     scorers \u001b[39m=\u001b[39m _check_multimetric_scoring(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscoring)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    217\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    218\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    223\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    226\u001b[0m     )\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py:1022\u001b[0m, in \u001b[0;36mcheck_scoring\u001b[0;34m(estimator, scoring, allow_none, raise_exc)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1021\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1022\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m   1023\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf no scoring is specified, the estimator passed should \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1024\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhave a \u001b[39m\u001b[39m'\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m'\u001b[39m\u001b[39m method. The estimator \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m does not.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m estimator\n\u001b[1;32m   1025\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: If no scoring is specified, the estimator passed should have a 'score' method. The estimator <pypots.imputation.saits.model.SAITS object at 0x7f19f1ee86d0> does not."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_layers': [2, 3, 4],\n",
    "    'd_model': [256, 512, 1024],\n",
    "    'dropout': [0.1, 0.2, 0.3],\n",
    "    'learning_rate': [1e-4, 1e-3, 1e-2]\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "saits = SAITS(\n",
    "    n_steps=data_normalized.shape[1],\n",
    "    n_features=data_normalized.shape[2],\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    d_ffn=512,\n",
    "    n_heads=8,\n",
    "    d_k=64,\n",
    "    d_v=64,\n",
    "    dropout=0.1,\n",
    "    attn_dropout=0.1,\n",
    "    diagonal_attention_mask=True,\n",
    "    ORT_weight=1,\n",
    "    MIT_weight=1,\n",
    "    batch_size=5,\n",
    "    epochs=10, \n",
    "    patience=6,\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    num_workers=0,\n",
    "    device=device, \n",
    "    saving_path=\"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/best_model\",\n",
    "    model_saving_strategy=\"best\",\n",
    "\n",
    "\n",
    "    \n",
    ")\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=saits, param_grid=param_grid, cv=3)\n",
    "grid_search.fit(train_data, val_data)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best parameters: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid non-printable character U+00A0 (<ipython-input-25-838d13c7a70f>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[25], line 8\u001b[0;36m\u001b[0m\n\u001b[0;31m    predictions = estimator.predict(X)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid non-printable character U+00A0\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# %%\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Custom scoring function\n",
    "def custom_mae_scorer(estimator, X, y):\n",
    "Â Â Â  predictions = estimator.predict(X)\n",
    "Â Â Â  mae = np.mean(np.abs(predictions - y))\n",
    "Â Â Â  return mae\n",
    "\n",
    "# Create a custom scorer\n",
    "mae_scorer = make_scorer(custom_mae_scorer, greater_is_better=False)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_layers': [2, 3, 4],\n",
    "    'd_model': [256, 512, 1024],\n",
    "    'dropout': [0.1, 0.2, 0.3],\n",
    "    'learning_rate': [1e-4, 1e-3, 1e-2]\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "saits = SAITS(\n",
    "    n_steps=data_normalized.shape[1],\n",
    "    n_features=data_normalized.shape[2],\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    d_ffn=512,\n",
    "    n_heads=8,\n",
    "    d_k=64,\n",
    "    d_v=64,\n",
    "    dropout=0.1,\n",
    "    attn_dropout=0.1,\n",
    "    diagonal_attention_mask=True,\n",
    "    ORT_weight=1,\n",
    "    MIT_weight=1,\n",
    "    batch_size=5,\n",
    "    epochs=10, \n",
    "    patience=6,\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    num_workers=0,\n",
    "    device=device, \n",
    "    saving_path=\"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/best_model\",\n",
    "    model_saving_strategy=\"best\",\n",
    "\n",
    "\n",
    "    \n",
    ")\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=saits, param_grid=param_grid, cv=3)\n",
    "grid_search.fit(train_data, val_data)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best parameters: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid non-printable character U+00A0 (<ipython-input-26-004ee1d3b0cf>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[26], line 7\u001b[0;36m\u001b[0m\n\u001b[0;31m    predictions = estimator.predict(X)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid non-printable character U+00A0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Custom scoring function\n",
    "def custom_mae_scorer(estimator, X, y):\n",
    "Â Â Â  predictions = estimator.predict(X)\n",
    "Â Â Â  mae = np.mean(np.abs(predictions - y))\n",
    "Â Â Â  return mae\n",
    "\n",
    "# Create a custom scorer\n",
    "mae_scorer = make_scorer(custom_mae_scorer, greater_is_better=False)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_layers': [2, 3, 4],\n",
    "    'd_model': [256, 512, 1024],\n",
    "    'dropout': [0.1, 0.2, 0.3],\n",
    "    'learning_rate': [1e-4, 1e-3, 1e-2]\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "saits = SAITS(\n",
    "    n_steps=data_normalized.shape[1],\n",
    "    n_features=data_normalized.shape[2],\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    d_ffn=512,\n",
    "    n_heads=8,\n",
    "    d_k=64,\n",
    "    d_v=64,\n",
    "    dropout=0.1,\n",
    "    attn_dropout=0.1,\n",
    "    diagonal_attention_mask=True,\n",
    "    ORT_weight=1,\n",
    "    MIT_weight=1,\n",
    "    batch_size=5,\n",
    "    epochs=10, \n",
    "    patience=6,\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    num_workers=0,\n",
    "    device=device, \n",
    "    saving_path=\"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/best_model\",\n",
    "    model_saving_strategy=\"best\",\n",
    "\n",
    "\n",
    "    \n",
    ")\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=saits, param_grid=param_grid, cv=3)\n",
    "grid_search.fit(train_data, val_data)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best parameters: {best_params}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
