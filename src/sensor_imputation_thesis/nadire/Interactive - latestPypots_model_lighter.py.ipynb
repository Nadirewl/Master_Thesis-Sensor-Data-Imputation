{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connected to .venv (Python 3.10.16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6adff6-816b-48a0-88bf-c7e765be8127",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\n",
      "████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗\n",
      "╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║\n",
      "   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║\n",
      "   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║\n",
      "   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║\n",
      "   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝\n",
      "ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai \u001b[0m\n",
      "\n",
      "Column time has 0 NaN values\n",
      "Column time has 0.0 Missing_rate\n",
      "Column fr_eng has 0 NaN values\n",
      "Column fr_eng has 0.0 Missing_rate\n",
      "Column te_exh_cyl_out__0 has 73 NaN values\n",
      "Column te_exh_cyl_out__0 has 0.0006111906496203082 Missing_rate\n",
      "Column pd_air_ic__0 has 73 NaN values\n",
      "Column pd_air_ic__0 has 0.0006111906496203082 Missing_rate\n",
      "Column pr_exh_turb_out__0 has 119439 NaN values\n",
      "Column pr_exh_turb_out__0 has 1.0 Missing_rate\n",
      "Column te_air_ic_out__0 has 73 NaN values\n",
      "Column te_air_ic_out__0 has 0.0006111906496203082 Missing_rate\n",
      "Column te_seawater has 73 NaN values\n",
      "Column te_seawater has 0.0006111906496203082 Missing_rate\n",
      "Column te_air_comp_in_a__0 has 119439 NaN values\n",
      "Column te_air_comp_in_a__0 has 1.0 Missing_rate\n",
      "Column te_air_comp_in_b__0 has 119439 NaN values\n",
      "Column te_air_comp_in_b__0 has 1.0 Missing_rate\n",
      "Column fr_tc__0 has 119439 NaN values\n",
      "Column fr_tc__0 has 1.0 Missing_rate\n",
      "Column pr_baro has 73 NaN values\n",
      "Column pr_baro has 0.0006111906496203082 Missing_rate\n",
      "Column pd_air_ic__0_1 has 73 NaN values\n",
      "Column pd_air_ic__0_1 has 0.0006111906496203082 Missing_rate\n",
      "Column pr_exh_rec has 73 NaN values\n",
      "Column pr_exh_rec has 0.0006111906496203082 Missing_rate\n",
      "Column te_exh_turb_in__0 has 119439 NaN values\n",
      "Column te_exh_turb_in__0 has 1.0 Missing_rate\n",
      "Column te_exh_turb_out__0 has 119439 NaN values\n",
      "Column te_exh_turb_out__0 has 1.0 Missing_rate\n",
      "Column bo_aux_blower_running has 0 NaN values\n",
      "Column bo_aux_blower_running has 0.0 Missing_rate\n",
      "Column re_eng_load has 173 NaN values\n",
      "Column re_eng_load has 0.0014484381148536073 Missing_rate\n",
      "Column pr_air_scav_ecs has 0 NaN values\n",
      "Column pr_air_scav_ecs has 0.0 Missing_rate\n",
      "Column pr_air_scav has 0 NaN values\n",
      "Column pr_air_scav has 0.0 Missing_rate\n",
      "Column te_air_scav_rec has 73 NaN values\n",
      "Column te_air_scav_rec has 0.0006111906496203082 Missing_rate\n",
      "Column te_air_ic_out__0_1 has 73 NaN values\n",
      "Column te_air_ic_out__0_1 has 0.0006111906496203082 Missing_rate\n",
      "Column pr_cyl_comp__0 has 173 NaN values\n",
      "Column pr_cyl_comp__0 has 0.0014484381148536073 Missing_rate\n",
      "Column pr_cyl_max__0 has 173 NaN values\n",
      "Column pr_cyl_max__0 has 0.0014484381148536073 Missing_rate\n",
      "Column se_mip__0 has 173 NaN values\n",
      "Column se_mip__0 has 0.0014484381148536073 Missing_rate\n",
      "Column te_exh_cyl_out__0_1 has 73 NaN values\n",
      "Column te_exh_cyl_out__0_1 has 0.0006111906496203082 Missing_rate\n",
      "Column fr_eng_setpoint has 0 NaN values\n",
      "Column fr_eng_setpoint has 0.0 Missing_rate\n",
      "Column te_air_scav_rec_iso has 51235 NaN values\n",
      "Column te_air_scav_rec_iso has 0.4289637388122807 Missing_rate\n",
      "Column pr_cyl_max_mv_iso has 51385 NaN values\n",
      "Column pr_cyl_max_mv_iso has 0.4302196100101307 Missing_rate\n",
      "Column pr_cyl_comp_mv_iso has 51385 NaN values\n",
      "Column pr_cyl_comp_mv_iso has 0.4302196100101307 Missing_rate\n",
      "Column fr_eng_ecs has 0 NaN values\n",
      "Column fr_eng_ecs has 0.0 Missing_rate\n",
      "Column pr_air_scav_iso has 51385 NaN values\n",
      "Column pr_air_scav_iso has 0.4302196100101307 Missing_rate\n",
      "Column engine_type_G95ME-C10.5-GI-EGRTC has 0 NaN values\n",
      "Column engine_type_G95ME-C10.5-GI-EGRTC has 0.0 Missing_rate\n",
      "Original size:119439, Sampled size: 39813\n",
      "Reshaped data:(39813, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    },
    {
     "ename": "InvalidUrlException",
     "evalue": "Invalid url: http:127.0.0.1:5001/api/2.0/mlflow/experiments/get-by-name",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidURL\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/utils/rest_utils.py:189\u001b[0m, in \u001b[0;36mhttp_request\u001b[0;34m(host_creds, endpoint, method, max_retries, backoff_factor, backoff_jitter, extra_headers, retry_codes, timeout, raise_on_status, respect_retry_after_header, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 189\u001b[0m     \u001b[39mreturn\u001b[39;00m _get_http_response_with_retries(\n\u001b[1;32m    190\u001b[0m         method,\n\u001b[1;32m    191\u001b[0m         url,\n\u001b[1;32m    192\u001b[0m         max_retries,\n\u001b[1;32m    193\u001b[0m         backoff_factor,\n\u001b[1;32m    194\u001b[0m         backoff_jitter,\n\u001b[1;32m    195\u001b[0m         retry_codes,\n\u001b[1;32m    196\u001b[0m         raise_on_status,\n\u001b[1;32m    197\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    198\u001b[0m         verify\u001b[39m=\u001b[39;49mhost_creds\u001b[39m.\u001b[39;49mverify,\n\u001b[1;32m    199\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    200\u001b[0m         respect_retry_after_header\u001b[39m=\u001b[39;49mrespect_retry_after_header,\n\u001b[1;32m    201\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    202\u001b[0m     )\n\u001b[1;32m    203\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout \u001b[39mas\u001b[39;00m to:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/utils/request_utils.py:237\u001b[0m, in \u001b[0;36m_get_http_response_with_retries\u001b[0;34m(method, url, max_retries, backoff_factor, backoff_jitter, retry_codes, raise_on_status, allow_redirects, respect_retry_after_header, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m allow_redirects \u001b[39m=\u001b[39m env_value \u001b[39mif\u001b[39;00m allow_redirects \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m allow_redirects\n\u001b[0;32m--> 237\u001b[0m \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method, url, allow_redirects\u001b[39m=\u001b[39;49mallow_redirects, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/requests/sessions.py:575\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    563\u001b[0m req \u001b[39m=\u001b[39m Request(\n\u001b[1;32m    564\u001b[0m     method\u001b[39m=\u001b[39mmethod\u001b[39m.\u001b[39mupper(),\n\u001b[1;32m    565\u001b[0m     url\u001b[39m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    573\u001b[0m     hooks\u001b[39m=\u001b[39mhooks,\n\u001b[1;32m    574\u001b[0m )\n\u001b[0;32m--> 575\u001b[0m prep \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare_request(req)\n\u001b[1;32m    577\u001b[0m proxies \u001b[39m=\u001b[39m proxies \u001b[39mor\u001b[39;00m {}\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/requests/sessions.py:484\u001b[0m, in \u001b[0;36mSession.prepare_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    483\u001b[0m p \u001b[39m=\u001b[39m PreparedRequest()\n\u001b[0;32m--> 484\u001b[0m p\u001b[39m.\u001b[39;49mprepare(\n\u001b[1;32m    485\u001b[0m     method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod\u001b[39m.\u001b[39;49mupper(),\n\u001b[1;32m    486\u001b[0m     url\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49murl,\n\u001b[1;32m    487\u001b[0m     files\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mfiles,\n\u001b[1;32m    488\u001b[0m     data\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mdata,\n\u001b[1;32m    489\u001b[0m     json\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mjson,\n\u001b[1;32m    490\u001b[0m     headers\u001b[39m=\u001b[39;49mmerge_setting(\n\u001b[1;32m    491\u001b[0m         request\u001b[39m.\u001b[39;49mheaders, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mheaders, dict_class\u001b[39m=\u001b[39;49mCaseInsensitiveDict\n\u001b[1;32m    492\u001b[0m     ),\n\u001b[1;32m    493\u001b[0m     params\u001b[39m=\u001b[39;49mmerge_setting(request\u001b[39m.\u001b[39;49mparams, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams),\n\u001b[1;32m    494\u001b[0m     auth\u001b[39m=\u001b[39;49mmerge_setting(auth, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mauth),\n\u001b[1;32m    495\u001b[0m     cookies\u001b[39m=\u001b[39;49mmerged_cookies,\n\u001b[1;32m    496\u001b[0m     hooks\u001b[39m=\u001b[39;49mmerge_hooks(request\u001b[39m.\u001b[39;49mhooks, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhooks),\n\u001b[1;32m    497\u001b[0m )\n\u001b[1;32m    498\u001b[0m \u001b[39mreturn\u001b[39;00m p\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/requests/models.py:367\u001b[0m, in \u001b[0;36mPreparedRequest.prepare\u001b[0;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_method(method)\n\u001b[0;32m--> 367\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare_url(url, params)\n\u001b[1;32m    368\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_headers(headers)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/requests/models.py:444\u001b[0m, in \u001b[0;36mPreparedRequest.prepare_url\u001b[0;34m(self, url, params)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m host:\n\u001b[0;32m--> 444\u001b[0m     \u001b[39mraise\u001b[39;00m InvalidURL(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid URL \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m!r}\u001b[39;00m\u001b[39m: No host supplied\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    446\u001b[0m \u001b[39m# In general, we want to try IDNA encoding the hostname if the string contains\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[39m# non-ASCII characters. This allows users to automatically get the correct IDNA\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[39m# behaviour. For strings containing only ASCII characters, we need to also verify\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[39m# it doesn't start with a wildcard (*), before allowing the unencoded hostname.\u001b[39;00m\n",
      "\u001b[0;31mInvalidURL\u001b[0m: Invalid URL 'http:127.0.0.1:5001/api/2.0/mlflow/experiments/get-by-name': No host supplied",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mInvalidUrlException\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/Pypots_model_lighter.py:226\u001b[0m\n\u001b[1;32m    224\u001b[0m mlflow\u001b[39m.\u001b[39mset_tracking_uri(\u001b[39m\"\u001b[39m\u001b[39mhttp:127.0.0.1:5001\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    225\u001b[0m client \u001b[39m=\u001b[39m mlflow\u001b[39m.\u001b[39mtracking\u001b[39m.\u001b[39mMlflowClient()\n\u001b[0;32m--> 226\u001b[0m mlflow\u001b[39m.\u001b[39;49mset_experiment(\u001b[39m\"\u001b[39;49m\u001b[39mSAITS_3\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    227\u001b[0m SAITS_run_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mSAITS_1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    231\u001b[0m \u001b[39mwith\u001b[39;00m mlflow\u001b[39m.\u001b[39mstart_run(run_name\u001b[39m=\u001b[39mSAITS_run_name) \u001b[39mas\u001b[39;00m run:\n\u001b[1;32m    232\u001b[0m \u001b[39m# initialize the model (from example)\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/tracking/fluent.py:157\u001b[0m, in \u001b[0;36mset_experiment\u001b[0;34m(experiment_name, experiment_id)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39mwith\u001b[39;00m _experiment_lock:\n\u001b[1;32m    156\u001b[0m     \u001b[39mif\u001b[39;00m experiment_id \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m         experiment \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mget_experiment_by_name(experiment_name)\n\u001b[1;32m    158\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m experiment:\n\u001b[1;32m    159\u001b[0m             \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/tracking/client.py:1257\u001b[0m, in \u001b[0;36mMlflowClient.get_experiment_by_name\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mget_experiment_by_name\u001b[39m(\u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Experiment]:\n\u001b[1;32m   1226\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Retrieve an experiment by experiment name from the backend store\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m \n\u001b[1;32m   1228\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1255\u001b[0m \u001b[39m        Lifecycle_stage: active\u001b[39;00m\n\u001b[1;32m   1256\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1257\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tracking_client\u001b[39m.\u001b[39;49mget_experiment_by_name(name)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py:502\u001b[0m, in \u001b[0;36mTrackingServiceClient.get_experiment_by_name\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mget_experiment_by_name\u001b[39m(\u001b[39mself\u001b[39m, name):\n\u001b[1;32m    495\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    497\u001b[0m \u001b[39m        name: The experiment name.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[39m        :py:class:`mlflow.entities.Experiment`\u001b[39;00m\n\u001b[1;32m    501\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 502\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstore\u001b[39m.\u001b[39;49mget_experiment_by_name(name)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/store/tracking/rest_store.py:522\u001b[0m, in \u001b[0;36mRestStore.get_experiment_by_name\u001b[0;34m(self, experiment_name)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    521\u001b[0m     req_body \u001b[39m=\u001b[39m message_to_json(GetExperimentByName(experiment_name\u001b[39m=\u001b[39mexperiment_name))\n\u001b[0;32m--> 522\u001b[0m     response_proto \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_endpoint(GetExperimentByName, req_body)\n\u001b[1;32m    523\u001b[0m     \u001b[39mreturn\u001b[39;00m Experiment\u001b[39m.\u001b[39mfrom_proto(response_proto\u001b[39m.\u001b[39mexperiment)\n\u001b[1;32m    524\u001b[0m \u001b[39mexcept\u001b[39;00m MlflowException \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/store/tracking/rest_store.py:82\u001b[0m, in \u001b[0;36mRestStore._call_endpoint\u001b[0;34m(self, api, json_body, endpoint)\u001b[0m\n\u001b[1;32m     80\u001b[0m     endpoint, method \u001b[39m=\u001b[39m _METHOD_TO_INFO[api]\n\u001b[1;32m     81\u001b[0m response_proto \u001b[39m=\u001b[39m api\u001b[39m.\u001b[39mResponse()\n\u001b[0;32m---> 82\u001b[0m \u001b[39mreturn\u001b[39;00m call_endpoint(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_host_creds(), endpoint, method, json_body, response_proto)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/utils/rest_utils.py:365\u001b[0m, in \u001b[0;36mcall_endpoint\u001b[0;34m(host_creds, endpoint, method, json_body, response_proto, extra_headers)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[39mif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mGET\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    364\u001b[0m     call_kwargs[\u001b[39m\"\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m json_body\n\u001b[0;32m--> 365\u001b[0m     response \u001b[39m=\u001b[39m http_request(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcall_kwargs)\n\u001b[1;32m    366\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    367\u001b[0m     call_kwargs[\u001b[39m\"\u001b[39m\u001b[39mjson\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m json_body\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/utils/rest_utils.py:210\u001b[0m, in \u001b[0;36mhttp_request\u001b[0;34m(host_creds, endpoint, method, max_retries, backoff_factor, backoff_jitter, extra_headers, retry_codes, timeout, raise_on_status, respect_retry_after_header, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[39mraise\u001b[39;00m MlflowException(\n\u001b[1;32m    205\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAPI request to \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m failed with timeout exception \u001b[39m\u001b[39m{\u001b[39;00mto\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    206\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m To increase the timeout, set the environment variable \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    207\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mMLFLOW_HTTP_REQUEST_TIMEOUT\u001b[39m!s}\u001b[39;00m\u001b[39m to a larger value.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    208\u001b[0m     ) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mto\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mInvalidURL \u001b[39mas\u001b[39;00m iu:\n\u001b[0;32m--> 210\u001b[0m     \u001b[39mraise\u001b[39;00m InvalidUrlException(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid url: \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39miu\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    212\u001b[0m     \u001b[39mraise\u001b[39;00m MlflowException(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAPI request to \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m failed with exception \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mInvalidUrlException\u001b[0m: Invalid url: http:127.0.0.1:5001/api/2.0/mlflow/experiments/get-by-name"
     ]
    }
   ],
   "source": [
    "#Import Pypots Library\n",
    "from pypots.optim import Adam\n",
    "from pypots.imputation import SAITS\n",
    "#from pypots.utils.metrics import calc_mae\n",
    "from pypots.nn.functional import calc_mae\n",
    "\n",
    "\n",
    "import argparse\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import data_insight\n",
    "from data_insight import setup_duckdb\n",
    "from duckdb import DuckDBPyConnection as DuckDB\n",
    "from duckdb import DuckDBPyRelation as Relation\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "from duckdb import DuckDBPyConnection as DuckDB\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna \n",
    "from optuna.visualization import plot_optimization_history\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, Dataset\n",
    "from pygrinder.missing_completely_at_random import mcar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import sensor_imputation_thesis.shared.load_data as load\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "#PatchTST might be an ideal choise if SAITS is too slow \n",
    "\n",
    "##Drop columns with different indexes while loading data.. Or the mean values \n",
    "\n",
    "df=pd.read_parquet(\"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/Newdataframeforpypots.parquet\")\n",
    "\n",
    "len(df)\n",
    "\n",
    "#current length of the dataframe is 119439\n",
    "\n",
    "# Check nan values in each column\n",
    "for col in df.columns:\n",
    "    print(f\"Column {col} has {df[col].isna().sum()} NaN values\")\n",
    "    missing_rate=df[col].isna().sum()/len(df[col])\n",
    "    print(f\"Column {col} has {missing_rate} Missing_rate\")\n",
    "\n",
    "\n",
    "#Try with smaller dataset, size 4000\n",
    "##SAMPLE the percengtage of the dataset, df.sample (averagely pick samples)\n",
    "#not df.sample cuz it will randomly select \n",
    "original_size=len(df)\n",
    "desired_fraction=0.3 #Select data every 3 minutes \n",
    "step=int(1/desired_fraction) #step_size=10 (sample every 10th (3/10) minute)\n",
    "\n",
    "#Systematic sampling: Start at a random offset to avoid bias \n",
    "start=np.random.randint(0,step) #Random start between 0-9\n",
    "df1=df.iloc[start::step].reset_index(drop=True)\n",
    "\n",
    "print(f\"Original size:{len(df)}, Sampled size: {len(df1)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Custom Dataset class\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Data processing code\n",
    "sensor_cols = [col for col in df1.columns if col != \"time\"]\n",
    "data = df1[sensor_cols].values\n",
    "\n",
    "#¤get feature names for printing mae later \n",
    "feature_names=df1[sensor_cols].columns.tolist()\n",
    "\n",
    "## Convert data to 3D arrays of shape n_samples, n_timesteps, n_features, X_ori refers to the original data without missing values \n",
    "## Reconstruct all columns simultaneously  #num_features: 119\n",
    "n_features = data.shape[1]  # exclude the time column\n",
    "n_steps = 20 #60 (was 60 previously) #(TRY TO CHANGE HERE)  # # window length, 1440 steps = 24 hours of 1-minute data, but here is revised to 60 again\n",
    "#total_elements = data.shape[0] * data.shape[1]\n",
    "n_samples = data.shape[0] // n_steps \n",
    "\n",
    "\n",
    "\n",
    "# Reshape to (n_samples // n_steps, n_steps, n_features)\n",
    "#data_reshaped = data.reshape((n_samples, n_steps, n_features))\n",
    "data_reshaped=data[:n_samples*n_steps].reshape(n_samples,n_steps,n_features)\n",
    "print(f\"Reshaped data:{data.shape}\")\n",
    "\n",
    "#Split into train, test, val, fit scaler only on the train set (prevent data leakage)\n",
    "\n",
    "#train_size = int(0.6 * len(data))\n",
    "#val_size = int(0.2 * len(data))\n",
    "#test_size = len(data) - train_size - val_size\n",
    "\n",
    "#train_data = data_reshaped[:train_size]\n",
    "#val_data = data_reshaped[train_size:train_size + val_size]\n",
    "#test_data= data_reshaped[train_size + val_size:]\n",
    "\n",
    "\n",
    "#Apply time series split \n",
    "#Split into train(60%), val(20%), and test (20%)\n",
    "train_data, temp_data=train_test_split(data_reshaped,test_size=0.4,shuffle=True)\n",
    "val_data, test_data=train_test_split(temp_data, test_size=0.5, shuffle=False)\n",
    "\n",
    "##Normalization is important because of the nature of mse calculation of saits, columns with large \n",
    "#values dominate the loss, making metrics meaningless. SAITS computes MSE/MAE column-wise and averages \n",
    "#them across all columns \n",
    "#  Apply minmax scaler here \n",
    "#normalize each feature independently\n",
    "scalers={}\n",
    "\n",
    "\n",
    "#train_scaled = np.zeros_like(data_reshaped[train_size])  # Initialize the normalized data array\n",
    "#val_scaled=np.zeros_like(data_reshaped[train_size:train_size + val_size])\n",
    "#test_scaled=np.zeros_like(data_reshaped[train_size + val_size:])\n",
    "\n",
    "train_scaled = np.zeros_like(train_data)\n",
    "val_scaled = np.zeros_like(val_data)\n",
    "test_scaled = np.zeros_like(test_data)\n",
    "\n",
    "\n",
    "for i in range(data_reshaped.shape[2]):\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1)) #changed to -1,1\n",
    "    # Flatten timesteps and samples for scaling\n",
    "    train_scaled[:, :, i] = scaler.fit_transform(train_data[:, :, i].reshape(-1, 1)).reshape(train_data.shape[0], train_data.shape[1])\n",
    "    val_scaled[:, :, i] = scaler.transform(val_data[:, :, i].reshape(-1, 1)).reshape(val_data.shape[0], val_data.shape[1])\n",
    "    test_scaled[:, :, i] = scaler.transform(test_data[:, :, i].reshape(-1, 1)).reshape(test_data.shape[0], test_data.shape[1])\n",
    "    scalers[i] = scaler  # Save scalers to inverse-transform later\n",
    "\n",
    "#Inverse Scale\n",
    "def inverse_scale(imputation, scalers):\n",
    "    n_features = imputation.shape[2]\n",
    "    imputation_denorm = np.empty_like(imputation)\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        imputation_denorm[:, :, i] = scalers[i].inverse_transform(imputation[:, :, i].reshape(-1, 1)).reshape(imputation.shape[0], imputation.shape[1])\n",
    "    \n",
    "    return imputation_denorm  \n",
    "\n",
    "\n",
    "#Optional: Artificially mask. Mask 20% of the data (MIT part)\n",
    "def mcar_f(X, mask_ratio=0.2):\n",
    "    \"\"\"Apply MCAR only to observed values.\"\"\"\n",
    "    observed_mask=~np.isnan(X) #find observed positions\n",
    "    artificial_mask=mcar(X,mask_ratio).astype(bool) #generate MCAR mask, cast to boolean\n",
    "    #combine masks \n",
    "    combined_mask=observed_mask & artificial_mask\n",
    "\n",
    "    #Apply masking\n",
    "    X_masked=X.copy()\n",
    "    X_masked[combined_mask]=np.nan\n",
    "    return X_masked,combined_mask\n",
    "\n",
    "\n",
    "#Use mcar on validation data \n",
    "val_X_masked, val_mask =mcar_f(val_scaled)\n",
    "val_X_ori=val_scaled.copy() \n",
    "\n",
    "test_X_masked, test_mask =mcar_f(test_scaled)\n",
    "test_X_ori=test_scaled.copy() \n",
    "\n",
    "#?? Problem: Can't have the best input for testing\n",
    "#1.Create synthetic test_data cuz if I drop nan values for test set, there's basically nothing left\n",
    "#synthetic_data=np.random.randn(n_samples,n_steps,n_features)\n",
    "#test_X_masked,test_mask=mcar_f(synthetic_data)\n",
    "#test_X_ori=synthetic_data.copy() #Ground truth\n",
    "\n",
    "# 2, Ensure no NaN values in synthetic data\n",
    "#test_X_masked = np.nan_to_num(test_X_masked, nan=np.nanmean(test_X_masked))\n",
    "#test_X_ori = np.nan_to_num(test_X_ori, nan=np.nanmean(test_X_ori))\n",
    "\n",
    "\n",
    "\n",
    "class Config:\n",
    "    no_cuda = False\n",
    "    no_mps = False\n",
    "    seed = 1\n",
    "\n",
    "args=Config()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "use_mps = not args.no_mps and torch.backends.mps.is_available()\n",
    "\n",
    "if args.cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA\")\n",
    "elif use_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "\n",
    "#MLflow set up\n",
    "mlflow.set_tracking_uri(\"http:127.0.0.1:5001\")\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "mlflow.set_experiment(\"SAITS_3\")\n",
    "SAITS_run_name = \"SAITS_1\"\n",
    "\n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name=SAITS_run_name) as run:\n",
    "# initialize the model (from example)\n",
    "    saits = SAITS(\n",
    "        n_steps=data_reshaped.shape[1],\n",
    "        n_features=data_reshaped.shape[2],\n",
    "        #n_layers=2, #deep network(4) for long sequences, here is revised for 2 since it always shows cuda error \n",
    "        n_layers=3,\n",
    "        d_model=512,\n",
    "        optimizer=Adam(lr=1e-3),\n",
    "        #lr=params[\"lr\"],\n",
    "        ORT_weight=1,\n",
    "        MIT_weight=1,\n",
    "        #d_model=512,  #d_modle must equal n_heads * d_k\n",
    "        d_ffn=512,\n",
    "        n_heads=8,\n",
    "        d_k=64,\n",
    "        d_v=64,\n",
    "        dropout=0.1,\n",
    "        attn_dropout=0.1,\n",
    "        diagonal_attention_mask=True,  # otherwise the original self-attention mechanism will be applied\n",
    "        #ORT_weight=1,  # you can adjust the weight values of arguments ORT_weight\n",
    "       # and MIT_weight to make the SAITS model focus more on one task. Usually you can just leave them to the default values, i.e. 1.\n",
    "        #MIT_weight=1,\n",
    "        batch_size=5, #try with 5 to see if it works, was 4. \n",
    "        # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "        epochs=10, #try with 10 to see if it runs\n",
    "        # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "        # You can leave it to defualt as None to disable early stopping.\n",
    "        patience=6, #initially was 3, tried to increase to see more possibilities \n",
    "        # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "        # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "        #optimizer=Adam(lr=1e-3),\n",
    "        # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "        # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "        # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "        num_workers=0,\n",
    "        # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "        # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "        device=\"CUDA\", \n",
    "        # set the path for saving tensorboard and trained model files \n",
    "        saving_path=\"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/best_model\",\n",
    "        # only save the best model after training finished.\n",
    "        # You can also set it as \"better\" to save models performing better ever during training.\n",
    "        model_saving_strategy=\"best\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
    "#use original missigness for trainig, use the mcar masked points as ground truth\n",
    "saits.fit(train_set={\"X\": train_scaled}, val_set={\"X\": val_X_masked , \"X_ori\":val_X_ori })\n",
    "\n",
    "\n",
    "##drop null values in test set \n",
    "\n",
    "# Check for NaN values across the entire array\n",
    "#nan_mask = np.isnan(test_X).any(axis=(1, 2))\n",
    "\n",
    "# Filter out samples (rows) that contain NaN values\n",
    "#test_X_clean = test_X[~nan_mask]\n",
    "#test_X_ori = test_X_clean\n",
    "#test_X_masked, test_mask = mcar_f(test_X_clean)\n",
    "\n",
    "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
    "# Convert the numpy array to a dictionary\n",
    "\n",
    "\n",
    "# Ensure saits_imputation and test_X are numpy arrays\n",
    "#if not isinstance(saits_imputation, np.ndarray):\n",
    "    #saits_imputation = np.array(saits_imputation)\n",
    "#if not isinstance(test_X, np.ndarray):\n",
    "     #test_X = np.array(test_X)\n",
    "\n",
    "#Inverse Scale\n",
    "def inverse_scale(imputation, scalers):\n",
    "    n_features = imputation.shape[2]\n",
    "    imputation_denorm = np.empty_like(imputation)\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        imputation_denorm[:, :, i] = scalers[i].inverse_transform(imputation[:, :, i].reshape(-1, 1)).reshape(imputation.shape[0], imputation.shape[1])\n",
    "    \n",
    "    return imputation_denorm  \n",
    "\n",
    "    \n",
    "#Apply function to the dataset \n",
    "test_set_dict = {\"X\": test_X_masked}\n",
    "test_imputation = saits.predict(test_set_dict)\n",
    "test_imputation_array = test_imputation[\"imputation\"]\n",
    "test_imputation_denorm = inverse_scale(test_imputation_array, scalers)\n",
    "test_ori_denorm=inverse_scale(test_X_ori, scalers)\n",
    "\n",
    "if args.cuda:\n",
    "    torch.cuda.empty_cache() \n",
    "\n",
    "\n",
    "#Calculate metrics per-feature: mean absolute error on the ground truth (artificially-missing values)\n",
    "mae_per_feature=[]\n",
    "percentage_mae_per_feature=[]\n",
    "\n",
    "for i in range(n_features):\n",
    "    #Extract imputation and ground truth for feature i\n",
    "    imputation_i=test_imputation_denorm[:,:,i]\n",
    "    ground_truth_i=test_ori_denorm[:,:,i]\n",
    "    mask_i=test_mask[:,:,i]\n",
    "    # Check for NaN values\n",
    "    if np.isnan(imputation_i).any() or np.isnan(ground_truth_i).any():\n",
    "        print(f\"NaN values detected in feature {i}\")\n",
    "        continue  # Skip this feature if NaN values are found\n",
    "    #Filter only artificially masked positions\n",
    "    mae_i=calc_mae(imputation_i,ground_truth_i,mask_i)\n",
    "    mae_per_feature.append(mae_i)\n",
    "    #Calculate the original standard deviation for the feature\n",
    "    std_dev_i = np.std(ground_truth_i[mask_i == 1])\n",
    "    # Calculate the percentage of MAE relative to the standard deviation   \n",
    "    if std_dev_i != 0:\n",
    "        percentage_mae_i = (mae_i / std_dev_i) * 100\n",
    "        percentage_mae_per_feature.append(percentage_mae_i)\n",
    "    else:\n",
    "        percentage_mae_i = float('inf')\n",
    "            \n",
    "    mlflow.log_metric(f\"MAE_{feature_names[i]}\", mae_i)\n",
    "    mlflow.log_metric(f\"Percentage_MAE_{feature_names[i]}\", percentage_mae_i)\n",
    "\n",
    "    print(f\"MAE for {feature_names[i]}: {mae_i:.4f}\")\n",
    "    mlflow.log_metric(f\"MAE_{feature_names[i]}\", mae_i)\n",
    "\n",
    "# calculate average MAE \n",
    "avg_mae=np.mean(mae_per_feature)\n",
    "print(f\"Testing mean absolute error: {avg_mae:.4f}\")\n",
    "mlflow.log_metric(\"avg_mae\", avg_mae)\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b8dd48-ef75-4b4f-928f-d25685c4e5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column time has 0 NaN values\n",
      "Column time has 0.0 Missing_rate\n",
      "Column fr_eng has 0 NaN values\n",
      "Column fr_eng has 0.0 Missing_rate\n",
      "Column te_exh_cyl_out__0 has 73 NaN values\n",
      "Column te_exh_cyl_out__0 has 0.0006111906496203082 Missing_rate\n",
      "Column pd_air_ic__0 has 73 NaN values\n",
      "Column pd_air_ic__0 has 0.0006111906496203082 Missing_rate\n",
      "Column pr_exh_turb_out__0 has 119439 NaN values\n",
      "Column pr_exh_turb_out__0 has 1.0 Missing_rate\n",
      "Column te_air_ic_out__0 has 73 NaN values\n",
      "Column te_air_ic_out__0 has 0.0006111906496203082 Missing_rate\n",
      "Column te_seawater has 73 NaN values\n",
      "Column te_seawater has 0.0006111906496203082 Missing_rate\n",
      "Column te_air_comp_in_a__0 has 119439 NaN values\n",
      "Column te_air_comp_in_a__0 has 1.0 Missing_rate\n",
      "Column te_air_comp_in_b__0 has 119439 NaN values\n",
      "Column te_air_comp_in_b__0 has 1.0 Missing_rate\n",
      "Column fr_tc__0 has 119439 NaN values\n",
      "Column fr_tc__0 has 1.0 Missing_rate\n",
      "Column pr_baro has 73 NaN values\n",
      "Column pr_baro has 0.0006111906496203082 Missing_rate\n",
      "Column pd_air_ic__0_1 has 73 NaN values\n",
      "Column pd_air_ic__0_1 has 0.0006111906496203082 Missing_rate\n",
      "Column pr_exh_rec has 73 NaN values\n",
      "Column pr_exh_rec has 0.0006111906496203082 Missing_rate\n",
      "Column te_exh_turb_in__0 has 119439 NaN values\n",
      "Column te_exh_turb_in__0 has 1.0 Missing_rate\n",
      "Column te_exh_turb_out__0 has 119439 NaN values\n",
      "Column te_exh_turb_out__0 has 1.0 Missing_rate\n",
      "Column bo_aux_blower_running has 0 NaN values\n",
      "Column bo_aux_blower_running has 0.0 Missing_rate\n",
      "Column re_eng_load has 173 NaN values\n",
      "Column re_eng_load has 0.0014484381148536073 Missing_rate\n",
      "Column pr_air_scav_ecs has 0 NaN values\n",
      "Column pr_air_scav_ecs has 0.0 Missing_rate\n",
      "Column pr_air_scav has 0 NaN values\n",
      "Column pr_air_scav has 0.0 Missing_rate\n",
      "Column te_air_scav_rec has 73 NaN values\n",
      "Column te_air_scav_rec has 0.0006111906496203082 Missing_rate\n",
      "Column te_air_ic_out__0_1 has 73 NaN values\n",
      "Column te_air_ic_out__0_1 has 0.0006111906496203082 Missing_rate\n",
      "Column pr_cyl_comp__0 has 173 NaN values\n",
      "Column pr_cyl_comp__0 has 0.0014484381148536073 Missing_rate\n",
      "Column pr_cyl_max__0 has 173 NaN values\n",
      "Column pr_cyl_max__0 has 0.0014484381148536073 Missing_rate\n",
      "Column se_mip__0 has 173 NaN values\n",
      "Column se_mip__0 has 0.0014484381148536073 Missing_rate\n",
      "Column te_exh_cyl_out__0_1 has 73 NaN values\n",
      "Column te_exh_cyl_out__0_1 has 0.0006111906496203082 Missing_rate\n",
      "Column fr_eng_setpoint has 0 NaN values\n",
      "Column fr_eng_setpoint has 0.0 Missing_rate\n",
      "Column te_air_scav_rec_iso has 51235 NaN values\n",
      "Column te_air_scav_rec_iso has 0.4289637388122807 Missing_rate\n",
      "Column pr_cyl_max_mv_iso has 51385 NaN values\n",
      "Column pr_cyl_max_mv_iso has 0.4302196100101307 Missing_rate\n",
      "Column pr_cyl_comp_mv_iso has 51385 NaN values\n",
      "Column pr_cyl_comp_mv_iso has 0.4302196100101307 Missing_rate\n",
      "Column fr_eng_ecs has 0 NaN values\n",
      "Column fr_eng_ecs has 0.0 Missing_rate\n",
      "Column pr_air_scav_iso has 51385 NaN values\n",
      "Column pr_air_scav_iso has 0.4302196100101307 Missing_rate\n",
      "Column engine_type_G95ME-C10.5-GI-EGRTC has 0 NaN values\n",
      "Column engine_type_G95ME-C10.5-GI-EGRTC has 0.0 Missing_rate\n",
      "Original size:119439, Sampled size: 39813\n",
      "Reshaped data:(39813, 31)\n",
      "Using CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n"
     ]
    },
    {
     "ename": "InvalidUrlException",
     "evalue": "Invalid url: http:127.0.0.1:5001/api/2.0/mlflow/experiments/get-by-name",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidURL\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/utils/rest_utils.py:189\u001b[0m, in \u001b[0;36mhttp_request\u001b[0;34m(host_creds, endpoint, method, max_retries, backoff_factor, backoff_jitter, extra_headers, retry_codes, timeout, raise_on_status, respect_retry_after_header, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 189\u001b[0m     \u001b[39mreturn\u001b[39;00m _get_http_response_with_retries(\n\u001b[1;32m    190\u001b[0m         method,\n\u001b[1;32m    191\u001b[0m         url,\n\u001b[1;32m    192\u001b[0m         max_retries,\n\u001b[1;32m    193\u001b[0m         backoff_factor,\n\u001b[1;32m    194\u001b[0m         backoff_jitter,\n\u001b[1;32m    195\u001b[0m         retry_codes,\n\u001b[1;32m    196\u001b[0m         raise_on_status,\n\u001b[1;32m    197\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    198\u001b[0m         verify\u001b[39m=\u001b[39;49mhost_creds\u001b[39m.\u001b[39;49mverify,\n\u001b[1;32m    199\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    200\u001b[0m         respect_retry_after_header\u001b[39m=\u001b[39;49mrespect_retry_after_header,\n\u001b[1;32m    201\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    202\u001b[0m     )\n\u001b[1;32m    203\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout \u001b[39mas\u001b[39;00m to:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/utils/request_utils.py:237\u001b[0m, in \u001b[0;36m_get_http_response_with_retries\u001b[0;34m(method, url, max_retries, backoff_factor, backoff_jitter, retry_codes, raise_on_status, allow_redirects, respect_retry_after_header, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m allow_redirects \u001b[39m=\u001b[39m env_value \u001b[39mif\u001b[39;00m allow_redirects \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m allow_redirects\n\u001b[0;32m--> 237\u001b[0m \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method, url, allow_redirects\u001b[39m=\u001b[39;49mallow_redirects, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/requests/sessions.py:575\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    563\u001b[0m req \u001b[39m=\u001b[39m Request(\n\u001b[1;32m    564\u001b[0m     method\u001b[39m=\u001b[39mmethod\u001b[39m.\u001b[39mupper(),\n\u001b[1;32m    565\u001b[0m     url\u001b[39m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    573\u001b[0m     hooks\u001b[39m=\u001b[39mhooks,\n\u001b[1;32m    574\u001b[0m )\n\u001b[0;32m--> 575\u001b[0m prep \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare_request(req)\n\u001b[1;32m    577\u001b[0m proxies \u001b[39m=\u001b[39m proxies \u001b[39mor\u001b[39;00m {}\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/requests/sessions.py:484\u001b[0m, in \u001b[0;36mSession.prepare_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    483\u001b[0m p \u001b[39m=\u001b[39m PreparedRequest()\n\u001b[0;32m--> 484\u001b[0m p\u001b[39m.\u001b[39;49mprepare(\n\u001b[1;32m    485\u001b[0m     method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod\u001b[39m.\u001b[39;49mupper(),\n\u001b[1;32m    486\u001b[0m     url\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49murl,\n\u001b[1;32m    487\u001b[0m     files\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mfiles,\n\u001b[1;32m    488\u001b[0m     data\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mdata,\n\u001b[1;32m    489\u001b[0m     json\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mjson,\n\u001b[1;32m    490\u001b[0m     headers\u001b[39m=\u001b[39;49mmerge_setting(\n\u001b[1;32m    491\u001b[0m         request\u001b[39m.\u001b[39;49mheaders, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mheaders, dict_class\u001b[39m=\u001b[39;49mCaseInsensitiveDict\n\u001b[1;32m    492\u001b[0m     ),\n\u001b[1;32m    493\u001b[0m     params\u001b[39m=\u001b[39;49mmerge_setting(request\u001b[39m.\u001b[39;49mparams, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams),\n\u001b[1;32m    494\u001b[0m     auth\u001b[39m=\u001b[39;49mmerge_setting(auth, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mauth),\n\u001b[1;32m    495\u001b[0m     cookies\u001b[39m=\u001b[39;49mmerged_cookies,\n\u001b[1;32m    496\u001b[0m     hooks\u001b[39m=\u001b[39;49mmerge_hooks(request\u001b[39m.\u001b[39;49mhooks, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhooks),\n\u001b[1;32m    497\u001b[0m )\n\u001b[1;32m    498\u001b[0m \u001b[39mreturn\u001b[39;00m p\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/requests/models.py:367\u001b[0m, in \u001b[0;36mPreparedRequest.prepare\u001b[0;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_method(method)\n\u001b[0;32m--> 367\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare_url(url, params)\n\u001b[1;32m    368\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_headers(headers)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/requests/models.py:444\u001b[0m, in \u001b[0;36mPreparedRequest.prepare_url\u001b[0;34m(self, url, params)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m host:\n\u001b[0;32m--> 444\u001b[0m     \u001b[39mraise\u001b[39;00m InvalidURL(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid URL \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m!r}\u001b[39;00m\u001b[39m: No host supplied\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    446\u001b[0m \u001b[39m# In general, we want to try IDNA encoding the hostname if the string contains\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[39m# non-ASCII characters. This allows users to automatically get the correct IDNA\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[39m# behaviour. For strings containing only ASCII characters, we need to also verify\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[39m# it doesn't start with a wildcard (*), before allowing the unencoded hostname.\u001b[39;00m\n",
      "\u001b[0;31mInvalidURL\u001b[0m: Invalid URL 'http:127.0.0.1:5001/api/2.0/mlflow/experiments/get-by-name': No host supplied",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mInvalidUrlException\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/Pypots_model_lighter.py:226\u001b[0m\n\u001b[1;32m    224\u001b[0m mlflow\u001b[39m.\u001b[39mset_tracking_uri(\u001b[39m\"\u001b[39m\u001b[39mhttp:127.0.0.1:5001\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    225\u001b[0m client \u001b[39m=\u001b[39m mlflow\u001b[39m.\u001b[39mtracking\u001b[39m.\u001b[39mMlflowClient()\n\u001b[0;32m--> 226\u001b[0m mlflow\u001b[39m.\u001b[39;49mset_experiment(\u001b[39m\"\u001b[39;49m\u001b[39mSAITS_3\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    227\u001b[0m SAITS_run_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mSAITS_1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    231\u001b[0m \u001b[39mwith\u001b[39;00m mlflow\u001b[39m.\u001b[39mstart_run(run_name\u001b[39m=\u001b[39mSAITS_run_name) \u001b[39mas\u001b[39;00m run:\n\u001b[1;32m    232\u001b[0m \u001b[39m# initialize the model (from example)\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/tracking/fluent.py:157\u001b[0m, in \u001b[0;36mset_experiment\u001b[0;34m(experiment_name, experiment_id)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39mwith\u001b[39;00m _experiment_lock:\n\u001b[1;32m    156\u001b[0m     \u001b[39mif\u001b[39;00m experiment_id \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m         experiment \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mget_experiment_by_name(experiment_name)\n\u001b[1;32m    158\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m experiment:\n\u001b[1;32m    159\u001b[0m             \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/tracking/client.py:1257\u001b[0m, in \u001b[0;36mMlflowClient.get_experiment_by_name\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mget_experiment_by_name\u001b[39m(\u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Experiment]:\n\u001b[1;32m   1226\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Retrieve an experiment by experiment name from the backend store\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m \n\u001b[1;32m   1228\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1255\u001b[0m \u001b[39m        Lifecycle_stage: active\u001b[39;00m\n\u001b[1;32m   1256\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1257\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tracking_client\u001b[39m.\u001b[39;49mget_experiment_by_name(name)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py:502\u001b[0m, in \u001b[0;36mTrackingServiceClient.get_experiment_by_name\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mget_experiment_by_name\u001b[39m(\u001b[39mself\u001b[39m, name):\n\u001b[1;32m    495\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    497\u001b[0m \u001b[39m        name: The experiment name.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[39m        :py:class:`mlflow.entities.Experiment`\u001b[39;00m\n\u001b[1;32m    501\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 502\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstore\u001b[39m.\u001b[39;49mget_experiment_by_name(name)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/store/tracking/rest_store.py:522\u001b[0m, in \u001b[0;36mRestStore.get_experiment_by_name\u001b[0;34m(self, experiment_name)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    521\u001b[0m     req_body \u001b[39m=\u001b[39m message_to_json(GetExperimentByName(experiment_name\u001b[39m=\u001b[39mexperiment_name))\n\u001b[0;32m--> 522\u001b[0m     response_proto \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_endpoint(GetExperimentByName, req_body)\n\u001b[1;32m    523\u001b[0m     \u001b[39mreturn\u001b[39;00m Experiment\u001b[39m.\u001b[39mfrom_proto(response_proto\u001b[39m.\u001b[39mexperiment)\n\u001b[1;32m    524\u001b[0m \u001b[39mexcept\u001b[39;00m MlflowException \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/store/tracking/rest_store.py:82\u001b[0m, in \u001b[0;36mRestStore._call_endpoint\u001b[0;34m(self, api, json_body, endpoint)\u001b[0m\n\u001b[1;32m     80\u001b[0m     endpoint, method \u001b[39m=\u001b[39m _METHOD_TO_INFO[api]\n\u001b[1;32m     81\u001b[0m response_proto \u001b[39m=\u001b[39m api\u001b[39m.\u001b[39mResponse()\n\u001b[0;32m---> 82\u001b[0m \u001b[39mreturn\u001b[39;00m call_endpoint(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_host_creds(), endpoint, method, json_body, response_proto)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/utils/rest_utils.py:365\u001b[0m, in \u001b[0;36mcall_endpoint\u001b[0;34m(host_creds, endpoint, method, json_body, response_proto, extra_headers)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[39mif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mGET\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    364\u001b[0m     call_kwargs[\u001b[39m\"\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m json_body\n\u001b[0;32m--> 365\u001b[0m     response \u001b[39m=\u001b[39m http_request(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcall_kwargs)\n\u001b[1;32m    366\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    367\u001b[0m     call_kwargs[\u001b[39m\"\u001b[39m\u001b[39mjson\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m json_body\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/utils/rest_utils.py:210\u001b[0m, in \u001b[0;36mhttp_request\u001b[0;34m(host_creds, endpoint, method, max_retries, backoff_factor, backoff_jitter, extra_headers, retry_codes, timeout, raise_on_status, respect_retry_after_header, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[39mraise\u001b[39;00m MlflowException(\n\u001b[1;32m    205\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAPI request to \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m failed with timeout exception \u001b[39m\u001b[39m{\u001b[39;00mto\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    206\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m To increase the timeout, set the environment variable \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    207\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mMLFLOW_HTTP_REQUEST_TIMEOUT\u001b[39m!s}\u001b[39;00m\u001b[39m to a larger value.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    208\u001b[0m     ) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mto\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mInvalidURL \u001b[39mas\u001b[39;00m iu:\n\u001b[0;32m--> 210\u001b[0m     \u001b[39mraise\u001b[39;00m InvalidUrlException(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid url: \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39miu\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    212\u001b[0m     \u001b[39mraise\u001b[39;00m MlflowException(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAPI request to \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m failed with exception \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mInvalidUrlException\u001b[0m: Invalid url: http:127.0.0.1:5001/api/2.0/mlflow/experiments/get-by-name"
     ]
    }
   ],
   "source": [
    "#Import Pypots Library\n",
    "from pypots.optim import Adam\n",
    "from pypots.imputation import SAITS\n",
    "#from pypots.utils.metrics import calc_mae\n",
    "from pypots.nn.functional import calc_mae\n",
    "\n",
    "\n",
    "import argparse\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import data_insight\n",
    "from data_insight import setup_duckdb\n",
    "from duckdb import DuckDBPyConnection as DuckDB\n",
    "from duckdb import DuckDBPyRelation as Relation\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "from duckdb import DuckDBPyConnection as DuckDB\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna \n",
    "from optuna.visualization import plot_optimization_history\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, Dataset\n",
    "from pygrinder.missing_completely_at_random import mcar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import sensor_imputation_thesis.shared.load_data as load\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "#PatchTST might be an ideal choise if SAITS is too slow \n",
    "\n",
    "##Drop columns with different indexes while loading data.. Or the mean values \n",
    "\n",
    "df=pd.read_parquet(\"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/Newdataframeforpypots.parquet\")\n",
    "\n",
    "len(df)\n",
    "\n",
    "#current length of the dataframe is 119439\n",
    "\n",
    "# Check nan values in each column\n",
    "for col in df.columns:\n",
    "    print(f\"Column {col} has {df[col].isna().sum()} NaN values\")\n",
    "    missing_rate=df[col].isna().sum()/len(df[col])\n",
    "    print(f\"Column {col} has {missing_rate} Missing_rate\")\n",
    "\n",
    "\n",
    "#Try with smaller dataset, size 4000\n",
    "##SAMPLE the percengtage of the dataset, df.sample (averagely pick samples)\n",
    "#not df.sample cuz it will randomly select \n",
    "original_size=len(df)\n",
    "desired_fraction=0.3 #Select data every 3 minutes \n",
    "step=int(1/desired_fraction) #step_size=10 (sample every 10th (3/10) minute)\n",
    "\n",
    "#Systematic sampling: Start at a random offset to avoid bias \n",
    "start=np.random.randint(0,step) #Random start between 0-9\n",
    "df1=df.iloc[start::step].reset_index(drop=True)\n",
    "\n",
    "print(f\"Original size:{len(df)}, Sampled size: {len(df1)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Custom Dataset class\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Data processing code\n",
    "sensor_cols = [col for col in df1.columns if col != \"time\"]\n",
    "data = df1[sensor_cols].values\n",
    "\n",
    "#¤get feature names for printing mae later \n",
    "feature_names=df1[sensor_cols].columns.tolist()\n",
    "\n",
    "## Convert data to 3D arrays of shape n_samples, n_timesteps, n_features, X_ori refers to the original data without missing values \n",
    "## Reconstruct all columns simultaneously  #num_features: 119\n",
    "n_features = data.shape[1]  # exclude the time column\n",
    "n_steps = 20 #60 (was 60 previously) #(TRY TO CHANGE HERE)  # # window length, 1440 steps = 24 hours of 1-minute data, but here is revised to 60 again\n",
    "#total_elements = data.shape[0] * data.shape[1]\n",
    "n_samples = data.shape[0] // n_steps \n",
    "\n",
    "\n",
    "\n",
    "# Reshape to (n_samples // n_steps, n_steps, n_features)\n",
    "#data_reshaped = data.reshape((n_samples, n_steps, n_features))\n",
    "data_reshaped=data[:n_samples*n_steps].reshape(n_samples,n_steps,n_features)\n",
    "print(f\"Reshaped data:{data.shape}\")\n",
    "\n",
    "#Split into train, test, val, fit scaler only on the train set (prevent data leakage)\n",
    "\n",
    "#train_size = int(0.6 * len(data))\n",
    "#val_size = int(0.2 * len(data))\n",
    "#test_size = len(data) - train_size - val_size\n",
    "\n",
    "#train_data = data_reshaped[:train_size]\n",
    "#val_data = data_reshaped[train_size:train_size + val_size]\n",
    "#test_data= data_reshaped[train_size + val_size:]\n",
    "\n",
    "\n",
    "#Apply time series split \n",
    "#Split into train(60%), val(20%), and test (20%)\n",
    "train_data, temp_data=train_test_split(data_reshaped,test_size=0.4,shuffle=True)\n",
    "val_data, test_data=train_test_split(temp_data, test_size=0.5, shuffle=False)\n",
    "\n",
    "##Normalization is important because of the nature of mse calculation of saits, columns with large \n",
    "#values dominate the loss, making metrics meaningless. SAITS computes MSE/MAE column-wise and averages \n",
    "#them across all columns \n",
    "#  Apply minmax scaler here \n",
    "#normalize each feature independently\n",
    "scalers={}\n",
    "\n",
    "\n",
    "#train_scaled = np.zeros_like(data_reshaped[train_size])  # Initialize the normalized data array\n",
    "#val_scaled=np.zeros_like(data_reshaped[train_size:train_size + val_size])\n",
    "#test_scaled=np.zeros_like(data_reshaped[train_size + val_size:])\n",
    "\n",
    "train_scaled = np.zeros_like(train_data)\n",
    "val_scaled = np.zeros_like(val_data)\n",
    "test_scaled = np.zeros_like(test_data)\n",
    "\n",
    "\n",
    "for i in range(data_reshaped.shape[2]):\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1)) #changed to -1,1\n",
    "    # Flatten timesteps and samples for scaling\n",
    "    train_scaled[:, :, i] = scaler.fit_transform(train_data[:, :, i].reshape(-1, 1)).reshape(train_data.shape[0], train_data.shape[1])\n",
    "    val_scaled[:, :, i] = scaler.transform(val_data[:, :, i].reshape(-1, 1)).reshape(val_data.shape[0], val_data.shape[1])\n",
    "    test_scaled[:, :, i] = scaler.transform(test_data[:, :, i].reshape(-1, 1)).reshape(test_data.shape[0], test_data.shape[1])\n",
    "    scalers[i] = scaler  # Save scalers to inverse-transform later\n",
    "\n",
    "#Inverse Scale\n",
    "def inverse_scale(imputation, scalers):\n",
    "    n_features = imputation.shape[2]\n",
    "    imputation_denorm = np.empty_like(imputation)\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        imputation_denorm[:, :, i] = scalers[i].inverse_transform(imputation[:, :, i].reshape(-1, 1)).reshape(imputation.shape[0], imputation.shape[1])\n",
    "    \n",
    "    return imputation_denorm  \n",
    "\n",
    "\n",
    "#Optional: Artificially mask. Mask 20% of the data (MIT part)\n",
    "def mcar_f(X, mask_ratio=0.2):\n",
    "    \"\"\"Apply MCAR only to observed values.\"\"\"\n",
    "    observed_mask=~np.isnan(X) #find observed positions\n",
    "    artificial_mask=mcar(X,mask_ratio).astype(bool) #generate MCAR mask, cast to boolean\n",
    "    #combine masks \n",
    "    combined_mask=observed_mask & artificial_mask\n",
    "\n",
    "    #Apply masking\n",
    "    X_masked=X.copy()\n",
    "    X_masked[combined_mask]=np.nan\n",
    "    return X_masked,combined_mask\n",
    "\n",
    "\n",
    "#Use mcar on validation data \n",
    "val_X_masked, val_mask =mcar_f(val_scaled)\n",
    "val_X_ori=val_scaled.copy() \n",
    "\n",
    "test_X_masked, test_mask =mcar_f(test_scaled)\n",
    "test_X_ori=test_scaled.copy() \n",
    "\n",
    "#?? Problem: Can't have the best input for testing\n",
    "#1.Create synthetic test_data cuz if I drop nan values for test set, there's basically nothing left\n",
    "#synthetic_data=np.random.randn(n_samples,n_steps,n_features)\n",
    "#test_X_masked,test_mask=mcar_f(synthetic_data)\n",
    "#test_X_ori=synthetic_data.copy() #Ground truth\n",
    "\n",
    "# 2, Ensure no NaN values in synthetic data\n",
    "#test_X_masked = np.nan_to_num(test_X_masked, nan=np.nanmean(test_X_masked))\n",
    "#test_X_ori = np.nan_to_num(test_X_ori, nan=np.nanmean(test_X_ori))\n",
    "\n",
    "\n",
    "\n",
    "class Config:\n",
    "    no_cuda = False\n",
    "    no_mps = False\n",
    "    seed = 1\n",
    "\n",
    "args=Config()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "use_mps = not args.no_mps and torch.backends.mps.is_available()\n",
    "\n",
    "if args.cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA\")\n",
    "elif use_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "\n",
    "#MLflow set up\n",
    "mlflow.set_tracking_uri(\"http:127.0.0.1:5001\")\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "mlflow.set_experiment(\"SAITS_3\")\n",
    "SAITS_run_name = \"SAITS_1\"\n",
    "\n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name=SAITS_run_name) as run:\n",
    "# initialize the model (from example)\n",
    "    saits = SAITS(\n",
    "        n_steps=data_reshaped.shape[1],\n",
    "        n_features=data_reshaped.shape[2],\n",
    "        #n_layers=2, #deep network(4) for long sequences, here is revised for 2 since it always shows cuda error \n",
    "        n_layers=3,\n",
    "        d_model=512,\n",
    "        optimizer=Adam(lr=1e-3),\n",
    "        #lr=params[\"lr\"],\n",
    "        ORT_weight=1,\n",
    "        MIT_weight=1,\n",
    "        #d_model=512,  #d_modle must equal n_heads * d_k\n",
    "        d_ffn=512,\n",
    "        n_heads=8,\n",
    "        d_k=64,\n",
    "        d_v=64,\n",
    "        dropout=0.1,\n",
    "        attn_dropout=0.1,\n",
    "        diagonal_attention_mask=True,  # otherwise the original self-attention mechanism will be applied\n",
    "        #ORT_weight=1,  # you can adjust the weight values of arguments ORT_weight\n",
    "       # and MIT_weight to make the SAITS model focus more on one task. Usually you can just leave them to the default values, i.e. 1.\n",
    "        #MIT_weight=1,\n",
    "        batch_size=5, #try with 5 to see if it works, was 4. \n",
    "        # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "        epochs=10, #try with 10 to see if it runs\n",
    "        # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "        # You can leave it to defualt as None to disable early stopping.\n",
    "        patience=6, #initially was 3, tried to increase to see more possibilities \n",
    "        # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "        # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "        #optimizer=Adam(lr=1e-3),\n",
    "        # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "        # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "        # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "        num_workers=0,\n",
    "        # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "        # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "        device=\"CUDA\", \n",
    "        # set the path for saving tensorboard and trained model files \n",
    "        saving_path=\"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/best_model\",\n",
    "        # only save the best model after training finished.\n",
    "        # You can also set it as \"better\" to save models performing better ever during training.\n",
    "        model_saving_strategy=\"best\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
    "#use original missigness for trainig, use the mcar masked points as ground truth\n",
    "saits.fit(train_set={\"X\": train_scaled}, val_set={\"X\": val_X_masked , \"X_ori\":val_X_ori })\n",
    "\n",
    "\n",
    "##drop null values in test set \n",
    "\n",
    "# Check for NaN values across the entire array\n",
    "#nan_mask = np.isnan(test_X).any(axis=(1, 2))\n",
    "\n",
    "# Filter out samples (rows) that contain NaN values\n",
    "#test_X_clean = test_X[~nan_mask]\n",
    "#test_X_ori = test_X_clean\n",
    "#test_X_masked, test_mask = mcar_f(test_X_clean)\n",
    "\n",
    "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
    "# Convert the numpy array to a dictionary\n",
    "\n",
    "\n",
    "# Ensure saits_imputation and test_X are numpy arrays\n",
    "#if not isinstance(saits_imputation, np.ndarray):\n",
    "    #saits_imputation = np.array(saits_imputation)\n",
    "#if not isinstance(test_X, np.ndarray):\n",
    "     #test_X = np.array(test_X)\n",
    "\n",
    "#Inverse Scale\n",
    "def inverse_scale(imputation, scalers):\n",
    "    n_features = imputation.shape[2]\n",
    "    imputation_denorm = np.empty_like(imputation)\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        imputation_denorm[:, :, i] = scalers[i].inverse_transform(imputation[:, :, i].reshape(-1, 1)).reshape(imputation.shape[0], imputation.shape[1])\n",
    "    \n",
    "    return imputation_denorm  \n",
    "\n",
    "    \n",
    "#Apply function to the dataset \n",
    "test_set_dict = {\"X\": test_X_masked}\n",
    "test_imputation = saits.predict(test_set_dict)\n",
    "test_imputation_array = test_imputation[\"imputation\"]\n",
    "test_imputation_denorm = inverse_scale(test_imputation_array, scalers)\n",
    "test_ori_denorm=inverse_scale(test_X_ori, scalers)\n",
    "\n",
    "if args.cuda:\n",
    "    torch.cuda.empty_cache() \n",
    "\n",
    "\n",
    "#Calculate metrics per-feature: mean absolute error on the ground truth (artificially-missing values)\n",
    "mae_per_feature=[]\n",
    "percentage_mae_per_feature=[]\n",
    "\n",
    "for i in range(n_features):\n",
    "    #Extract imputation and ground truth for feature i\n",
    "    imputation_i=test_imputation_denorm[:,:,i]\n",
    "    ground_truth_i=test_ori_denorm[:,:,i]\n",
    "    mask_i=test_mask[:,:,i]\n",
    "    # Check for NaN values\n",
    "    if np.isnan(imputation_i).any() or np.isnan(ground_truth_i).any():\n",
    "        print(f\"NaN values detected in feature {i}\")\n",
    "        continue  # Skip this feature if NaN values are found\n",
    "    #Filter only artificially masked positions\n",
    "    mae_i=calc_mae(imputation_i,ground_truth_i,mask_i)\n",
    "    mae_per_feature.append(mae_i)\n",
    "    #Calculate the original standard deviation for the feature\n",
    "    std_dev_i = np.std(ground_truth_i[mask_i == 1])\n",
    "    # Calculate the percentage of MAE relative to the standard deviation   \n",
    "    if std_dev_i != 0:\n",
    "        percentage_mae_i = (mae_i / std_dev_i) * 100\n",
    "        percentage_mae_per_feature.append(percentage_mae_i)\n",
    "    else:\n",
    "        percentage_mae_i = float('inf')\n",
    "            \n",
    "    mlflow.log_metric(f\"MAE_{feature_names[i]}\", mae_i)\n",
    "    mlflow.log_metric(f\"Percentage_MAE_{feature_names[i]}\", percentage_mae_i)\n",
    "\n",
    "    print(f\"MAE for {feature_names[i]}: {mae_i:.4f}\")\n",
    "    mlflow.log_metric(f\"MAE_{feature_names[i]}\", mae_i)\n",
    "\n",
    "# calculate average MAE \n",
    "avg_mae=np.mean(mae_per_feature)\n",
    "print(f\"Testing mean absolute error: {avg_mae:.4f}\")\n",
    "mlflow.log_metric(\"avg_mae\", avg_mae)\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4f69c1-55e4-463b-9d6d-ff703d132061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column time has 0 NaN values\n",
      "Column time has 0.0 Missing_rate\n",
      "Column fr_eng has 0 NaN values\n",
      "Column fr_eng has 0.0 Missing_rate\n",
      "Column te_exh_cyl_out__0 has 73 NaN values\n",
      "Column te_exh_cyl_out__0 has 0.0006111906496203082 Missing_rate\n",
      "Column pd_air_ic__0 has 73 NaN values\n",
      "Column pd_air_ic__0 has 0.0006111906496203082 Missing_rate\n",
      "Column pr_exh_turb_out__0 has 119439 NaN values\n",
      "Column pr_exh_turb_out__0 has 1.0 Missing_rate\n",
      "Column te_air_ic_out__0 has 73 NaN values\n",
      "Column te_air_ic_out__0 has 0.0006111906496203082 Missing_rate\n",
      "Column te_seawater has 73 NaN values\n",
      "Column te_seawater has 0.0006111906496203082 Missing_rate\n",
      "Column te_air_comp_in_a__0 has 119439 NaN values\n",
      "Column te_air_comp_in_a__0 has 1.0 Missing_rate\n",
      "Column te_air_comp_in_b__0 has 119439 NaN values\n",
      "Column te_air_comp_in_b__0 has 1.0 Missing_rate\n",
      "Column fr_tc__0 has 119439 NaN values\n",
      "Column fr_tc__0 has 1.0 Missing_rate\n",
      "Column pr_baro has 73 NaN values\n",
      "Column pr_baro has 0.0006111906496203082 Missing_rate\n",
      "Column pd_air_ic__0_1 has 73 NaN values\n",
      "Column pd_air_ic__0_1 has 0.0006111906496203082 Missing_rate\n",
      "Column pr_exh_rec has 73 NaN values\n",
      "Column pr_exh_rec has 0.0006111906496203082 Missing_rate\n",
      "Column te_exh_turb_in__0 has 119439 NaN values\n",
      "Column te_exh_turb_in__0 has 1.0 Missing_rate\n",
      "Column te_exh_turb_out__0 has 119439 NaN values\n",
      "Column te_exh_turb_out__0 has 1.0 Missing_rate\n",
      "Column bo_aux_blower_running has 0 NaN values\n",
      "Column bo_aux_blower_running has 0.0 Missing_rate\n",
      "Column re_eng_load has 173 NaN values\n",
      "Column re_eng_load has 0.0014484381148536073 Missing_rate\n",
      "Column pr_air_scav_ecs has 0 NaN values\n",
      "Column pr_air_scav_ecs has 0.0 Missing_rate\n",
      "Column pr_air_scav has 0 NaN values\n",
      "Column pr_air_scav has 0.0 Missing_rate\n",
      "Column te_air_scav_rec has 73 NaN values\n",
      "Column te_air_scav_rec has 0.0006111906496203082 Missing_rate\n",
      "Column te_air_ic_out__0_1 has 73 NaN values\n",
      "Column te_air_ic_out__0_1 has 0.0006111906496203082 Missing_rate\n",
      "Column pr_cyl_comp__0 has 173 NaN values\n",
      "Column pr_cyl_comp__0 has 0.0014484381148536073 Missing_rate\n",
      "Column pr_cyl_max__0 has 173 NaN values\n",
      "Column pr_cyl_max__0 has 0.0014484381148536073 Missing_rate\n",
      "Column se_mip__0 has 173 NaN values\n",
      "Column se_mip__0 has 0.0014484381148536073 Missing_rate\n",
      "Column te_exh_cyl_out__0_1 has 73 NaN values\n",
      "Column te_exh_cyl_out__0_1 has 0.0006111906496203082 Missing_rate\n",
      "Column fr_eng_setpoint has 0 NaN values\n",
      "Column fr_eng_setpoint has 0.0 Missing_rate\n",
      "Column te_air_scav_rec_iso has 51235 NaN values\n",
      "Column te_air_scav_rec_iso has 0.4289637388122807 Missing_rate\n",
      "Column pr_cyl_max_mv_iso has 51385 NaN values\n",
      "Column pr_cyl_max_mv_iso has 0.4302196100101307 Missing_rate\n",
      "Column pr_cyl_comp_mv_iso has 51385 NaN values\n",
      "Column pr_cyl_comp_mv_iso has 0.4302196100101307 Missing_rate\n",
      "Column fr_eng_ecs has 0 NaN values\n",
      "Column fr_eng_ecs has 0.0 Missing_rate\n",
      "Column pr_air_scav_iso has 51385 NaN values\n",
      "Column pr_air_scav_iso has 0.4302196100101307 Missing_rate\n",
      "Column engine_type_G95ME-C10.5-GI-EGRTC has 0 NaN values\n",
      "Column engine_type_G95ME-C10.5-GI-EGRTC has 0.0 Missing_rate\n",
      "Original size:119439, Sampled size: 39813\n",
      "Reshaped data:(39813, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 09:33:21 [INFO]: Using the given device: cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run SAITS_1 at: http://localhost:5000/#/experiments/4/runs/d12424341f064d51867433866c913569\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/4\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "You are trying to use CUDA for model training, but CUDA is not available in your environment.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/Pypots_model_lighter.py:233\u001b[0m\n\u001b[1;32m    227\u001b[0m SAITS_run_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mSAITS_1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    231\u001b[0m \u001b[39mwith\u001b[39;00m mlflow\u001b[39m.\u001b[39mstart_run(run_name\u001b[39m=\u001b[39mSAITS_run_name) \u001b[39mas\u001b[39;00m run:\n\u001b[1;32m    232\u001b[0m \u001b[39m# initialize the model (from example)\u001b[39;00m\n\u001b[0;32m--> 233\u001b[0m     saits \u001b[39m=\u001b[39m SAITS(\n\u001b[1;32m    234\u001b[0m         n_steps\u001b[39m=\u001b[39;49mdata_reshaped\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m],\n\u001b[1;32m    235\u001b[0m         n_features\u001b[39m=\u001b[39;49mdata_reshaped\u001b[39m.\u001b[39;49mshape[\u001b[39m2\u001b[39;49m],\n\u001b[1;32m    236\u001b[0m         \u001b[39m#n_layers=2, #deep network(4) for long sequences, here is revised for 2 since it always shows cuda error \u001b[39;49;00m\n\u001b[1;32m    237\u001b[0m         n_layers\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[1;32m    238\u001b[0m         d_model\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m,\n\u001b[1;32m    239\u001b[0m         optimizer\u001b[39m=\u001b[39;49mAdam(lr\u001b[39m=\u001b[39;49m\u001b[39m1e-3\u001b[39;49m),\n\u001b[1;32m    240\u001b[0m         \u001b[39m#lr=params[\"lr\"],\u001b[39;49;00m\n\u001b[1;32m    241\u001b[0m         ORT_weight\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m    242\u001b[0m         MIT_weight\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m    243\u001b[0m         \u001b[39m#d_model=512,  #d_modle must equal n_heads * d_k\u001b[39;49;00m\n\u001b[1;32m    244\u001b[0m         d_ffn\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m,\n\u001b[1;32m    245\u001b[0m         n_heads\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m,\n\u001b[1;32m    246\u001b[0m         d_k\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m,\n\u001b[1;32m    247\u001b[0m         d_v\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m,\n\u001b[1;32m    248\u001b[0m         dropout\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m,\n\u001b[1;32m    249\u001b[0m         attn_dropout\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m,\n\u001b[1;32m    250\u001b[0m         diagonal_attention_mask\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,  \u001b[39m# otherwise the original self-attention mechanism will be applied\u001b[39;49;00m\n\u001b[1;32m    251\u001b[0m         \u001b[39m#ORT_weight=1,  # you can adjust the weight values of arguments ORT_weight\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m        \u001b[39m# and MIT_weight to make the SAITS model focus more on one task. Usually you can just leave them to the default values, i.e. 1.\u001b[39;49;00m\n\u001b[1;32m    253\u001b[0m         \u001b[39m#MIT_weight=1,\u001b[39;49;00m\n\u001b[1;32m    254\u001b[0m         batch_size\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, \u001b[39m#try with 5 to see if it works, was 4. \u001b[39;49;00m\n\u001b[1;32m    255\u001b[0m         \u001b[39m# here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\u001b[39;49;00m\n\u001b[1;32m    256\u001b[0m         epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, \u001b[39m#try with 10 to see if it runs\u001b[39;49;00m\n\u001b[1;32m    257\u001b[0m         \u001b[39m# here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\u001b[39;49;00m\n\u001b[1;32m    258\u001b[0m         \u001b[39m# You can leave it to defualt as None to disable early stopping.\u001b[39;49;00m\n\u001b[1;32m    259\u001b[0m         patience\u001b[39m=\u001b[39;49m\u001b[39m6\u001b[39;49m, \u001b[39m#initially was 3, tried to increase to see more possibilities \u001b[39;49;00m\n\u001b[1;32m    260\u001b[0m         \u001b[39m# give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\u001b[39;49;00m\n\u001b[1;32m    261\u001b[0m         \u001b[39m# initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\u001b[39;49;00m\n\u001b[1;32m    262\u001b[0m         \u001b[39m#optimizer=Adam(lr=1e-3),\u001b[39;49;00m\n\u001b[1;32m    263\u001b[0m         \u001b[39m# this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\u001b[39;49;00m\n\u001b[1;32m    264\u001b[0m         \u001b[39m# Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\u001b[39;49;00m\n\u001b[1;32m    265\u001b[0m         \u001b[39m# You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\u001b[39;49;00m\n\u001b[1;32m    266\u001b[0m         num_workers\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m    267\u001b[0m         \u001b[39m# just leave it to default as None, PyPOTS will automatically assign the best device for you.\u001b[39;49;00m\n\u001b[1;32m    268\u001b[0m         \u001b[39m# Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\u001b[39;49;00m\n\u001b[1;32m    269\u001b[0m         device\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mCUDA\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[1;32m    270\u001b[0m         \u001b[39m# set the path for saving tensorboard and trained model files \u001b[39;49;00m\n\u001b[1;32m    271\u001b[0m         saving_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/best_model\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    272\u001b[0m         \u001b[39m# only save the best model after training finished.\u001b[39;49;00m\n\u001b[1;32m    273\u001b[0m         \u001b[39m# You can also set it as \"better\" to save models performing better ever during training.\u001b[39;49;00m\n\u001b[1;32m    274\u001b[0m         model_saving_strategy\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mbest\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    275\u001b[0m )\n\u001b[1;32m    281\u001b[0m \u001b[39m# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[39m#use original missigness for trainig, use the mcar masked points as ground truth\u001b[39;00m\n\u001b[1;32m    283\u001b[0m saits\u001b[39m.\u001b[39mfit(train_set\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m: train_scaled}, val_set\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m: val_X_masked , \u001b[39m\"\u001b[39m\u001b[39mX_ori\u001b[39m\u001b[39m\"\u001b[39m:val_X_ori })\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/pypots/imputation/saits/model.py:153\u001b[0m, in \u001b[0;36mSAITS.__init__\u001b[0;34m(self, n_steps, n_features, n_layers, d_model, n_heads, d_k, d_v, d_ffn, dropout, attn_dropout, diagonal_attention_mask, ORT_weight, MIT_weight, batch_size, epochs, patience, training_loss, validation_metric, optimizer, num_workers, device, saving_path, model_saving_strategy, verbose)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[1;32m    127\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    128\u001b[0m     n_steps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    151\u001b[0m     verbose: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    152\u001b[0m ):\n\u001b[0;32m--> 153\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    154\u001b[0m         training_loss\u001b[39m=\u001b[39;49mtraining_loss,\n\u001b[1;32m    155\u001b[0m         validation_metric\u001b[39m=\u001b[39;49mvalidation_metric,\n\u001b[1;32m    156\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    157\u001b[0m         epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m    158\u001b[0m         patience\u001b[39m=\u001b[39;49mpatience,\n\u001b[1;32m    159\u001b[0m         num_workers\u001b[39m=\u001b[39;49mnum_workers,\n\u001b[1;32m    160\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    161\u001b[0m         saving_path\u001b[39m=\u001b[39;49msaving_path,\n\u001b[1;32m    162\u001b[0m         model_saving_strategy\u001b[39m=\u001b[39;49mmodel_saving_strategy,\n\u001b[1;32m    163\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    164\u001b[0m     )\n\u001b[1;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m d_model \u001b[39m!=\u001b[39m n_heads \u001b[39m*\u001b[39m d_k:\n\u001b[1;32m    167\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    168\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m‼️ d_model must = n_heads * d_k, it should be divisible by n_heads \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    169\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mand the result should be equal to d_k, but got d_model=\u001b[39m\u001b[39m{\u001b[39;00md_model\u001b[39m}\u001b[39;00m\u001b[39m, n_heads=\u001b[39m\u001b[39m{\u001b[39;00mn_heads\u001b[39m}\u001b[39;00m\u001b[39m, d_k=\u001b[39m\u001b[39m{\u001b[39;00md_k\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    170\u001b[0m         )\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/pypots/imputation/base.py:244\u001b[0m, in \u001b[0;36mBaseNNImputer.__init__\u001b[0;34m(self, training_loss, validation_metric, batch_size, epochs, patience, num_workers, device, enable_amp, saving_path, model_saving_strategy, verbose)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[1;32m    231\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    232\u001b[0m     training_loss: Union[Criterion, \u001b[39mtype\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    242\u001b[0m     verbose: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    243\u001b[0m ):\n\u001b[0;32m--> 244\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    245\u001b[0m         training_loss\u001b[39m=\u001b[39;49mtraining_loss,\n\u001b[1;32m    246\u001b[0m         validation_metric\u001b[39m=\u001b[39;49mvalidation_metric,\n\u001b[1;32m    247\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    248\u001b[0m         epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m    249\u001b[0m         patience\u001b[39m=\u001b[39;49mpatience,\n\u001b[1;32m    250\u001b[0m         num_workers\u001b[39m=\u001b[39;49mnum_workers,\n\u001b[1;32m    251\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    252\u001b[0m         enable_amp\u001b[39m=\u001b[39;49menable_amp,\n\u001b[1;32m    253\u001b[0m         saving_path\u001b[39m=\u001b[39;49msaving_path,\n\u001b[1;32m    254\u001b[0m         model_saving_strategy\u001b[39m=\u001b[39;49mmodel_saving_strategy,\n\u001b[1;32m    255\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    256\u001b[0m     )\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/pypots/base.py:588\u001b[0m, in \u001b[0;36mBaseNNModel.__init__\u001b[0;34m(self, training_loss, validation_metric, batch_size, epochs, patience, num_workers, device, enable_amp, saving_path, model_saving_strategy, verbose)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[1;32m    575\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    576\u001b[0m     training_loss: Union[Criterion, \u001b[39mtype\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    586\u001b[0m     verbose: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    587\u001b[0m ):\n\u001b[0;32m--> 588\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    589\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    590\u001b[0m         enable_amp\u001b[39m=\u001b[39;49menable_amp,\n\u001b[1;32m    591\u001b[0m         saving_path\u001b[39m=\u001b[39;49msaving_path,\n\u001b[1;32m    592\u001b[0m         model_saving_strategy\u001b[39m=\u001b[39;49mmodel_saving_strategy,\n\u001b[1;32m    593\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    594\u001b[0m     )\n\u001b[1;32m    596\u001b[0m     \u001b[39m# check patience\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     \u001b[39mif\u001b[39;00m patience \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/pypots/base.py:109\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, device, enable_amp, saving_path, model_saving_strategy, verbose)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msummary_writer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[39m# set up the device for model running below\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setup_device(device)\n\u001b[1;32m    111\u001b[0m \u001b[39m# set up saving_path to save the trained model and training logs\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setup_path(saving_path)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/pypots/base.py:169\u001b[0m, in \u001b[0;36mBaseModel._setup_device\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39m# check CUDA availability if using CUDA\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice, \u001b[39mlist\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtype) \u001b[39mor\u001b[39;00m (\n\u001b[1;32m    166\u001b[0m     \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice, torch\u001b[39m.\u001b[39mdevice) \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype\n\u001b[1;32m    167\u001b[0m ):\n\u001b[1;32m    168\u001b[0m     \u001b[39massert\u001b[39;00m (\n\u001b[0;32m--> 169\u001b[0m         torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39mand\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice_count() \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    170\u001b[0m     ), \u001b[39m\"\u001b[39m\u001b[39mYou are trying to use CUDA for model training, but CUDA is not available in your environment.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mgetenv(\u001b[39m\"\u001b[39m\u001b[39mENABLE_AMP\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    173\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menable_amp:\n",
      "\u001b[0;31mAssertionError\u001b[0m: You are trying to use CUDA for model training, but CUDA is not available in your environment."
     ]
    }
   ],
   "source": [
    "#Import Pypots Library\n",
    "from pypots.optim import Adam\n",
    "from pypots.imputation import SAITS\n",
    "#from pypots.utils.metrics import calc_mae\n",
    "from pypots.nn.functional import calc_mae\n",
    "\n",
    "\n",
    "import argparse\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import data_insight\n",
    "from data_insight import setup_duckdb\n",
    "from duckdb import DuckDBPyConnection as DuckDB\n",
    "from duckdb import DuckDBPyRelation as Relation\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "from duckdb import DuckDBPyConnection as DuckDB\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna \n",
    "from optuna.visualization import plot_optimization_history\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, Dataset\n",
    "from pygrinder.missing_completely_at_random import mcar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import sensor_imputation_thesis.shared.load_data as load\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "#PatchTST might be an ideal choise if SAITS is too slow \n",
    "\n",
    "##Drop columns with different indexes while loading data.. Or the mean values \n",
    "\n",
    "df=pd.read_parquet(\"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/Newdataframeforpypots.parquet\")\n",
    "\n",
    "len(df)\n",
    "\n",
    "#current length of the dataframe is 119439\n",
    "\n",
    "# Check nan values in each column\n",
    "for col in df.columns:\n",
    "    print(f\"Column {col} has {df[col].isna().sum()} NaN values\")\n",
    "    missing_rate=df[col].isna().sum()/len(df[col])\n",
    "    print(f\"Column {col} has {missing_rate} Missing_rate\")\n",
    "\n",
    "\n",
    "#Try with smaller dataset, size 4000\n",
    "##SAMPLE the percengtage of the dataset, df.sample (averagely pick samples)\n",
    "#not df.sample cuz it will randomly select \n",
    "original_size=len(df)\n",
    "desired_fraction=0.3 #Select data every 3 minutes \n",
    "step=int(1/desired_fraction) #step_size=10 (sample every 10th (3/10) minute)\n",
    "\n",
    "#Systematic sampling: Start at a random offset to avoid bias \n",
    "start=np.random.randint(0,step) #Random start between 0-9\n",
    "df1=df.iloc[start::step].reset_index(drop=True)\n",
    "\n",
    "print(f\"Original size:{len(df)}, Sampled size: {len(df1)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Custom Dataset class\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Data processing code\n",
    "sensor_cols = [col for col in df1.columns if col != \"time\"]\n",
    "data = df1[sensor_cols].values\n",
    "\n",
    "#¤get feature names for printing mae later \n",
    "feature_names=df1[sensor_cols].columns.tolist()\n",
    "\n",
    "## Convert data to 3D arrays of shape n_samples, n_timesteps, n_features, X_ori refers to the original data without missing values \n",
    "## Reconstruct all columns simultaneously  #num_features: 119\n",
    "n_features = data.shape[1]  # exclude the time column\n",
    "n_steps = 20 #60 (was 60 previously) #(TRY TO CHANGE HERE)  # # window length, 1440 steps = 24 hours of 1-minute data, but here is revised to 60 again\n",
    "#total_elements = data.shape[0] * data.shape[1]\n",
    "n_samples = data.shape[0] // n_steps \n",
    "\n",
    "\n",
    "\n",
    "# Reshape to (n_samples // n_steps, n_steps, n_features)\n",
    "#data_reshaped = data.reshape((n_samples, n_steps, n_features))\n",
    "data_reshaped=data[:n_samples*n_steps].reshape(n_samples,n_steps,n_features)\n",
    "print(f\"Reshaped data:{data.shape}\")\n",
    "\n",
    "#Split into train, test, val, fit scaler only on the train set (prevent data leakage)\n",
    "\n",
    "#train_size = int(0.6 * len(data))\n",
    "#val_size = int(0.2 * len(data))\n",
    "#test_size = len(data) - train_size - val_size\n",
    "\n",
    "#train_data = data_reshaped[:train_size]\n",
    "#val_data = data_reshaped[train_size:train_size + val_size]\n",
    "#test_data= data_reshaped[train_size + val_size:]\n",
    "\n",
    "\n",
    "#Apply time series split \n",
    "#Split into train(60%), val(20%), and test (20%)\n",
    "train_data, temp_data=train_test_split(data_reshaped,test_size=0.4,shuffle=True)\n",
    "val_data, test_data=train_test_split(temp_data, test_size=0.5, shuffle=False)\n",
    "\n",
    "##Normalization is important because of the nature of mse calculation of saits, columns with large \n",
    "#values dominate the loss, making metrics meaningless. SAITS computes MSE/MAE column-wise and averages \n",
    "#them across all columns \n",
    "#  Apply minmax scaler here \n",
    "#normalize each feature independently\n",
    "scalers={}\n",
    "\n",
    "\n",
    "#train_scaled = np.zeros_like(data_reshaped[train_size])  # Initialize the normalized data array\n",
    "#val_scaled=np.zeros_like(data_reshaped[train_size:train_size + val_size])\n",
    "#test_scaled=np.zeros_like(data_reshaped[train_size + val_size:])\n",
    "\n",
    "train_scaled = np.zeros_like(train_data)\n",
    "val_scaled = np.zeros_like(val_data)\n",
    "test_scaled = np.zeros_like(test_data)\n",
    "\n",
    "\n",
    "for i in range(data_reshaped.shape[2]):\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1)) #changed to -1,1\n",
    "    # Flatten timesteps and samples for scaling\n",
    "    train_scaled[:, :, i] = scaler.fit_transform(train_data[:, :, i].reshape(-1, 1)).reshape(train_data.shape[0], train_data.shape[1])\n",
    "    val_scaled[:, :, i] = scaler.transform(val_data[:, :, i].reshape(-1, 1)).reshape(val_data.shape[0], val_data.shape[1])\n",
    "    test_scaled[:, :, i] = scaler.transform(test_data[:, :, i].reshape(-1, 1)).reshape(test_data.shape[0], test_data.shape[1])\n",
    "    scalers[i] = scaler  # Save scalers to inverse-transform later\n",
    "\n",
    "#Inverse Scale\n",
    "def inverse_scale(imputation, scalers):\n",
    "    n_features = imputation.shape[2]\n",
    "    imputation_denorm = np.empty_like(imputation)\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        imputation_denorm[:, :, i] = scalers[i].inverse_transform(imputation[:, :, i].reshape(-1, 1)).reshape(imputation.shape[0], imputation.shape[1])\n",
    "    \n",
    "    return imputation_denorm  \n",
    "\n",
    "\n",
    "#Optional: Artificially mask. Mask 20% of the data (MIT part)\n",
    "def mcar_f(X, mask_ratio=0.2):\n",
    "    \"\"\"Apply MCAR only to observed values.\"\"\"\n",
    "    observed_mask=~np.isnan(X) #find observed positions\n",
    "    artificial_mask=mcar(X,mask_ratio).astype(bool) #generate MCAR mask, cast to boolean\n",
    "    #combine masks \n",
    "    combined_mask=observed_mask & artificial_mask\n",
    "\n",
    "    #Apply masking\n",
    "    X_masked=X.copy()\n",
    "    X_masked[combined_mask]=np.nan\n",
    "    return X_masked,combined_mask\n",
    "\n",
    "\n",
    "#Use mcar on validation data \n",
    "val_X_masked, val_mask =mcar_f(val_scaled)\n",
    "val_X_ori=val_scaled.copy() \n",
    "\n",
    "test_X_masked, test_mask =mcar_f(test_scaled)\n",
    "test_X_ori=test_scaled.copy() \n",
    "\n",
    "#?? Problem: Can't have the best input for testing\n",
    "#1.Create synthetic test_data cuz if I drop nan values for test set, there's basically nothing left\n",
    "#synthetic_data=np.random.randn(n_samples,n_steps,n_features)\n",
    "#test_X_masked,test_mask=mcar_f(synthetic_data)\n",
    "#test_X_ori=synthetic_data.copy() #Ground truth\n",
    "\n",
    "# 2, Ensure no NaN values in synthetic data\n",
    "#test_X_masked = np.nan_to_num(test_X_masked, nan=np.nanmean(test_X_masked))\n",
    "#test_X_ori = np.nan_to_num(test_X_ori, nan=np.nanmean(test_X_ori))\n",
    "\n",
    "\n",
    "\n",
    "class Config:\n",
    "    no_cuda = False\n",
    "    no_mps = False\n",
    "    seed = 1\n",
    "\n",
    "args=Config()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "use_mps = not args.no_mps and torch.backends.mps.is_available()\n",
    "\n",
    "if args.cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA\")\n",
    "elif use_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "\n",
    "#MLflow set up\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "mlflow.set_experiment(\"SAITS_3\")\n",
    "SAITS_run_name = \"SAITS_1\"\n",
    "\n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name=SAITS_run_name) as run:\n",
    "# initialize the model (from example)\n",
    "    saits = SAITS(\n",
    "        n_steps=data_reshaped.shape[1],\n",
    "        n_features=data_reshaped.shape[2],\n",
    "        #n_layers=2, #deep network(4) for long sequences, here is revised for 2 since it always shows cuda error \n",
    "        n_layers=3,\n",
    "        d_model=512,\n",
    "        optimizer=Adam(lr=1e-3),\n",
    "        #lr=params[\"lr\"],\n",
    "        ORT_weight=1,\n",
    "        MIT_weight=1,\n",
    "        #d_model=512,  #d_modle must equal n_heads * d_k\n",
    "        d_ffn=512,\n",
    "        n_heads=8,\n",
    "        d_k=64,\n",
    "        d_v=64,\n",
    "        dropout=0.1,\n",
    "        attn_dropout=0.1,\n",
    "        diagonal_attention_mask=True,  # otherwise the original self-attention mechanism will be applied\n",
    "        #ORT_weight=1,  # you can adjust the weight values of arguments ORT_weight\n",
    "       # and MIT_weight to make the SAITS model focus more on one task. Usually you can just leave them to the default values, i.e. 1.\n",
    "        #MIT_weight=1,\n",
    "        batch_size=5, #try with 5 to see if it works, was 4. \n",
    "        # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "        epochs=10, #try with 10 to see if it runs\n",
    "        # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "        # You can leave it to defualt as None to disable early stopping.\n",
    "        patience=6, #initially was 3, tried to increase to see more possibilities \n",
    "        # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "        # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "        #optimizer=Adam(lr=1e-3),\n",
    "        # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "        # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "        # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "        num_workers=0,\n",
    "        # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "        # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "        device=\"CUDA\", \n",
    "        # set the path for saving tensorboard and trained model files \n",
    "        saving_path=\"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/best_model\",\n",
    "        # only save the best model after training finished.\n",
    "        # You can also set it as \"better\" to save models performing better ever during training.\n",
    "        model_saving_strategy=\"best\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
    "#use original missigness for trainig, use the mcar masked points as ground truth\n",
    "saits.fit(train_set={\"X\": train_scaled}, val_set={\"X\": val_X_masked , \"X_ori\":val_X_ori })\n",
    "\n",
    "\n",
    "##drop null values in test set \n",
    "\n",
    "# Check for NaN values across the entire array\n",
    "#nan_mask = np.isnan(test_X).any(axis=(1, 2))\n",
    "\n",
    "# Filter out samples (rows) that contain NaN values\n",
    "#test_X_clean = test_X[~nan_mask]\n",
    "#test_X_ori = test_X_clean\n",
    "#test_X_masked, test_mask = mcar_f(test_X_clean)\n",
    "\n",
    "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
    "# Convert the numpy array to a dictionary\n",
    "\n",
    "\n",
    "# Ensure saits_imputation and test_X are numpy arrays\n",
    "#if not isinstance(saits_imputation, np.ndarray):\n",
    "    #saits_imputation = np.array(saits_imputation)\n",
    "#if not isinstance(test_X, np.ndarray):\n",
    "     #test_X = np.array(test_X)\n",
    "\n",
    "#Inverse Scale\n",
    "def inverse_scale(imputation, scalers):\n",
    "    n_features = imputation.shape[2]\n",
    "    imputation_denorm = np.empty_like(imputation)\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        imputation_denorm[:, :, i] = scalers[i].inverse_transform(imputation[:, :, i].reshape(-1, 1)).reshape(imputation.shape[0], imputation.shape[1])\n",
    "    \n",
    "    return imputation_denorm  \n",
    "\n",
    "    \n",
    "#Apply function to the dataset \n",
    "test_set_dict = {\"X\": test_X_masked}\n",
    "test_imputation = saits.predict(test_set_dict)\n",
    "test_imputation_array = test_imputation[\"imputation\"]\n",
    "test_imputation_denorm = inverse_scale(test_imputation_array, scalers)\n",
    "test_ori_denorm=inverse_scale(test_X_ori, scalers)\n",
    "\n",
    "if args.cuda:\n",
    "    torch.cuda.empty_cache() \n",
    "\n",
    "\n",
    "#Calculate metrics per-feature: mean absolute error on the ground truth (artificially-missing values)\n",
    "mae_per_feature=[]\n",
    "percentage_mae_per_feature=[]\n",
    "\n",
    "for i in range(n_features):\n",
    "    #Extract imputation and ground truth for feature i\n",
    "    imputation_i=test_imputation_denorm[:,:,i]\n",
    "    ground_truth_i=test_ori_denorm[:,:,i]\n",
    "    mask_i=test_mask[:,:,i]\n",
    "    # Check for NaN values\n",
    "    if np.isnan(imputation_i).any() or np.isnan(ground_truth_i).any():\n",
    "        print(f\"NaN values detected in feature {i}\")\n",
    "        continue  # Skip this feature if NaN values are found\n",
    "    #Filter only artificially masked positions\n",
    "    mae_i=calc_mae(imputation_i,ground_truth_i,mask_i)\n",
    "    mae_per_feature.append(mae_i)\n",
    "    #Calculate the original standard deviation for the feature\n",
    "    std_dev_i = np.std(ground_truth_i[mask_i == 1])\n",
    "    # Calculate the percentage of MAE relative to the standard deviation   \n",
    "    if std_dev_i != 0:\n",
    "        percentage_mae_i = (mae_i / std_dev_i) * 100\n",
    "        percentage_mae_per_feature.append(percentage_mae_i)\n",
    "    else:\n",
    "        percentage_mae_i = float('inf')\n",
    "            \n",
    "    mlflow.log_metric(f\"MAE_{feature_names[i]}\", mae_i)\n",
    "    mlflow.log_metric(f\"Percentage_MAE_{feature_names[i]}\", percentage_mae_i)\n",
    "\n",
    "    print(f\"MAE for {feature_names[i]}: {mae_i:.4f}\")\n",
    "    mlflow.log_metric(f\"MAE_{feature_names[i]}\", mae_i)\n",
    "\n",
    "# calculate average MAE \n",
    "avg_mae=np.mean(mae_per_feature)\n",
    "print(f\"Testing mean absolute error: {avg_mae:.4f}\")\n",
    "mlflow.log_metric(\"avg_mae\", avg_mae)\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d77997-e43e-4259-9025-74365aef6ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column time has 0 NaN values\n",
      "Column time has 0.0 Missing_rate\n",
      "Column fr_eng has 0 NaN values\n",
      "Column fr_eng has 0.0 Missing_rate\n",
      "Column te_exh_cyl_out__0 has 73 NaN values\n",
      "Column te_exh_cyl_out__0 has 0.0006111906496203082 Missing_rate\n",
      "Column pd_air_ic__0 has 73 NaN values\n",
      "Column pd_air_ic__0 has 0.0006111906496203082 Missing_rate\n",
      "Column pr_exh_turb_out__0 has 119439 NaN values\n",
      "Column pr_exh_turb_out__0 has 1.0 Missing_rate\n",
      "Column te_air_ic_out__0 has 73 NaN values\n",
      "Column te_air_ic_out__0 has 0.0006111906496203082 Missing_rate\n",
      "Column te_seawater has 73 NaN values\n",
      "Column te_seawater has 0.0006111906496203082 Missing_rate\n",
      "Column te_air_comp_in_a__0 has 119439 NaN values\n",
      "Column te_air_comp_in_a__0 has 1.0 Missing_rate\n",
      "Column te_air_comp_in_b__0 has 119439 NaN values\n",
      "Column te_air_comp_in_b__0 has 1.0 Missing_rate\n",
      "Column fr_tc__0 has 119439 NaN values\n",
      "Column fr_tc__0 has 1.0 Missing_rate\n",
      "Column pr_baro has 73 NaN values\n",
      "Column pr_baro has 0.0006111906496203082 Missing_rate\n",
      "Column pd_air_ic__0_1 has 73 NaN values\n",
      "Column pd_air_ic__0_1 has 0.0006111906496203082 Missing_rate\n",
      "Column pr_exh_rec has 73 NaN values\n",
      "Column pr_exh_rec has 0.0006111906496203082 Missing_rate\n",
      "Column te_exh_turb_in__0 has 119439 NaN values\n",
      "Column te_exh_turb_in__0 has 1.0 Missing_rate\n",
      "Column te_exh_turb_out__0 has 119439 NaN values\n",
      "Column te_exh_turb_out__0 has 1.0 Missing_rate\n",
      "Column bo_aux_blower_running has 0 NaN values\n",
      "Column bo_aux_blower_running has 0.0 Missing_rate\n",
      "Column re_eng_load has 173 NaN values\n",
      "Column re_eng_load has 0.0014484381148536073 Missing_rate\n",
      "Column pr_air_scav_ecs has 0 NaN values\n",
      "Column pr_air_scav_ecs has 0.0 Missing_rate\n",
      "Column pr_air_scav has 0 NaN values\n",
      "Column pr_air_scav has 0.0 Missing_rate\n",
      "Column te_air_scav_rec has 73 NaN values\n",
      "Column te_air_scav_rec has 0.0006111906496203082 Missing_rate\n",
      "Column te_air_ic_out__0_1 has 73 NaN values\n",
      "Column te_air_ic_out__0_1 has 0.0006111906496203082 Missing_rate\n",
      "Column pr_cyl_comp__0 has 173 NaN values\n",
      "Column pr_cyl_comp__0 has 0.0014484381148536073 Missing_rate\n",
      "Column pr_cyl_max__0 has 173 NaN values\n",
      "Column pr_cyl_max__0 has 0.0014484381148536073 Missing_rate\n",
      "Column se_mip__0 has 173 NaN values\n",
      "Column se_mip__0 has 0.0014484381148536073 Missing_rate\n",
      "Column te_exh_cyl_out__0_1 has 73 NaN values\n",
      "Column te_exh_cyl_out__0_1 has 0.0006111906496203082 Missing_rate\n",
      "Column fr_eng_setpoint has 0 NaN values\n",
      "Column fr_eng_setpoint has 0.0 Missing_rate\n",
      "Column te_air_scav_rec_iso has 51235 NaN values\n",
      "Column te_air_scav_rec_iso has 0.4289637388122807 Missing_rate\n",
      "Column pr_cyl_max_mv_iso has 51385 NaN values\n",
      "Column pr_cyl_max_mv_iso has 0.4302196100101307 Missing_rate\n",
      "Column pr_cyl_comp_mv_iso has 51385 NaN values\n",
      "Column pr_cyl_comp_mv_iso has 0.4302196100101307 Missing_rate\n",
      "Column fr_eng_ecs has 0 NaN values\n",
      "Column fr_eng_ecs has 0.0 Missing_rate\n",
      "Column pr_air_scav_iso has 51385 NaN values\n",
      "Column pr_air_scav_iso has 0.4302196100101307 Missing_rate\n",
      "Column engine_type_G95ME-C10.5-GI-EGRTC has 0 NaN values\n",
      "Column engine_type_G95ME-C10.5-GI-EGRTC has 0.0 Missing_rate\n",
      "Original size:119439, Sampled size: 39813\n",
      "Reshaped data:(39813, 31)\n",
      "Using CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "2025-05-14 09:33:59 [INFO]: Using the given device: cpu\n",
      "2025-05-14 09:33:59 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/best_model/20250514_T093359\n",
      "2025-05-14 09:33:59 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/best_model/20250514_T093359/tensorboard\n",
      "2025-05-14 09:33:59 [INFO]: Using customized MAE as the training loss function.\n",
      "2025-05-14 09:33:59 [INFO]: Using customized MSE as the validation metric function.\n",
      "2025-05-14 09:33:59 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 9,554,538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run SAITS_1 at: http://localhost:5000/#/experiments/4/runs/f2158cd06733406bbaa47c820046834b\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 09:34:40 [INFO]: Epoch 001 - training loss (MAE): 0.5010, validation MSE: 0.2346\n",
      "2025-05-14 09:35:21 [INFO]: Epoch 002 - training loss (MAE): 0.3689, validation MSE: 0.2470\n",
      "2025-05-14 09:36:11 [INFO]: Epoch 003 - training loss (MAE): 0.3208, validation MSE: 0.1938\n",
      "2025-05-14 09:37:10 [INFO]: Epoch 004 - training loss (MAE): 0.2752, validation MSE: 0.1970\n",
      "2025-05-14 09:38:18 [INFO]: Epoch 005 - training loss (MAE): 0.2557, validation MSE: 0.1862\n",
      "2025-05-14 09:39:34 [INFO]: Epoch 006 - training loss (MAE): 0.2496, validation MSE: 0.2211\n",
      "2025-05-14 09:40:49 [INFO]: Epoch 007 - training loss (MAE): 0.2405, validation MSE: 0.2205\n",
      "2025-05-14 09:42:02 [INFO]: Epoch 008 - training loss (MAE): 0.2364, validation MSE: 0.2109\n",
      "2025-05-14 09:43:16 [INFO]: Epoch 009 - training loss (MAE): 0.2375, validation MSE: 0.2272\n",
      "2025-05-14 09:44:30 [INFO]: Epoch 010 - training loss (MAE): 0.2254, validation MSE: 0.2232\n",
      "2025-05-14 09:44:30 [INFO]: Finished training. The best model is from epoch#5.\n",
      "2025-05-14 09:44:30 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/best_model/20250514_T093359/SAITS.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for fr_eng: 0.1834\n",
      "NaN values detected in feature 1\n",
      "NaN values detected in feature 2\n",
      "NaN values detected in feature 3\n",
      "NaN values detected in feature 4\n",
      "NaN values detected in feature 5\n",
      "NaN values detected in feature 6\n",
      "NaN values detected in feature 7\n",
      "NaN values detected in feature 8\n",
      "NaN values detected in feature 9\n",
      "NaN values detected in feature 10\n",
      "NaN values detected in feature 11\n",
      "NaN values detected in feature 12\n",
      "NaN values detected in feature 13\n",
      "MAE for bo_aux_blower_running: 0.4758\n",
      "NaN values detected in feature 15\n",
      "MAE for pr_air_scav_ecs: 59725.1059\n",
      "MAE for pr_air_scav: 59225.6507\n",
      "NaN values detected in feature 18\n",
      "NaN values detected in feature 19\n",
      "NaN values detected in feature 20\n",
      "NaN values detected in feature 21\n",
      "NaN values detected in feature 22\n",
      "NaN values detected in feature 23\n",
      "MAE for fr_eng_setpoint: 0.1787\n",
      "NaN values detected in feature 25\n",
      "NaN values detected in feature 26\n",
      "NaN values detected in feature 27\n",
      "MAE for fr_eng_ecs: 0.1799\n",
      "NaN values detected in feature 29\n",
      "MAE for engine_type_G95ME-C10.5-GI-EGRTC: 0.0083\n",
      "Testing mean absolute error: 16993.1118\n",
      "🏃 View run incongruous-gull-870 at: http://localhost:5000/#/experiments/4/runs/944e7ecf78bd48d98fdea55a5726bae8\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/4\n"
     ]
    }
   ],
   "source": [
    "#Import Pypots Library\n",
    "from pypots.optim import Adam\n",
    "from pypots.imputation import SAITS\n",
    "#from pypots.utils.metrics import calc_mae\n",
    "from pypots.nn.functional import calc_mae\n",
    "\n",
    "\n",
    "import argparse\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import data_insight\n",
    "from data_insight import setup_duckdb\n",
    "from duckdb import DuckDBPyConnection as DuckDB\n",
    "from duckdb import DuckDBPyRelation as Relation\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "from duckdb import DuckDBPyConnection as DuckDB\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna \n",
    "from optuna.visualization import plot_optimization_history\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, Dataset\n",
    "from pygrinder.missing_completely_at_random import mcar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import sensor_imputation_thesis.shared.load_data as load\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "#PatchTST might be an ideal choise if SAITS is too slow \n",
    "\n",
    "##Drop columns with different indexes while loading data.. Or the mean values \n",
    "\n",
    "df=pd.read_parquet(\"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/Newdataframeforpypots.parquet\")\n",
    "\n",
    "len(df)\n",
    "\n",
    "#current length of the dataframe is 119439\n",
    "\n",
    "# Check nan values in each column\n",
    "for col in df.columns:\n",
    "    print(f\"Column {col} has {df[col].isna().sum()} NaN values\")\n",
    "    missing_rate=df[col].isna().sum()/len(df[col])\n",
    "    print(f\"Column {col} has {missing_rate} Missing_rate\")\n",
    "\n",
    "\n",
    "#Try with smaller dataset, size 4000\n",
    "##SAMPLE the percengtage of the dataset, df.sample (averagely pick samples)\n",
    "#not df.sample cuz it will randomly select \n",
    "original_size=len(df)\n",
    "desired_fraction=0.3 #Select data every 3 minutes \n",
    "step=int(1/desired_fraction) #step_size=10 (sample every 10th (3/10) minute)\n",
    "\n",
    "#Systematic sampling: Start at a random offset to avoid bias \n",
    "start=np.random.randint(0,step) #Random start between 0-9\n",
    "df1=df.iloc[start::step].reset_index(drop=True)\n",
    "\n",
    "print(f\"Original size:{len(df)}, Sampled size: {len(df1)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Custom Dataset class\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Data processing code\n",
    "sensor_cols = [col for col in df1.columns if col != \"time\"]\n",
    "data = df1[sensor_cols].values\n",
    "\n",
    "#¤get feature names for printing mae later \n",
    "feature_names=df1[sensor_cols].columns.tolist()\n",
    "\n",
    "## Convert data to 3D arrays of shape n_samples, n_timesteps, n_features, X_ori refers to the original data without missing values \n",
    "## Reconstruct all columns simultaneously  #num_features: 119\n",
    "n_features = data.shape[1]  # exclude the time column\n",
    "n_steps = 20 #60 (was 60 previously) #(TRY TO CHANGE HERE)  # # window length, 1440 steps = 24 hours of 1-minute data, but here is revised to 60 again\n",
    "#total_elements = data.shape[0] * data.shape[1]\n",
    "n_samples = data.shape[0] // n_steps \n",
    "\n",
    "\n",
    "\n",
    "# Reshape to (n_samples // n_steps, n_steps, n_features)\n",
    "#data_reshaped = data.reshape((n_samples, n_steps, n_features))\n",
    "data_reshaped=data[:n_samples*n_steps].reshape(n_samples,n_steps,n_features)\n",
    "print(f\"Reshaped data:{data.shape}\")\n",
    "\n",
    "#Split into train, test, val, fit scaler only on the train set (prevent data leakage)\n",
    "\n",
    "#train_size = int(0.6 * len(data))\n",
    "#val_size = int(0.2 * len(data))\n",
    "#test_size = len(data) - train_size - val_size\n",
    "\n",
    "#train_data = data_reshaped[:train_size]\n",
    "#val_data = data_reshaped[train_size:train_size + val_size]\n",
    "#test_data= data_reshaped[train_size + val_size:]\n",
    "\n",
    "\n",
    "#Apply time series split \n",
    "#Split into train(60%), val(20%), and test (20%)\n",
    "train_data, temp_data=train_test_split(data_reshaped,test_size=0.4,shuffle=True)\n",
    "val_data, test_data=train_test_split(temp_data, test_size=0.5, shuffle=False)\n",
    "\n",
    "##Normalization is important because of the nature of mse calculation of saits, columns with large \n",
    "#values dominate the loss, making metrics meaningless. SAITS computes MSE/MAE column-wise and averages \n",
    "#them across all columns \n",
    "#  Apply minmax scaler here \n",
    "#normalize each feature independently\n",
    "scalers={}\n",
    "\n",
    "\n",
    "#train_scaled = np.zeros_like(data_reshaped[train_size])  # Initialize the normalized data array\n",
    "#val_scaled=np.zeros_like(data_reshaped[train_size:train_size + val_size])\n",
    "#test_scaled=np.zeros_like(data_reshaped[train_size + val_size:])\n",
    "\n",
    "train_scaled = np.zeros_like(train_data)\n",
    "val_scaled = np.zeros_like(val_data)\n",
    "test_scaled = np.zeros_like(test_data)\n",
    "\n",
    "\n",
    "for i in range(data_reshaped.shape[2]):\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1)) #changed to -1,1\n",
    "    # Flatten timesteps and samples for scaling\n",
    "    train_scaled[:, :, i] = scaler.fit_transform(train_data[:, :, i].reshape(-1, 1)).reshape(train_data.shape[0], train_data.shape[1])\n",
    "    val_scaled[:, :, i] = scaler.transform(val_data[:, :, i].reshape(-1, 1)).reshape(val_data.shape[0], val_data.shape[1])\n",
    "    test_scaled[:, :, i] = scaler.transform(test_data[:, :, i].reshape(-1, 1)).reshape(test_data.shape[0], test_data.shape[1])\n",
    "    scalers[i] = scaler  # Save scalers to inverse-transform later\n",
    "\n",
    "#Inverse Scale\n",
    "def inverse_scale(imputation, scalers):\n",
    "    n_features = imputation.shape[2]\n",
    "    imputation_denorm = np.empty_like(imputation)\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        imputation_denorm[:, :, i] = scalers[i].inverse_transform(imputation[:, :, i].reshape(-1, 1)).reshape(imputation.shape[0], imputation.shape[1])\n",
    "    \n",
    "    return imputation_denorm  \n",
    "\n",
    "\n",
    "#Optional: Artificially mask. Mask 20% of the data (MIT part)\n",
    "def mcar_f(X, mask_ratio=0.2):\n",
    "    \"\"\"Apply MCAR only to observed values.\"\"\"\n",
    "    observed_mask=~np.isnan(X) #find observed positions\n",
    "    artificial_mask=mcar(X,mask_ratio).astype(bool) #generate MCAR mask, cast to boolean\n",
    "    #combine masks \n",
    "    combined_mask=observed_mask & artificial_mask\n",
    "\n",
    "    #Apply masking\n",
    "    X_masked=X.copy()\n",
    "    X_masked[combined_mask]=np.nan\n",
    "    return X_masked,combined_mask\n",
    "\n",
    "\n",
    "#Use mcar on validation data \n",
    "val_X_masked, val_mask =mcar_f(val_scaled)\n",
    "val_X_ori=val_scaled.copy() \n",
    "\n",
    "test_X_masked, test_mask =mcar_f(test_scaled)\n",
    "test_X_ori=test_scaled.copy() \n",
    "\n",
    "#?? Problem: Can't have the best input for testing\n",
    "#1.Create synthetic test_data cuz if I drop nan values for test set, there's basically nothing left\n",
    "#synthetic_data=np.random.randn(n_samples,n_steps,n_features)\n",
    "#test_X_masked,test_mask=mcar_f(synthetic_data)\n",
    "#test_X_ori=synthetic_data.copy() #Ground truth\n",
    "\n",
    "# 2, Ensure no NaN values in synthetic data\n",
    "#test_X_masked = np.nan_to_num(test_X_masked, nan=np.nanmean(test_X_masked))\n",
    "#test_X_ori = np.nan_to_num(test_X_ori, nan=np.nanmean(test_X_ori))\n",
    "\n",
    "\n",
    "\n",
    "class Config:\n",
    "    no_cuda = False\n",
    "    no_mps = False\n",
    "    seed = 1\n",
    "\n",
    "args=Config()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "use_mps = not args.no_mps and torch.backends.mps.is_available()\n",
    "\n",
    "if args.cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA\")\n",
    "elif use_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "\n",
    "#MLflow set up\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "mlflow.set_experiment(\"SAITS_3\")\n",
    "SAITS_run_name = \"SAITS_1\"\n",
    "\n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name=SAITS_run_name) as run:\n",
    "# initialize the model (from example)\n",
    "    saits = SAITS(\n",
    "        n_steps=data_reshaped.shape[1],\n",
    "        n_features=data_reshaped.shape[2],\n",
    "        #n_layers=2, #deep network(4) for long sequences, here is revised for 2 since it always shows cuda error \n",
    "        n_layers=3,\n",
    "        d_model=512,\n",
    "        optimizer=Adam(lr=1e-3),\n",
    "        #lr=params[\"lr\"],\n",
    "        ORT_weight=1,\n",
    "        MIT_weight=1,\n",
    "        #d_model=512,  #d_modle must equal n_heads * d_k\n",
    "        d_ffn=512,\n",
    "        n_heads=8,\n",
    "        d_k=64,\n",
    "        d_v=64,\n",
    "        dropout=0.1,\n",
    "        attn_dropout=0.1,\n",
    "        diagonal_attention_mask=True,  # otherwise the original self-attention mechanism will be applied\n",
    "        #ORT_weight=1,  # you can adjust the weight values of arguments ORT_weight\n",
    "       # and MIT_weight to make the SAITS model focus more on one task. Usually you can just leave them to the default values, i.e. 1.\n",
    "        #MIT_weight=1,\n",
    "        batch_size=5, #try with 5 to see if it works, was 4. \n",
    "        # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "        epochs=10, #try with 10 to see if it runs\n",
    "        # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "        # You can leave it to defualt as None to disable early stopping.\n",
    "        patience=6, #initially was 3, tried to increase to see more possibilities \n",
    "        # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "        # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "        #optimizer=Adam(lr=1e-3),\n",
    "        # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "        # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "        # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "        num_workers=0,\n",
    "        # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "        # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "        device=device, \n",
    "        # set the path for saving tensorboard and trained model files \n",
    "        saving_path=\"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/best_model\",\n",
    "        # only save the best model after training finished.\n",
    "        # You can also set it as \"better\" to save models performing better ever during training.\n",
    "        model_saving_strategy=\"best\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
    "#use original missigness for trainig, use the mcar masked points as ground truth\n",
    "saits.fit(train_set={\"X\": train_scaled}, val_set={\"X\": val_X_masked , \"X_ori\":val_X_ori })\n",
    "\n",
    "\n",
    "##drop null values in test set \n",
    "\n",
    "# Check for NaN values across the entire array\n",
    "#nan_mask = np.isnan(test_X).any(axis=(1, 2))\n",
    "\n",
    "# Filter out samples (rows) that contain NaN values\n",
    "#test_X_clean = test_X[~nan_mask]\n",
    "#test_X_ori = test_X_clean\n",
    "#test_X_masked, test_mask = mcar_f(test_X_clean)\n",
    "\n",
    "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
    "# Convert the numpy array to a dictionary\n",
    "\n",
    "\n",
    "# Ensure saits_imputation and test_X are numpy arrays\n",
    "#if not isinstance(saits_imputation, np.ndarray):\n",
    "    #saits_imputation = np.array(saits_imputation)\n",
    "#if not isinstance(test_X, np.ndarray):\n",
    "     #test_X = np.array(test_X)\n",
    "\n",
    "#Inverse Scale\n",
    "def inverse_scale(imputation, scalers):\n",
    "    n_features = imputation.shape[2]\n",
    "    imputation_denorm = np.empty_like(imputation)\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        imputation_denorm[:, :, i] = scalers[i].inverse_transform(imputation[:, :, i].reshape(-1, 1)).reshape(imputation.shape[0], imputation.shape[1])\n",
    "    \n",
    "    return imputation_denorm  \n",
    "\n",
    "    \n",
    "#Apply function to the dataset \n",
    "test_set_dict = {\"X\": test_X_masked}\n",
    "test_imputation = saits.predict(test_set_dict)\n",
    "test_imputation_array = test_imputation[\"imputation\"]\n",
    "test_imputation_denorm = inverse_scale(test_imputation_array, scalers)\n",
    "test_ori_denorm=inverse_scale(test_X_ori, scalers)\n",
    "\n",
    "if args.cuda:\n",
    "    torch.cuda.empty_cache() \n",
    "\n",
    "\n",
    "#Calculate metrics per-feature: mean absolute error on the ground truth (artificially-missing values)\n",
    "mae_per_feature=[]\n",
    "percentage_mae_per_feature=[]\n",
    "\n",
    "for i in range(n_features):\n",
    "    #Extract imputation and ground truth for feature i\n",
    "    imputation_i=test_imputation_denorm[:,:,i]\n",
    "    ground_truth_i=test_ori_denorm[:,:,i]\n",
    "    mask_i=test_mask[:,:,i]\n",
    "    # Check for NaN values\n",
    "    if np.isnan(imputation_i).any() or np.isnan(ground_truth_i).any():\n",
    "        print(f\"NaN values detected in feature {i}\")\n",
    "        continue  # Skip this feature if NaN values are found\n",
    "    #Filter only artificially masked positions\n",
    "    mae_i=calc_mae(imputation_i,ground_truth_i,mask_i)\n",
    "    mae_per_feature.append(mae_i)\n",
    "    #Calculate the original standard deviation for the feature\n",
    "    std_dev_i = np.std(ground_truth_i[mask_i == 1])\n",
    "    # Calculate the percentage of MAE relative to the standard deviation   \n",
    "    if std_dev_i != 0:\n",
    "        percentage_mae_i = (mae_i / std_dev_i) * 100\n",
    "        percentage_mae_per_feature.append(percentage_mae_i)\n",
    "    else:\n",
    "        percentage_mae_i = float('inf')\n",
    "            \n",
    "    mlflow.log_metric(f\"MAE_{feature_names[i]}\", mae_i)\n",
    "    mlflow.log_metric(f\"Percentage_MAE_{feature_names[i]}\", percentage_mae_i)\n",
    "\n",
    "    print(f\"MAE for {feature_names[i]}: {mae_i:.4f}\")\n",
    "    mlflow.log_metric(f\"MAE_{feature_names[i]}\", mae_i)\n",
    "\n",
    "# calculate average MAE \n",
    "avg_mae=np.mean(mae_per_feature)\n",
    "print(f\"Testing mean absolute error: {avg_mae:.4f}\")\n",
    "mlflow.log_metric(\"avg_mae\", avg_mae)\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64068.60854917041"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[\"pr_air_scav_ecs\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64068.60854917041"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[\"pr_air_scav\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2104741155094039"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[\"fr_eng_ecs\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2128835870768723"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[\"fr_eng_setpoint\"].std()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
