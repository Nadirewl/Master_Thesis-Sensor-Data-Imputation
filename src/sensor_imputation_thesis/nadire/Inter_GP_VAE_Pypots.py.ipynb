{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connected to .venv (Python 3.10.16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143e4b3a-9991-406f-838c-4305380fd361",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\n",
      "████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗\n",
      "╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║\n",
      "   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║\n",
      "   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║\n",
      "   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║\n",
      "   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝\n",
      "ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai \u001b[0m\n",
      "\n",
      "Column time has 0 NaN values\n",
      "Column time has 0.0 Missing_rate\n",
      "Column fr_eng has 0 NaN values\n",
      "Column fr_eng has 0.0 Missing_rate\n",
      "Column te_exh_cyl_out__0 has 0 NaN values\n",
      "Column te_exh_cyl_out__0 has 0.0 Missing_rate\n",
      "Column pd_air_ic__0 has 0 NaN values\n",
      "Column pd_air_ic__0 has 0.0 Missing_rate\n",
      "Column pr_exh_turb_out__0 has 316581 NaN values\n",
      "Column pr_exh_turb_out__0 has 1.0 Missing_rate\n",
      "Column te_air_ic_out__0 has 0 NaN values\n",
      "Column te_air_ic_out__0 has 0.0 Missing_rate\n",
      "Column te_seawater has 0 NaN values\n",
      "Column te_seawater has 0.0 Missing_rate\n",
      "Column te_air_comp_in_a__0 has 316581 NaN values\n",
      "Column te_air_comp_in_a__0 has 1.0 Missing_rate\n",
      "Column te_air_comp_in_b__0 has 316581 NaN values\n",
      "Column te_air_comp_in_b__0 has 1.0 Missing_rate\n",
      "Column fr_tc__0 has 316581 NaN values\n",
      "Column fr_tc__0 has 1.0 Missing_rate\n",
      "Column pr_baro has 0 NaN values\n",
      "Column pr_baro has 0.0 Missing_rate\n",
      "Column pd_air_ic__0_1 has 0 NaN values\n",
      "Column pd_air_ic__0_1 has 0.0 Missing_rate\n",
      "Column pr_exh_rec has 0 NaN values\n",
      "Column pr_exh_rec has 0.0 Missing_rate\n",
      "Column te_exh_turb_in__0 has 316581 NaN values\n",
      "Column te_exh_turb_in__0 has 1.0 Missing_rate\n",
      "Column te_exh_turb_out__0 has 316581 NaN values\n",
      "Column te_exh_turb_out__0 has 1.0 Missing_rate\n",
      "Column bo_aux_blower_running has 0 NaN values\n",
      "Column bo_aux_blower_running has 0.0 Missing_rate\n",
      "Column re_eng_load has 426 NaN values\n",
      "Column re_eng_load has 0.0013456271854596453 Missing_rate\n",
      "Column pr_air_scav_ecs has 0 NaN values\n",
      "Column pr_air_scav_ecs has 0.0 Missing_rate\n",
      "Column pr_air_scav has 0 NaN values\n",
      "Column pr_air_scav has 0.0 Missing_rate\n",
      "Column te_air_scav_rec has 0 NaN values\n",
      "Column te_air_scav_rec has 0.0 Missing_rate\n",
      "Column te_air_ic_out__0_1 has 0 NaN values\n",
      "Column te_air_ic_out__0_1 has 0.0 Missing_rate\n",
      "Column pr_cyl_comp__0 has 426 NaN values\n",
      "Column pr_cyl_comp__0 has 0.0013456271854596453 Missing_rate\n",
      "Column pr_cyl_max__0 has 426 NaN values\n",
      "Column pr_cyl_max__0 has 0.0013456271854596453 Missing_rate\n",
      "Column se_mip__0 has 426 NaN values\n",
      "Column se_mip__0 has 0.0013456271854596453 Missing_rate\n",
      "Column te_exh_cyl_out__0_1 has 0 NaN values\n",
      "Column te_exh_cyl_out__0_1 has 0.0 Missing_rate\n",
      "Column fr_eng_setpoint has 0 NaN values\n",
      "Column fr_eng_setpoint has 0.0 Missing_rate\n",
      "Column te_air_scav_rec_iso has 126055 NaN values\n",
      "Column te_air_scav_rec_iso has 0.3981761381763277 Missing_rate\n",
      "Column pr_cyl_max_mv_iso has 128557 NaN values\n",
      "Column pr_cyl_max_mv_iso has 0.40607932882895686 Missing_rate\n",
      "Column pr_cyl_comp_mv_iso has 128557 NaN values\n",
      "Column pr_cyl_comp_mv_iso has 0.40607932882895686 Missing_rate\n",
      "Column fr_eng_ecs has 0 NaN values\n",
      "Column fr_eng_ecs has 0.0 Missing_rate\n",
      "Column pr_air_scav_iso has 128557 NaN values\n",
      "Column pr_air_scav_iso has 0.40607932882895686 Missing_rate\n",
      "Column engine_type_G80ME-C9.5-GI-LPSCR has 0 NaN values\n",
      "Column engine_type_G80ME-C9.5-GI-LPSCR has 0.0 Missing_rate\n",
      "Original size:316581, Sampled size: 105527\n",
      "Reshaped data:(105527, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Using CUDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/19 22:50:13 INFO mlflow.tracking.fluent: Experiment with name 'GP_VAE_2' does not exist. Creating a new experiment.\n",
      "2025/05/19 22:50:13 INFO mlflow.tracking.fluent: Experiment with name 'GP-VAE-2' does not exist. Creating a new experiment.\n",
      "[I 2025-05-19 22:50:13,818] A new study created in memory with name: no-name-80320a2b-4c2d-4a98-b708-5ed8770305df\n",
      "2025-05-19 22:50:13 [INFO]: Using the given device: cuda\n",
      "2025-05-19 22:50:13 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250519_T225013\n",
      "2025-05-19 22:50:13 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250519_T225013/tensorboard\n",
      "2025-05-19 22:50:13 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n",
      "2025-05-19 22:50:16 [INFO]: Epoch 001 - training loss (default): 11669.4683, validation loss: 22860.8210\n",
      "2025-05-19 22:50:18 [INFO]: Epoch 002 - training loss (default): 9140.2305, validation loss: 22556.5072\n",
      "2025-05-19 22:50:20 [INFO]: Epoch 003 - training loss (default): 9130.8550, validation loss: 22367.9447\n",
      "2025-05-19 22:50:23 [INFO]: Epoch 004 - training loss (default): 9128.7534, validation loss: 22226.3288\n",
      "2025-05-19 22:50:25 [INFO]: Epoch 005 - training loss (default): 9127.2673, validation loss: 22117.2249\n",
      "2025-05-19 22:50:27 [INFO]: Epoch 006 - training loss (default): 9126.8047, validation loss: 22025.6043\n",
      "2025-05-19 22:50:29 [INFO]: Epoch 007 - training loss (default): 9125.9924, validation loss: 21946.3762\n",
      "2025-05-19 22:50:31 [INFO]: Epoch 008 - training loss (default): 9125.5405, validation loss: 21877.3547\n",
      "2025-05-19 22:50:34 [INFO]: Epoch 009 - training loss (default): 9125.1428, validation loss: 21812.6977\n",
      "2025-05-19 22:50:36 [INFO]: Epoch 010 - training loss (default): 9124.8479, validation loss: 21749.0933\n",
      "2025-05-19 22:50:38 [INFO]: Epoch 011 - training loss (default): 9124.9772, validation loss: 21694.1316\n",
      "2025-05-19 22:50:40 [INFO]: Epoch 012 - training loss (default): 9124.4896, validation loss: 21637.8719\n",
      "2025-05-19 22:50:42 [INFO]: Epoch 013 - training loss (default): 9124.7257, validation loss: 21591.6631\n",
      "2025-05-19 22:50:44 [INFO]: Epoch 014 - training loss (default): 9124.3289, validation loss: 21546.4849\n",
      "2025-05-19 22:50:47 [INFO]: Epoch 015 - training loss (default): 9124.2393, validation loss: 21497.4286\n",
      "2025-05-19 22:50:49 [INFO]: Epoch 016 - training loss (default): 9124.4223, validation loss: 21457.4530\n",
      "2025-05-19 22:50:49 [INFO]: Finished training. The best model is from epoch#16.\n",
      "2025-05-19 22:50:49 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250519_T225013/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-19 22:50:49,927] Trial 0 finished with values: [316478.3729327418, 406613.69210850203] and parameters: {'lr': 0.0001704281313940993, 'epochs': 16, 'batch_size': 14}.\n",
      "2025-05-19 22:50:49 [INFO]: Using the given device: cuda\n",
      "2025-05-19 22:50:49 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250519_T225049\n",
      "2025-05-19 22:50:49 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250519_T225049/tensorboard\n",
      "2025-05-19 22:50:49 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/b25851d697854fdf8c6719ae17207083\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 22:50:52 [INFO]: Epoch 001 - training loss (default): 9978.8988, validation loss: 21890.1545\n",
      "2025-05-19 22:50:54 [INFO]: Epoch 002 - training loss (default): 9128.3903, validation loss: 21643.0596\n",
      "2025-05-19 22:50:57 [INFO]: Epoch 003 - training loss (default): 9126.3512, validation loss: 21473.5833\n",
      "2025-05-19 22:50:59 [INFO]: Epoch 004 - training loss (default): 9125.5432, validation loss: 21339.3894\n",
      "2025-05-19 22:51:02 [INFO]: Epoch 005 - training loss (default): 9125.0234, validation loss: 21238.3811\n",
      "2025-05-19 22:51:04 [INFO]: Epoch 006 - training loss (default): 9124.7301, validation loss: 21150.8241\n",
      "2025-05-19 22:51:07 [INFO]: Epoch 007 - training loss (default): 9124.5835, validation loss: 21077.1386\n",
      "2025-05-19 22:51:10 [INFO]: Epoch 008 - training loss (default): 9124.3002, validation loss: 21011.0777\n",
      "2025-05-19 22:51:12 [INFO]: Epoch 009 - training loss (default): 9124.1059, validation loss: 20958.1911\n",
      "2025-05-19 22:51:15 [INFO]: Epoch 010 - training loss (default): 9123.9638, validation loss: 20907.7041\n",
      "2025-05-19 22:51:18 [INFO]: Epoch 011 - training loss (default): 9123.6545, validation loss: 20848.5832\n",
      "2025-05-19 22:51:20 [INFO]: Epoch 012 - training loss (default): 9123.6533, validation loss: 20798.1087\n",
      "2025-05-19 22:51:23 [INFO]: Epoch 013 - training loss (default): 9123.5474, validation loss: 20747.6256\n",
      "2025-05-19 22:51:25 [INFO]: Epoch 014 - training loss (default): 9123.4227, validation loss: 20691.7293\n",
      "2025-05-19 22:51:28 [INFO]: Epoch 015 - training loss (default): 9123.3790, validation loss: 20640.4633\n",
      "2025-05-19 22:51:30 [INFO]: Epoch 016 - training loss (default): 9123.3341, validation loss: 20595.0121\n",
      "2025-05-19 22:51:33 [INFO]: Epoch 017 - training loss (default): 9123.2918, validation loss: 20550.4433\n",
      "2025-05-19 22:51:36 [INFO]: Epoch 018 - training loss (default): 9123.3021, validation loss: 20509.5957\n",
      "2025-05-19 22:51:38 [INFO]: Epoch 019 - training loss (default): 9123.3602, validation loss: 20474.3506\n",
      "2025-05-19 22:51:38 [INFO]: Finished training. The best model is from epoch#19.\n",
      "2025-05-19 22:51:38 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250519_T225049/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-19 22:51:39,196] Trial 1 finished with values: [321771.8614608287, 461946.163265407] and parameters: {'lr': 0.00046604823908184394, 'epochs': 19, 'batch_size': 12}.\n",
      "2025-05-19 22:51:39 [INFO]: Using the given device: cuda\n",
      "2025-05-19 22:51:39 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250519_T225139\n",
      "2025-05-19 22:51:39 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250519_T225139/tensorboard\n",
      "2025-05-19 22:51:39 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/cadd370fecc7461f956ed204e7e4b7fb\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 22:51:41 [INFO]: Epoch 001 - training loss (default): 10540.1574, validation loss: 22315.8322\n",
      "2025-05-19 22:51:43 [INFO]: Epoch 002 - training loss (default): 9130.4513, validation loss: 22008.8104\n",
      "2025-05-19 22:51:46 [INFO]: Epoch 003 - training loss (default): 9127.7234, validation loss: 21812.1921\n",
      "2025-05-19 22:51:48 [INFO]: Epoch 004 - training loss (default): 9126.4930, validation loss: 21664.0030\n",
      "2025-05-19 22:51:50 [INFO]: Epoch 005 - training loss (default): 9125.8423, validation loss: 21545.2151\n",
      "2025-05-19 22:51:53 [INFO]: Epoch 006 - training loss (default): 9125.2495, validation loss: 21442.2666\n",
      "2025-05-19 22:51:55 [INFO]: Epoch 007 - training loss (default): 9124.9736, validation loss: 21353.0749\n",
      "2025-05-19 22:51:57 [INFO]: Epoch 008 - training loss (default): 9124.6476, validation loss: 21274.8237\n",
      "2025-05-19 22:52:00 [INFO]: Epoch 009 - training loss (default): 9124.5043, validation loss: 21205.5013\n",
      "2025-05-19 22:52:02 [INFO]: Epoch 010 - training loss (default): 9124.3948, validation loss: 21144.8725\n",
      "2025-05-19 22:52:04 [INFO]: Epoch 011 - training loss (default): 9124.3214, validation loss: 21091.9575\n",
      "2025-05-19 22:52:04 [INFO]: Finished training. The best model is from epoch#11.\n",
      "2025-05-19 22:52:04 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250519_T225139/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-19 22:52:05,572] Trial 2 finished with values: [314569.3917306948, 409205.8877926546] and parameters: {'lr': 0.0003162564228356125, 'epochs': 11, 'batch_size': 13}.\n",
      "2025-05-19 22:52:05 [INFO]: Using the given device: cuda\n",
      "2025-05-19 22:52:05 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250519_T225205\n",
      "2025-05-19 22:52:05 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250519_T225205/tensorboard\n",
      "2025-05-19 22:52:05 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/9f6d1369581641ef88f8c0d1179fa32f\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 22:52:09 [INFO]: Epoch 001 - training loss (default): 10407.9615, validation loss: 22344.2183\n",
      "2025-05-19 22:52:12 [INFO]: Epoch 002 - training loss (default): 9130.7042, validation loss: 22093.7795\n",
      "2025-05-19 22:52:16 [INFO]: Epoch 003 - training loss (default): 9127.8214, validation loss: 21943.8718\n",
      "2025-05-19 22:52:19 [INFO]: Epoch 004 - training loss (default): 9126.6722, validation loss: 21822.1376\n",
      "2025-05-19 22:52:22 [INFO]: Epoch 005 - training loss (default): 9125.9316, validation loss: 21719.6457\n",
      "2025-05-19 22:52:26 [INFO]: Epoch 006 - training loss (default): 9125.3987, validation loss: 21630.8705\n",
      "2025-05-19 22:52:29 [INFO]: Epoch 007 - training loss (default): 9124.9800, validation loss: 21553.6136\n",
      "2025-05-19 22:52:33 [INFO]: Epoch 008 - training loss (default): 9124.7287, validation loss: 21485.1410\n",
      "2025-05-19 22:52:36 [INFO]: Epoch 009 - training loss (default): 9124.4651, validation loss: 21426.8873\n",
      "2025-05-19 22:52:40 [INFO]: Epoch 010 - training loss (default): 9124.3287, validation loss: 21378.8900\n",
      "2025-05-19 22:52:43 [INFO]: Epoch 011 - training loss (default): 9124.0289, validation loss: 21326.0399\n",
      "2025-05-19 22:52:47 [INFO]: Epoch 012 - training loss (default): 9123.9030, validation loss: 21284.1073\n",
      "2025-05-19 22:52:51 [INFO]: Epoch 013 - training loss (default): 9123.7151, validation loss: 21238.8275\n",
      "2025-05-19 22:52:54 [INFO]: Epoch 014 - training loss (default): 9123.5886, validation loss: 21198.6850\n",
      "2025-05-19 22:52:54 [INFO]: Finished training. The best model is from epoch#14.\n",
      "2025-05-19 22:52:54 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250519_T225205/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-19 22:52:55,643] Trial 3 finished with values: [312021.7489440709, 436930.24347313656] and parameters: {'lr': 0.00020943290721424612, 'epochs': 14, 'batch_size': 8}.\n",
      "2025-05-19 22:52:55 [INFO]: Using the given device: cuda\n",
      "2025-05-19 22:52:55 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250519_T225255\n",
      "2025-05-19 22:52:55 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250519_T225255/tensorboard\n",
      "2025-05-19 22:52:55 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/efc8c5993ebe49b79510e3677132fa09\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 22:52:58 [INFO]: Epoch 001 - training loss (default): 9846.5052, validation loss: 21664.2411\n",
      "2025-05-19 22:53:01 [INFO]: Epoch 002 - training loss (default): 9127.2023, validation loss: 21355.1006\n",
      "2025-05-19 22:53:04 [INFO]: Epoch 003 - training loss (default): 9125.5760, validation loss: 21164.5458\n",
      "2025-05-19 22:53:07 [INFO]: Epoch 004 - training loss (default): 9125.0163, validation loss: 21026.7654\n",
      "2025-05-19 22:53:10 [INFO]: Epoch 005 - training loss (default): 9124.5555, validation loss: 20914.7993\n",
      "2025-05-19 22:53:13 [INFO]: Epoch 006 - training loss (default): 9124.2636, validation loss: 20823.2091\n",
      "2025-05-19 22:53:16 [INFO]: Epoch 007 - training loss (default): 9124.1494, validation loss: 20753.4097\n",
      "2025-05-19 22:53:19 [INFO]: Epoch 008 - training loss (default): 9123.7953, validation loss: 20683.0910\n",
      "2025-05-19 22:53:22 [INFO]: Epoch 009 - training loss (default): 9123.6037, validation loss: 20616.2844\n",
      "2025-05-19 22:53:25 [INFO]: Epoch 010 - training loss (default): 9123.5920, validation loss: 20568.3574\n",
      "2025-05-19 22:53:28 [INFO]: Epoch 011 - training loss (default): 9123.4489, validation loss: 20503.5983\n",
      "2025-05-19 22:53:31 [INFO]: Epoch 012 - training loss (default): 9123.3641, validation loss: 20467.2113\n",
      "2025-05-19 22:53:33 [INFO]: Epoch 013 - training loss (default): 9123.3622, validation loss: 20423.9067\n",
      "2025-05-19 22:53:33 [INFO]: Finished training. The best model is from epoch#13.\n",
      "2025-05-19 22:53:33 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250519_T225255/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-19 22:53:34,532] Trial 4 finished with values: [333494.30425831483, 477358.01158874435] and parameters: {'lr': 0.0005086467435686872, 'epochs': 13, 'batch_size': 10}.\n",
      "2025-05-19 22:53:34 [INFO]: Using the given device: cuda\n",
      "2025-05-19 22:53:34 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250519_T225334\n",
      "2025-05-19 22:53:34 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250519_T225334/tensorboard\n",
      "2025-05-19 22:53:34 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/b947c52674404f7cb6c53f363e7db200\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 22:53:39 [INFO]: Epoch 001 - training loss (default): 9353.9612, validation loss: 20731.1173\n",
      "2025-05-19 22:53:45 [INFO]: Epoch 002 - training loss (default): 9125.3628, validation loss: 20563.5664\n",
      "2025-05-19 22:53:50 [INFO]: Epoch 003 - training loss (default): 9124.8708, validation loss: 20508.9881\n",
      "2025-05-19 22:53:55 [INFO]: Epoch 004 - training loss (default): 9124.2663, validation loss: 20457.7714\n",
      "2025-05-19 22:54:01 [INFO]: Epoch 005 - training loss (default): 9123.9203, validation loss: 20315.8302\n",
      "2025-05-19 22:54:06 [INFO]: Epoch 006 - training loss (default): 9123.6946, validation loss: 20307.4837\n",
      "2025-05-19 22:54:11 [INFO]: Epoch 007 - training loss (default): 9123.6612, validation loss: 20105.3309\n",
      "2025-05-19 22:54:17 [INFO]: Epoch 008 - training loss (default): 9123.5234, validation loss: 20040.2857\n",
      "2025-05-19 22:54:22 [INFO]: Epoch 009 - training loss (default): 9123.3788, validation loss: 19852.8430\n",
      "2025-05-19 22:54:27 [INFO]: Epoch 010 - training loss (default): 9123.4088, validation loss: 19493.9281\n",
      "2025-05-19 22:54:34 [INFO]: Epoch 011 - training loss (default): 9123.2644, validation loss: 19191.1909\n",
      "2025-05-19 22:54:39 [INFO]: Epoch 012 - training loss (default): 9123.1847, validation loss: 18888.9693\n",
      "2025-05-19 22:54:45 [INFO]: Epoch 013 - training loss (default): 9123.0965, validation loss: 18319.9585\n",
      "2025-05-19 22:54:50 [INFO]: Epoch 014 - training loss (default): 9123.0697, validation loss: 17830.0839\n",
      "2025-05-19 22:54:56 [INFO]: Epoch 015 - training loss (default): 9123.0912, validation loss: 17461.2320\n",
      "2025-05-19 22:55:01 [INFO]: Epoch 016 - training loss (default): 9122.9715, validation loss: 17034.5732\n",
      "2025-05-19 22:55:06 [INFO]: Epoch 017 - training loss (default): 9122.9106, validation loss: 16405.1793\n",
      "2025-05-19 22:55:12 [INFO]: Epoch 018 - training loss (default): 9122.9320, validation loss: 16104.9881\n",
      "2025-05-19 22:55:17 [INFO]: Epoch 019 - training loss (default): 9122.8352, validation loss: 15819.4017\n",
      "2025-05-19 22:55:17 [INFO]: Finished training. The best model is from epoch#19.\n",
      "2025-05-19 22:55:17 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250519_T225334/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-19 22:55:18,273] Trial 5 finished with values: [317963.1586264935, 455580.2292209908] and parameters: {'lr': 0.0008752538034088159, 'epochs': 19, 'batch_size': 5}.\n",
      "2025-05-19 22:55:18 [INFO]: Using the given device: cuda\n",
      "2025-05-19 22:55:18 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250519_T225518\n",
      "2025-05-19 22:55:18 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250519_T225518/tensorboard\n",
      "2025-05-19 22:55:18 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/bafea9273bd94abeb22ef19756d290ed\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 22:55:22 [INFO]: Epoch 001 - training loss (default): 9609.6060, validation loss: 21450.8878\n",
      "2025-05-19 22:55:26 [INFO]: Epoch 002 - training loss (default): 9125.9232, validation loss: 21185.1344\n",
      "2025-05-19 22:55:30 [INFO]: Epoch 003 - training loss (default): 9124.9774, validation loss: 21028.2179\n",
      "2025-05-19 22:55:34 [INFO]: Epoch 004 - training loss (default): 9124.7810, validation loss: 20908.9630\n",
      "2025-05-19 22:55:38 [INFO]: Epoch 005 - training loss (default): 9124.4892, validation loss: 20820.0712\n",
      "2025-05-19 22:55:42 [INFO]: Epoch 006 - training loss (default): 9124.3911, validation loss: 20762.5896\n",
      "2025-05-19 22:55:46 [INFO]: Epoch 007 - training loss (default): 9123.9307, validation loss: 20687.1977\n",
      "2025-05-19 22:55:51 [INFO]: Epoch 008 - training loss (default): 9123.5644, validation loss: 20612.4713\n",
      "2025-05-19 22:55:56 [INFO]: Epoch 009 - training loss (default): 9123.4825, validation loss: 20533.0609\n",
      "2025-05-19 22:56:00 [INFO]: Epoch 010 - training loss (default): 9123.5641, validation loss: 20484.0841\n",
      "2025-05-19 22:56:00 [INFO]: Finished training. The best model is from epoch#10.\n",
      "2025-05-19 22:56:00 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250519_T225518/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-19 22:56:00,967] Trial 6 finished with values: [332063.22014325875, 475107.3750530148] and parameters: {'lr': 0.0005305717913643639, 'epochs': 10, 'batch_size': 7}.\n",
      "2025-05-19 22:56:00 [INFO]: Using the given device: cuda\n",
      "2025-05-19 22:56:00 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250519_T225600\n",
      "2025-05-19 22:56:00 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250519_T225600/tensorboard\n",
      "2025-05-19 22:56:00 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/fa069e2a0d164595b5ccf431ca8f3da7\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 22:56:04 [INFO]: Epoch 001 - training loss (default): 9777.7843, validation loss: 21675.3546\n",
      "2025-05-19 22:56:07 [INFO]: Epoch 002 - training loss (default): 9127.0944, validation loss: 21297.4761\n",
      "2025-05-19 22:56:11 [INFO]: Epoch 003 - training loss (default): 9125.5157, validation loss: 21071.1036\n",
      "2025-05-19 22:56:15 [INFO]: Epoch 004 - training loss (default): 9124.9748, validation loss: 20922.4289\n",
      "2025-05-19 22:56:18 [INFO]: Epoch 005 - training loss (default): 9124.5767, validation loss: 20798.0840\n",
      "2025-05-19 22:56:22 [INFO]: Epoch 006 - training loss (default): 9124.3178, validation loss: 20697.2786\n",
      "2025-05-19 22:56:26 [INFO]: Epoch 007 - training loss (default): 9124.0242, validation loss: 20608.8538\n",
      "2025-05-19 22:56:29 [INFO]: Epoch 008 - training loss (default): 9123.8435, validation loss: 20527.6828\n",
      "2025-05-19 22:56:32 [INFO]: Epoch 009 - training loss (default): 9123.6265, validation loss: 20447.3202\n",
      "2025-05-19 22:56:36 [INFO]: Epoch 010 - training loss (default): 9123.5977, validation loss: 20390.7741\n",
      "2025-05-19 22:56:40 [INFO]: Epoch 011 - training loss (default): 9123.4021, validation loss: 20313.6643\n",
      "2025-05-19 22:56:40 [INFO]: Finished training. The best model is from epoch#11.\n",
      "2025-05-19 22:56:40 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250519_T225600/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-19 22:56:40,770] Trial 7 finished with values: [316703.6145334785, 451838.01813143794] and parameters: {'lr': 0.00040039036187399375, 'epochs': 11, 'batch_size': 8}.\n",
      "2025-05-19 22:56:40 [INFO]: Using the given device: cuda\n",
      "2025-05-19 22:56:40 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250519_T225640\n",
      "2025-05-19 22:56:40 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250519_T225640/tensorboard\n",
      "2025-05-19 22:56:40 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/0898f76fc65d4520855707a68e590156\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 22:56:43 [INFO]: Epoch 001 - training loss (default): 10068.6461, validation loss: 21907.9115\n",
      "2025-05-19 22:56:46 [INFO]: Epoch 002 - training loss (default): 9127.5650, validation loss: 21673.9956\n",
      "2025-05-19 22:56:49 [INFO]: Epoch 003 - training loss (default): 9125.7771, validation loss: 21522.0588\n",
      "2025-05-19 22:56:52 [INFO]: Epoch 004 - training loss (default): 9125.0538, validation loss: 21410.1648\n",
      "2025-05-19 22:56:55 [INFO]: Epoch 005 - training loss (default): 9124.7070, validation loss: 21320.1477\n",
      "2025-05-19 22:56:58 [INFO]: Epoch 006 - training loss (default): 9124.5084, validation loss: 21242.3446\n",
      "2025-05-19 22:57:01 [INFO]: Epoch 007 - training loss (default): 9124.2445, validation loss: 21166.5282\n",
      "2025-05-19 22:57:04 [INFO]: Epoch 008 - training loss (default): 9124.2001, validation loss: 21119.5911\n",
      "2025-05-19 22:57:07 [INFO]: Epoch 009 - training loss (default): 9123.9592, validation loss: 21055.5204\n",
      "2025-05-19 22:57:10 [INFO]: Epoch 010 - training loss (default): 9123.8091, validation loss: 20985.6119\n",
      "2025-05-19 22:57:13 [INFO]: Epoch 011 - training loss (default): 9123.7371, validation loss: 20943.3335\n",
      "2025-05-19 22:57:13 [INFO]: Finished training. The best model is from epoch#11.\n",
      "2025-05-19 22:57:13 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250519_T225640/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-19 22:57:14,538] Trial 8 finished with values: [317639.97061383846, 452181.89225430466] and parameters: {'lr': 0.00046353380456412536, 'epochs': 11, 'batch_size': 11}.\n",
      "2025-05-19 22:57:14 [INFO]: Using the given device: cuda\n",
      "2025-05-19 22:57:14 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250519_T225714\n",
      "2025-05-19 22:57:14 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250519_T225714/tensorboard\n",
      "2025-05-19 22:57:14 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/8cd65d77cf65497a8a31eea3ad1faa7c\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 22:57:17 [INFO]: Epoch 001 - training loss (default): 10024.7653, validation loss: 21260.5430\n",
      "2025-05-19 22:57:21 [INFO]: Epoch 002 - training loss (default): 9128.0963, validation loss: 20834.3593\n",
      "2025-05-19 22:57:24 [INFO]: Epoch 003 - training loss (default): 9126.5857, validation loss: 20615.0654\n",
      "2025-05-19 22:57:27 [INFO]: Epoch 004 - training loss (default): 9125.7249, validation loss: 20464.5100\n",
      "2025-05-19 22:57:30 [INFO]: Epoch 005 - training loss (default): 9125.1479, validation loss: 20347.4020\n",
      "2025-05-19 22:57:33 [INFO]: Epoch 006 - training loss (default): 9124.7165, validation loss: 20242.6674\n",
      "2025-05-19 22:57:36 [INFO]: Epoch 007 - training loss (default): 9124.4796, validation loss: 20153.9985\n",
      "2025-05-19 22:57:41 [INFO]: Epoch 008 - training loss (default): 9124.2670, validation loss: 20077.9207\n",
      "2025-05-19 22:57:45 [INFO]: Epoch 009 - training loss (default): 9124.0293, validation loss: 19998.7656\n",
      "2025-05-19 22:57:49 [INFO]: Epoch 010 - training loss (default): 9123.8539, validation loss: 19929.7059\n",
      "2025-05-19 22:57:52 [INFO]: Epoch 011 - training loss (default): 9123.6849, validation loss: 19865.3034\n",
      "2025-05-19 22:57:56 [INFO]: Epoch 012 - training loss (default): 9123.5440, validation loss: 19804.7552\n",
      "2025-05-19 22:57:59 [INFO]: Epoch 013 - training loss (default): 9123.3497, validation loss: 19735.0169\n",
      "2025-05-19 22:58:02 [INFO]: Epoch 014 - training loss (default): 9123.3755, validation loss: 19691.5169\n",
      "2025-05-19 22:58:02 [INFO]: Finished training. The best model is from epoch#14.\n",
      "2025-05-19 22:58:02 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250519_T225714/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-19 22:58:03,570] Trial 9 finished with values: [331623.7726320681, 474579.2345077271] and parameters: {'lr': 0.0003217054036792562, 'epochs': 14, 'batch_size': 9}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/7060a3a2bca74d1cac7da85e22beed7b\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n",
      "🏃 View run GPVAE_Optuna_Study at: http://localhost:5000/#/experiments/832352739106302318/runs/2b73088bf5864b02ba1e8eef1c6d61e3\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "A single best trial cannot be retrieved from a multi-objective study. Consider using Study.best_trials to retrieve a list containing the best trials.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 333\u001b[0m\n\u001b[1;32m    330\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(directions\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    331\u001b[0m study\u001b[39m.\u001b[39moptimize(objective, n_trials\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[0;32m--> 333\u001b[0m best_params \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39;49mbest_trial\u001b[39m.\u001b[39mparams\n\u001b[1;32m    334\u001b[0m best_value \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_trial\u001b[39m.\u001b[39mvalue \u001b[39m# or best_trial.values if multi-objective\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \u001b[39m# Log best parameters\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/optuna/study/study.py:157\u001b[0m, in \u001b[0;36mStudy.best_trial\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return the best trial in the study.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \n\u001b[1;32m    142\u001b[0m \u001b[39m.. note::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m \n\u001b[1;32m    154\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_multi_objective():\n\u001b[0;32m--> 157\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    158\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mA single best trial cannot be retrieved from a multi-objective study. Consider \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39musing Study.best_trials to retrieve a list containing the best trials.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m     )\n\u001b[1;32m    162\u001b[0m best_trial \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_storage\u001b[39m.\u001b[39mget_best_trial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_study_id)\n\u001b[1;32m    164\u001b[0m \u001b[39m# If the trial with the best value is infeasible, select the best trial from all feasible\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[39m# trials. Note that the behavior is undefined when constrained optimization without the\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[39m# violation value in the best-valued trial.\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: A single best trial cannot be retrieved from a multi-objective study. Consider using Study.best_trials to retrieve a list containing the best trials."
     ]
    }
   ],
   "source": [
    "#Import Pypots Library\n",
    "from pypots.optim import Adam\n",
    "from pypots.imputation import GPVAE\n",
    "#from pypots.utils.metrics import calc_mae\n",
    "from pypots.nn.functional import calc_mae\n",
    "\n",
    "\n",
    "import argparse\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import data_insight\n",
    "from data_insight import setup_duckdb\n",
    "from duckdb import DuckDBPyConnection as DuckDB\n",
    "from duckdb import DuckDBPyRelation as Relation\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "from duckdb import DuckDBPyConnection as DuckDB\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna \n",
    "from optuna.visualization import plot_optimization_history\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, Dataset\n",
    "from pygrinder.missing_completely_at_random import mcar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import sensor_imputation_thesis.shared.load_data as load\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "#PatchTST might be an ideal choise if SAITS is too slow \n",
    "\n",
    "##Drop columns with different indexes while loading data.. Or the mean values \n",
    "\n",
    "df=pd.read_parquet(\"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/ny_df_for_pypots.parquet\")\n",
    "\n",
    "len(df)\n",
    "\n",
    "#current length of the dataframe is 119439\n",
    "\n",
    "# Check nan values in each column\n",
    "for col in df.columns:\n",
    "    print(f\"Column {col} has {df[col].isna().sum()} NaN values\")\n",
    "    missing_rate=df[col].isna().sum()/len(df[col])\n",
    "    print(f\"Column {col} has {missing_rate} Missing_rate\")\n",
    "\n",
    "\n",
    "#Try with smaller dataset, size 4000\n",
    "##SAMPLE the percengtage of the dataset, df.sample (averagely pick samples)\n",
    "#not df.sample cuz it will randomly select \n",
    "original_size=len(df)\n",
    "desired_fraction=0.3 #Select data every 3 minutes \n",
    "step=int(1/desired_fraction) #step_size=10 (sample every 10th (3/10) minute)\n",
    "\n",
    "#Systematic sampling: Start at a random offset to avoid bias \n",
    "start=np.random.randint(0,step) #Random start between 0-9\n",
    "df1=df.iloc[start::step].reset_index(drop=True)\n",
    "\n",
    "print(f\"Original size:{len(df)}, Sampled size: {len(df1)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Custom Dataset class\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Data processing code\n",
    "sensor_cols = [col for col in df1.columns if col != \"time\"]\n",
    "data = df1[sensor_cols].values\n",
    "\n",
    "#¤get feature names for printing mae later \n",
    "feature_names=df1[sensor_cols].columns.tolist()\n",
    "\n",
    "## Convert data to 3D arrays of shape n_samples, n_timesteps, n_features, X_ori refers to the original data without missing values \n",
    "## Reconstruct all columns simultaneously  #num_features: 119\n",
    "n_features = data.shape[1]  # exclude the time column\n",
    "n_steps = 20 #60 (was 60 previously) #(TRY TO CHANGE HERE)  # # window length, 1440 steps = 24 hours of 1-minute data, but here is revised to 60 again\n",
    "#total_elements = data.shape[0] * data.shape[1]\n",
    "n_samples = data.shape[0] // n_steps \n",
    "\n",
    "\n",
    "\n",
    "# Reshape to (n_samples // n_steps, n_steps, n_features)\n",
    "#data_reshaped = data.reshape((n_samples, n_steps, n_features))\n",
    "data_reshaped=data[:n_samples*n_steps].reshape(n_samples,n_steps,n_features)\n",
    "print(f\"Reshaped data:{data.shape}\")\n",
    "\n",
    "#Split into train, test, val, fit scaler only on the train set (prevent data leakage)\n",
    "\n",
    "#train_size = int(0.6 * len(data))\n",
    "#val_size = int(0.2 * len(data))\n",
    "#test_size = len(data) - train_size - val_size\n",
    "\n",
    "#train_data = data_reshaped[:train_size]\n",
    "#val_data = data_reshaped[train_size:train_size + val_size]\n",
    "#test_data= data_reshaped[train_size + val_size:]\n",
    "\n",
    "\n",
    "#Apply time series split \n",
    "#Split into train(60%), val(20%), and test (20%)\n",
    "train_data, temp_data=train_test_split(data_reshaped,test_size=0.4,shuffle=True)\n",
    "val_data, test_data=train_test_split(temp_data, test_size=0.5, shuffle=False)\n",
    "\n",
    "##Normalization is important because of the nature of mse calculation of saits, columns with large \n",
    "#values dominate the loss, making metrics meaningless. SAITS computes MSE/MAE column-wise and averages \n",
    "#them across all columns \n",
    "#  Apply minmax scaler here \n",
    "#normalize each feature independently\n",
    "scalers={}\n",
    "\n",
    "\n",
    "#train_scaled = np.zeros_like(data_reshaped[train_size])  # Initialize the normalized data array\n",
    "#val_scaled=np.zeros_like(data_reshaped[train_size:train_size + val_size])\n",
    "#test_scaled=np.zeros_like(data_reshaped[train_size + val_size:])\n",
    "\n",
    "train_scaled = np.zeros_like(train_data)\n",
    "val_scaled = np.zeros_like(val_data)\n",
    "test_scaled = np.zeros_like(test_data)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(data_reshaped.shape[2]):\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1)) #changed to -1,1\n",
    "    # Flatten timesteps and samples for scaling\n",
    "    train_scaled[:, :, i] = scaler.fit_transform(train_data[:, :, i].reshape(-1, 1)).reshape(train_data.shape[0], train_data.shape[1])\n",
    "    val_scaled[:, :, i] = scaler.transform(val_data[:, :, i].reshape(-1, 1)).reshape(val_data.shape[0], val_data.shape[1])\n",
    "    test_scaled[:, :, i] = scaler.transform(test_data[:, :, i].reshape(-1, 1)).reshape(test_data.shape[0], test_data.shape[1])\n",
    "    scalers[i] = scaler  # Save scalers to inverse-transform later\n",
    "\n",
    "#Inverse Scale\n",
    "def inverse_scale(imputation, scalers):\n",
    "    n_samples, n_timesteps, n_features = imputation.shape\n",
    "    imputation_denorm = np.empty_like(imputation)\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        reshaped = imputation[:, :, i].reshape(-1, 1)\n",
    "        inversed = scalers[i].inverse_transform(reshaped)\n",
    "        imputation_denorm[:, :, i] = inversed.reshape(n_samples, n_timesteps)\n",
    "    \n",
    "    return imputation_denorm\n",
    "\n",
    "\n",
    "\n",
    "#Optional: Artificially mask. Mask 20% of the data (MIT part). Try masking 30% here \n",
    "def mcar_f(X, mask_ratio=0.3):\n",
    "    \"\"\"Apply MCAR only to observed values.\"\"\"\n",
    "    observed_mask=~np.isnan(X) #find observed positions\n",
    "    artificial_mask=mcar(X,mask_ratio).astype(bool) #generate MCAR mask, cast to boolean\n",
    "    #combine masks \n",
    "    combined_mask=observed_mask & artificial_mask\n",
    "\n",
    "    #Apply masking\n",
    "    X_masked=X.copy()\n",
    "    X_masked[combined_mask]=np.nan\n",
    "    return X_masked,combined_mask\n",
    "\n",
    "\n",
    "#Use mcar on validation data \n",
    "val_X_masked, val_mask =mcar_f(val_scaled)\n",
    "val_X_ori=val_scaled.copy() \n",
    "\n",
    "test_X_masked, test_mask =mcar_f(test_scaled)\n",
    "test_X_ori=test_scaled.copy() \n",
    "\n",
    "\n",
    "#?? Problem: Can't have the best input for testing\n",
    "#1.Create synthetic test_data cuz if I drop nan values for test set, there's basically nothing left\n",
    "#synthetic_data=np.random.randn(n_samples,n_steps,n_features)\n",
    "#test_X_masked,test_mask=mcar_f(synthetic_data)\n",
    "#test_X_ori=synthetic_data.copy() #Ground truth\n",
    "\n",
    "# 2, Ensure no NaN values in synthetic data\n",
    "#test_X_masked = np.nan_to_num(test_X_masked, nan=np.nanmean(test_X_masked))\n",
    "#test_X_ori = np.nan_to_num(test_X_ori, nan=np.nanmean(test_X_ori))\n",
    "\n",
    "\n",
    "\n",
    "class Config:\n",
    "    no_cuda = False\n",
    "    no_mps = False\n",
    "    seed = 1\n",
    "\n",
    "args=Config()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "use_mps = not args.no_mps and torch.backends.mps.is_available()\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "\n",
    "if args.cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA\")\n",
    "elif use_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "train_scaled = torch.tensor(train_scaled, dtype=torch.float32)\n",
    "val_X_masked = torch.tensor(val_X_masked, dtype=torch.float32)\n",
    "val_X_ori = torch.tensor(val_X_ori, dtype=torch.float32)\n",
    "\n",
    "train_scaled = train_scaled.to(device)\n",
    "val_X_masked = val_X_masked.to(device)\n",
    "val_X_ori = val_X_ori.to(device)\n",
    "\n",
    "\n",
    "#MLflow set up\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "mlflow.set_experiment(\"GP_VAE_2\")\n",
    "\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"lr\": trial.suggest_float(\"lr\", 1e-4, 1e-3, log=True),\n",
    "        \"epochs\": trial.suggest_int(\"epochs\", 10, 20),\n",
    "        \"batch_size\": trial.suggest_int(\"batch_size\", 4, 16)\n",
    "    }\n",
    "\n",
    "    with mlflow.start_run(run_name=\"GP-VAE-Trial\", nested=True):\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        gp_vae = GPVAE(\n",
    "            n_steps=data_reshaped.shape[1],\n",
    "            n_features=data_reshaped.shape[2],\n",
    "            latent_size=37,\n",
    "            encoder_sizes=(128,128),\n",
    "            decoder_sizes=(256,256),\n",
    "            kernel=\"cauchy\",\n",
    "            beta=0.2,\n",
    "            M=1,  #The number of Monte Carlo samples for ELBO estimation during training.\n",
    "            K=1,  #The number of importance weights for IWAE model training loss.\n",
    "            sigma=1.005, # The scale parameter for a kernel function\n",
    "            length_scale=7.0, #The length scale parameter for a kernel function\n",
    "            kernel_scales=1, #The number of different length scales over latent space dimensions\n",
    "            window_size=24,  # Window size for the inference CNN.\n",
    "            batch_size=params[\"batch_size\"],\n",
    "            # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "            epochs=params[\"epochs\"],\n",
    "            # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "            # You can leave it to defualt as None to disable early stopping.\n",
    "            patience=3,\n",
    "            # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "            # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "            optimizer=Adam(lr=params[\"lr\"]),\n",
    "            # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "            # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "            # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "            num_workers=0,\n",
    "            # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "            # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "            device=device,\n",
    "            # set the path for saving tensorboard and trained model files \n",
    "            saving_path=\"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model\",\n",
    "            # only save the best model after training finished.\n",
    "            # You can also set it as \"better\" to save models performing better ever during training.\n",
    "            model_saving_strategy=\"best\",\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
    "        gp_vae.fit(train_set={\"X\": train_scaled}, val_set={\"X\": val_X_masked, \"X_ori\": val_X_ori})\n",
    "        gp_vae_results = gp_vae.predict({\"X\": test_X_masked}, n_sampling_times=2)\n",
    "        gp_vae_imputation = gp_vae_results[\"imputation\"]\n",
    "\n",
    "        print(f\"The shape of gp_vae_imputation is {gp_vae_imputation.shape}\")\n",
    "\n",
    "        # for error calculation, we need to take the mean value of the multiple samplings for each data sample\n",
    "        mean_gp_vae_imputation = gp_vae_imputation.mean(axis=1)\n",
    "\n",
    "        test_imputation_denorm = inverse_scale(mean_gp_vae_imputation, scalers)\n",
    "        test_ori_denorm = inverse_scale(test_X_ori, scalers)\n",
    "\n",
    "        mae_list, rmse_list = [], []\n",
    "        for i in range(n_features):\n",
    "            mask_i = test_mask[:, :, i]\n",
    "            pred = test_imputation_denorm[:, :, i][mask_i]\n",
    "            true = test_ori_denorm[:, :, i][mask_i]\n",
    "            if len(true) == 0: continue\n",
    "            mae = np.mean(np.abs(pred - true))\n",
    "            rmse = np.sqrt(mean_squared_error(true, pred))\n",
    "            mae_list.append(mae)\n",
    "            rmse_list.append(rmse)\n",
    "            mlflow.log_metric(f\"MAE_{i}\", mae)\n",
    "            mlflow.log_metric(f\"RMSE_{i}\", rmse)\n",
    "\n",
    "        mlflow.log_metric(\"avg_mae\", np.mean(mae_list))\n",
    "        mlflow.log_metric(\"avg_rmse\", np.mean(rmse_list))\n",
    "\n",
    "        return np.mean(mae_list), np.mean(rmse_list)\n",
    "\n",
    "# Run Optuna study\n",
    "mlflow.set_experiment(\"GP-VAE-2\")\n",
    "with mlflow.start_run(run_name=\"GPVAE_Optuna_Study\") as parent_run:\n",
    "    study = optuna.create_study(directions=[\"minimize\",\"minimize\"])\n",
    "    study.optimize(objective, n_trials=10)\n",
    "\n",
    "    best_params = study.best_trial.params\n",
    "    best_value = study.best_trial.value # or best_trial.values if multi-objective\n",
    "\n",
    "    # Log best parameters\n",
    "    mlflow.log_params(best_params)\n",
    "\n",
    "    # Log best metric(s)\n",
    "    mlflow.log_metric(\"best_objective_value\", best_value)\n",
    "\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Best Objective Value:\", best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fr_eng',\n",
       " 'te_exh_cyl_out__0',\n",
       " 'pd_air_ic__0',\n",
       " 'pr_exh_turb_out__0',\n",
       " 'te_air_ic_out__0',\n",
       " 'te_seawater',\n",
       " 'te_air_comp_in_a__0',\n",
       " 'te_air_comp_in_b__0',\n",
       " 'fr_tc__0',\n",
       " 'pr_baro',\n",
       " 'pd_air_ic__0_1',\n",
       " 'pr_exh_rec',\n",
       " 'te_exh_turb_in__0',\n",
       " 'te_exh_turb_out__0',\n",
       " 'bo_aux_blower_running',\n",
       " 're_eng_load',\n",
       " 'pr_air_scav_ecs',\n",
       " 'pr_air_scav',\n",
       " 'te_air_scav_rec',\n",
       " 'te_air_ic_out__0_1',\n",
       " 'pr_cyl_comp__0',\n",
       " 'pr_cyl_max__0',\n",
       " 'se_mip__0',\n",
       " 'te_exh_cyl_out__0_1',\n",
       " 'fr_eng_setpoint',\n",
       " 'te_air_scav_rec_iso',\n",
       " 'pr_cyl_max_mv_iso',\n",
       " 'pr_cyl_comp_mv_iso',\n",
       " 'fr_eng_ecs',\n",
       " 'pr_air_scav_iso',\n",
       " 'engine_type_G80ME-C9.5-GI-LPSCR']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.82180184402368"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[\"te_exh_cyl_out__0\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14288534662599145"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[\"fr_eng\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2921270072758015"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[\"te_air_scav_rec\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>fr_eng</th>\n",
       "      <th>te_exh_cyl_out__0</th>\n",
       "      <th>pd_air_ic__0</th>\n",
       "      <th>pr_exh_turb_out__0</th>\n",
       "      <th>te_air_ic_out__0</th>\n",
       "      <th>te_seawater</th>\n",
       "      <th>te_air_comp_in_a__0</th>\n",
       "      <th>te_air_comp_in_b__0</th>\n",
       "      <th>fr_tc__0</th>\n",
       "      <th>...</th>\n",
       "      <th>pr_cyl_max__0</th>\n",
       "      <th>se_mip__0</th>\n",
       "      <th>te_exh_cyl_out__0_1</th>\n",
       "      <th>fr_eng_setpoint</th>\n",
       "      <th>te_air_scav_rec_iso</th>\n",
       "      <th>pr_cyl_max_mv_iso</th>\n",
       "      <th>pr_cyl_comp_mv_iso</th>\n",
       "      <th>fr_eng_ecs</th>\n",
       "      <th>pr_air_scav_iso</th>\n",
       "      <th>engine_type_G80ME-C9.5-GI-LPSCR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>105527</td>\n",
       "      <td>105527.000000</td>\n",
       "      <td>105527.000000</td>\n",
       "      <td>105527.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>105527.000000</td>\n",
       "      <td>1.055270e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.053790e+05</td>\n",
       "      <td>1.053790e+05</td>\n",
       "      <td>105527.000000</td>\n",
       "      <td>105527.000000</td>\n",
       "      <td>63515.000000</td>\n",
       "      <td>6.268200e+04</td>\n",
       "      <td>6.268200e+04</td>\n",
       "      <td>105527.000000</td>\n",
       "      <td>62682.000000</td>\n",
       "      <td>105527.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2024-03-04 03:38:36.214807296</td>\n",
       "      <td>0.832822</td>\n",
       "      <td>529.817308</td>\n",
       "      <td>3111.211728</td>\n",
       "      <td>NaN</td>\n",
       "      <td>307.180888</td>\n",
       "      <td>2.731500e+02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.451904e+07</td>\n",
       "      <td>9.473421e+05</td>\n",
       "      <td>529.817308</td>\n",
       "      <td>0.832486</td>\n",
       "      <td>304.498497</td>\n",
       "      <td>1.456075e+07</td>\n",
       "      <td>1.077581e+07</td>\n",
       "      <td>0.832822</td>\n",
       "      <td>99827.213871</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2023-10-01 05:00:00</td>\n",
       "      <td>0.169932</td>\n",
       "      <td>319.150000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>293.350000</td>\n",
       "      <td>2.731500e+02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.044394e+06</td>\n",
       "      <td>-3.990992e+04</td>\n",
       "      <td>319.150000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>299.030721</td>\n",
       "      <td>4.031951e+06</td>\n",
       "      <td>4.143255e+06</td>\n",
       "      <td>0.169932</td>\n",
       "      <td>970.760107</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2023-12-12 22:18:30</td>\n",
       "      <td>0.777478</td>\n",
       "      <td>513.150000</td>\n",
       "      <td>1770.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>305.850000</td>\n",
       "      <td>2.731500e+02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.282290e+07</td>\n",
       "      <td>7.899361e+05</td>\n",
       "      <td>513.150000</td>\n",
       "      <td>0.778309</td>\n",
       "      <td>302.687664</td>\n",
       "      <td>1.300370e+07</td>\n",
       "      <td>9.346764e+06</td>\n",
       "      <td>0.777478</td>\n",
       "      <td>60163.341706</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2024-03-05 10:30:00</td>\n",
       "      <td>0.897176</td>\n",
       "      <td>542.150000</td>\n",
       "      <td>3290.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>307.150000</td>\n",
       "      <td>2.731500e+02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.566881e+07</td>\n",
       "      <td>1.045076e+06</td>\n",
       "      <td>542.150000</td>\n",
       "      <td>0.899969</td>\n",
       "      <td>303.826959</td>\n",
       "      <td>1.551522e+07</td>\n",
       "      <td>1.157614e+07</td>\n",
       "      <td>0.897176</td>\n",
       "      <td>106679.359693</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2024-05-13 19:55:30</td>\n",
       "      <td>0.917475</td>\n",
       "      <td>553.150000</td>\n",
       "      <td>4370.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>308.150000</td>\n",
       "      <td>2.731500e+02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.637885e+07</td>\n",
       "      <td>1.121633e+06</td>\n",
       "      <td>553.150000</td>\n",
       "      <td>0.916650</td>\n",
       "      <td>307.118440</td>\n",
       "      <td>1.628880e+07</td>\n",
       "      <td>1.227650e+07</td>\n",
       "      <td>0.917475</td>\n",
       "      <td>131557.217291</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2024-07-31 16:36:00</td>\n",
       "      <td>1.022311</td>\n",
       "      <td>597.150000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>318.750000</td>\n",
       "      <td>2.731500e+02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.882522e+07</td>\n",
       "      <td>1.513627e+06</td>\n",
       "      <td>597.150000</td>\n",
       "      <td>1.016633</td>\n",
       "      <td>310.080785</td>\n",
       "      <td>1.897977e+07</td>\n",
       "      <td>1.560624e+07</td>\n",
       "      <td>1.022311</td>\n",
       "      <td>225486.996753</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142885</td>\n",
       "      <td>39.821802</td>\n",
       "      <td>1808.911991</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.789162</td>\n",
       "      <td>5.684369e-14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.560751e+06</td>\n",
       "      <td>2.658281e+05</td>\n",
       "      <td>39.821802</td>\n",
       "      <td>0.143879</td>\n",
       "      <td>2.474538</td>\n",
       "      <td>2.467797e+06</td>\n",
       "      <td>2.251713e+06</td>\n",
       "      <td>0.142885</td>\n",
       "      <td>52369.802338</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                time         fr_eng  te_exh_cyl_out__0  \\\n",
       "count                         105527  105527.000000      105527.000000   \n",
       "mean   2024-03-04 03:38:36.214807296       0.832822         529.817308   \n",
       "min              2023-10-01 05:00:00       0.169932         319.150000   \n",
       "25%              2023-12-12 22:18:30       0.777478         513.150000   \n",
       "50%              2024-03-05 10:30:00       0.897176         542.150000   \n",
       "75%              2024-05-13 19:55:30       0.917475         553.150000   \n",
       "max              2024-07-31 16:36:00       1.022311         597.150000   \n",
       "std                              NaN       0.142885          39.821802   \n",
       "\n",
       "        pd_air_ic__0  pr_exh_turb_out__0  te_air_ic_out__0   te_seawater  \\\n",
       "count  105527.000000                 0.0     105527.000000  1.055270e+05   \n",
       "mean     3111.211728                 NaN        307.180888  2.731500e+02   \n",
       "min         0.000000                 NaN        293.350000  2.731500e+02   \n",
       "25%      1770.000000                 NaN        305.850000  2.731500e+02   \n",
       "50%      3290.000000                 NaN        307.150000  2.731500e+02   \n",
       "75%      4370.000000                 NaN        308.150000  2.731500e+02   \n",
       "max     10000.000000                 NaN        318.750000  2.731500e+02   \n",
       "std      1808.911991                 NaN          1.789162  5.684369e-14   \n",
       "\n",
       "       te_air_comp_in_a__0  te_air_comp_in_b__0  fr_tc__0  ...  pr_cyl_max__0  \\\n",
       "count                  0.0                  0.0       0.0  ...   1.053790e+05   \n",
       "mean                   NaN                  NaN       NaN  ...   1.451904e+07   \n",
       "min                    NaN                  NaN       NaN  ...   4.044394e+06   \n",
       "25%                    NaN                  NaN       NaN  ...   1.282290e+07   \n",
       "50%                    NaN                  NaN       NaN  ...   1.566881e+07   \n",
       "75%                    NaN                  NaN       NaN  ...   1.637885e+07   \n",
       "max                    NaN                  NaN       NaN  ...   1.882522e+07   \n",
       "std                    NaN                  NaN       NaN  ...   2.560751e+06   \n",
       "\n",
       "          se_mip__0  te_exh_cyl_out__0_1  fr_eng_setpoint  \\\n",
       "count  1.053790e+05        105527.000000    105527.000000   \n",
       "mean   9.473421e+05           529.817308         0.832486   \n",
       "min   -3.990992e+04           319.150000         0.000000   \n",
       "25%    7.899361e+05           513.150000         0.778309   \n",
       "50%    1.045076e+06           542.150000         0.899969   \n",
       "75%    1.121633e+06           553.150000         0.916650   \n",
       "max    1.513627e+06           597.150000         1.016633   \n",
       "std    2.658281e+05            39.821802         0.143879   \n",
       "\n",
       "       te_air_scav_rec_iso  pr_cyl_max_mv_iso  pr_cyl_comp_mv_iso  \\\n",
       "count         63515.000000       6.268200e+04        6.268200e+04   \n",
       "mean            304.498497       1.456075e+07        1.077581e+07   \n",
       "min             299.030721       4.031951e+06        4.143255e+06   \n",
       "25%             302.687664       1.300370e+07        9.346764e+06   \n",
       "50%             303.826959       1.551522e+07        1.157614e+07   \n",
       "75%             307.118440       1.628880e+07        1.227650e+07   \n",
       "max             310.080785       1.897977e+07        1.560624e+07   \n",
       "std               2.474538       2.467797e+06        2.251713e+06   \n",
       "\n",
       "          fr_eng_ecs  pr_air_scav_iso  engine_type_G80ME-C9.5-GI-LPSCR  \n",
       "count  105527.000000     62682.000000                         105527.0  \n",
       "mean        0.832822     99827.213871                              1.0  \n",
       "min         0.169932       970.760107                              1.0  \n",
       "25%         0.777478     60163.341706                              1.0  \n",
       "50%         0.897176    106679.359693                              1.0  \n",
       "75%         0.917475    131557.217291                              1.0  \n",
       "max         1.022311    225486.996753                              1.0  \n",
       "std         0.142885     52369.802338                              0.0  \n",
       "\n",
       "[8 rows x 32 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "759.7741848141841"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[\"pr_baro\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "759.7741848141841"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[\"pr_baro\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2560750.9406238147"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[\"pr_cyl_max__0\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d232c1-f081-410b-9e9d-cb7a3ae12c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column time has 0 NaN values\n",
      "Column time has 0.0 Missing_rate\n",
      "Column fr_eng has 0 NaN values\n",
      "Column fr_eng has 0.0 Missing_rate\n",
      "Column te_exh_cyl_out__0 has 0 NaN values\n",
      "Column te_exh_cyl_out__0 has 0.0 Missing_rate\n",
      "Column pd_air_ic__0 has 0 NaN values\n",
      "Column pd_air_ic__0 has 0.0 Missing_rate\n",
      "Column pr_exh_turb_out__0 has 316581 NaN values\n",
      "Column pr_exh_turb_out__0 has 1.0 Missing_rate\n",
      "Column te_air_ic_out__0 has 0 NaN values\n",
      "Column te_air_ic_out__0 has 0.0 Missing_rate\n",
      "Column te_seawater has 0 NaN values\n",
      "Column te_seawater has 0.0 Missing_rate\n",
      "Column te_air_comp_in_a__0 has 316581 NaN values\n",
      "Column te_air_comp_in_a__0 has 1.0 Missing_rate\n",
      "Column te_air_comp_in_b__0 has 316581 NaN values\n",
      "Column te_air_comp_in_b__0 has 1.0 Missing_rate\n",
      "Column fr_tc__0 has 316581 NaN values\n",
      "Column fr_tc__0 has 1.0 Missing_rate\n",
      "Column pr_baro has 0 NaN values\n",
      "Column pr_baro has 0.0 Missing_rate\n",
      "Column pd_air_ic__0_1 has 0 NaN values\n",
      "Column pd_air_ic__0_1 has 0.0 Missing_rate\n",
      "Column pr_exh_rec has 0 NaN values\n",
      "Column pr_exh_rec has 0.0 Missing_rate\n",
      "Column te_exh_turb_in__0 has 316581 NaN values\n",
      "Column te_exh_turb_in__0 has 1.0 Missing_rate\n",
      "Column te_exh_turb_out__0 has 316581 NaN values\n",
      "Column te_exh_turb_out__0 has 1.0 Missing_rate\n",
      "Column bo_aux_blower_running has 0 NaN values\n",
      "Column bo_aux_blower_running has 0.0 Missing_rate\n",
      "Column re_eng_load has 426 NaN values\n",
      "Column re_eng_load has 0.0013456271854596453 Missing_rate\n",
      "Column pr_air_scav_ecs has 0 NaN values\n",
      "Column pr_air_scav_ecs has 0.0 Missing_rate\n",
      "Column pr_air_scav has 0 NaN values\n",
      "Column pr_air_scav has 0.0 Missing_rate\n",
      "Column te_air_scav_rec has 0 NaN values\n",
      "Column te_air_scav_rec has 0.0 Missing_rate\n",
      "Column te_air_ic_out__0_1 has 0 NaN values\n",
      "Column te_air_ic_out__0_1 has 0.0 Missing_rate\n",
      "Column pr_cyl_comp__0 has 426 NaN values\n",
      "Column pr_cyl_comp__0 has 0.0013456271854596453 Missing_rate\n",
      "Column pr_cyl_max__0 has 426 NaN values\n",
      "Column pr_cyl_max__0 has 0.0013456271854596453 Missing_rate\n",
      "Column se_mip__0 has 426 NaN values\n",
      "Column se_mip__0 has 0.0013456271854596453 Missing_rate\n",
      "Column te_exh_cyl_out__0_1 has 0 NaN values\n",
      "Column te_exh_cyl_out__0_1 has 0.0 Missing_rate\n",
      "Column fr_eng_setpoint has 0 NaN values\n",
      "Column fr_eng_setpoint has 0.0 Missing_rate\n",
      "Column te_air_scav_rec_iso has 126055 NaN values\n",
      "Column te_air_scav_rec_iso has 0.3981761381763277 Missing_rate\n",
      "Column pr_cyl_max_mv_iso has 128557 NaN values\n",
      "Column pr_cyl_max_mv_iso has 0.40607932882895686 Missing_rate\n",
      "Column pr_cyl_comp_mv_iso has 128557 NaN values\n",
      "Column pr_cyl_comp_mv_iso has 0.40607932882895686 Missing_rate\n",
      "Column fr_eng_ecs has 0 NaN values\n",
      "Column fr_eng_ecs has 0.0 Missing_rate\n",
      "Column pr_air_scav_iso has 128557 NaN values\n",
      "Column pr_air_scav_iso has 0.40607932882895686 Missing_rate\n",
      "Column engine_type_G80ME-C9.5-GI-LPSCR has 0 NaN values\n",
      "Column engine_type_G80ME-C9.5-GI-LPSCR has 0.0 Missing_rate\n",
      "Original size:316581, Sampled size: 105527\n",
      "Reshaped data:(105527, 31)\n",
      "CUDA available: True\n",
      "Using CUDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "[I 2025-05-20 00:33:10,118] A new study created in memory with name: no-name-5f63202d-f249-4cf2-984e-5320fadef355\n",
      "2025-05-20 00:33:10 [INFO]: Using the given device: cuda\n",
      "2025-05-20 00:33:10 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T003310\n",
      "2025-05-20 00:33:10 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T003310/tensorboard\n",
      "2025-05-20 00:33:10 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n",
      "2025-05-20 00:33:10 [ERROR]: ❌ Exception: sqrt(): argument 'input' (position 1) must be Tensor, not float\n",
      "[W 2025-05-20 00:33:10,168] Trial 0 failed with parameters: {'lr': 0.0001422856010872562, 'epochs': 46, 'batch_size': 96, 'length_scale': 4.9602277955495016, 'beta': 0.4271430372817627, 'kernel': 'matern'} because of the following error: RuntimeError('Training got interrupted. Model was not trained. Please investigate the error printed above.').\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/pypots/base.py\", line 737, in _train_model\n",
      "    results = self.model(inputs, calc_criterion=True)\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/pypots/imputation/gpvae/core.py\", line 101, in forward\n",
      "    elbo_loss = self.backbone(X, missing_mask)\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/pypots/nn/modules/gpvae/backbone.py\", line 160, in forward\n",
      "    self.prior = self._init_prior(device=X.device)\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/pypots/nn/modules/gpvae/backbone.py\", line 121, in _init_prior\n",
      "    kernel_matrices.append(matern_kernel(self.time_length, self.length_scale / 2**i))\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/pypots/nn/modules/gpvae/layers.py\", line 39, in matern_kernel\n",
      "    distance_matrix_scaled = distance_matrix / torch.sqrt(length_scale).type(torch.float32)\n",
      "TypeError: sqrt(): argument 'input' (position 1) must be Tensor, not float\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"<ipython-input-12-54ee6dc3f036>\", line 303, in objective\n",
      "    gp_vae.fit(train_set={\"X\": train_scaled}, val_set={\"X\": val_X_masked, \"X_ori\": val_X_ori})\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/pypots/imputation/gpvae/model.py\", line 269, in fit\n",
      "    self._train_model(train_dataloader, val_dataloader)\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/pypots/base.py\", line 814, in _train_model\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.\n",
      "[W 2025-05-20 00:33:10,171] Trial 0 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/15c38ce0534446fcb51213a37a58a9b7\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n",
      "🏃 View run GPVAE_Optuna_Study(2) at: http://localhost:5000/#/experiments/832352739106302318/runs/a1950ed8c0a04febb7e4910ed83d7038\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Training got interrupted. Model was not trained. Please investigate the error printed above.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/pypots/base.py:737\u001b[0m, in \u001b[0;36mBaseNNModel._train_model\u001b[0;34m(self, train_dataloader, val_dataloader)\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> 737\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(inputs, calc_criterion\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    738\u001b[0m loss \u001b[39m=\u001b[39m results[\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39msum()\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/pypots/imputation/gpvae/core.py:101\u001b[0m, in \u001b[0;36m_GPVAE.forward\u001b[0;34m(self, inputs, calc_criterion, n_sampling_times)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39mif\u001b[39;00m calc_criterion:\n\u001b[0;32m--> 101\u001b[0m     elbo_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone(X, missing_mask)\n\u001b[1;32m    102\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:  \u001b[39m# if in the training mode (the training stage), return loss result from training_loss\u001b[39;00m\n\u001b[1;32m    103\u001b[0m         \u001b[39m# `loss` is always the item for backward propagating to update the model\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/pypots/nn/modules/gpvae/backbone.py:160\u001b[0m, in \u001b[0;36mBackboneGPVAE.forward\u001b[0;34m(self, X, missing_mask)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprior \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprior \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init_prior(device\u001b[39m=\u001b[39;49mX\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    162\u001b[0m qz_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode(X)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/pypots/nn/modules/gpvae/backbone.py:121\u001b[0m, in \u001b[0;36mBackboneGPVAE._init_prior\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmatern\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 121\u001b[0m     kernel_matrices\u001b[39m.\u001b[39mappend(matern_kernel(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtime_length, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlength_scale \u001b[39m/\u001b[39;49m \u001b[39m2\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mi))\n\u001b[1;32m    122\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcauchy\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/pypots/nn/modules/gpvae/layers.py:39\u001b[0m, in \u001b[0;36mmatern_kernel\u001b[0;34m(T, length_scale)\u001b[0m\n\u001b[1;32m     38\u001b[0m distance_matrix \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mabs(xs_in \u001b[39m-\u001b[39m xs_out)\n\u001b[0;32m---> 39\u001b[0m distance_matrix_scaled \u001b[39m=\u001b[39m distance_matrix \u001b[39m/\u001b[39m torch\u001b[39m.\u001b[39;49msqrt(length_scale)\u001b[39m.\u001b[39mtype(torch\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m     40\u001b[0m kernel_matrix \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39mdistance_matrix_scaled)\n",
      "\u001b[0;31mTypeError\u001b[0m: sqrt(): argument 'input' (position 1) must be Tensor, not float",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 377\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[39mwith\u001b[39;00m mlflow\u001b[39m.\u001b[39mstart_run(run_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGPVAE_Optuna_Study(2)\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m parent_run:\n\u001b[1;32m    376\u001b[0m     study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 377\u001b[0m     study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\n\u001b[1;32m    379\u001b[0m     best_params \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_trial\u001b[39m.\u001b[39mparams\n\u001b[1;32m    380\u001b[0m     best_value \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_trial\u001b[39m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     _optimize(\n\u001b[1;32m    476\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    477\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    478\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    479\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    480\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    481\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    482\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    483\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    484\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    485\u001b[0m     )\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         _optimize_sequential(\n\u001b[1;32m     64\u001b[0m             study,\n\u001b[1;32m     65\u001b[0m             func,\n\u001b[1;32m     66\u001b[0m             n_trials,\n\u001b[1;32m     67\u001b[0m             timeout,\n\u001b[1;32m     68\u001b[0m             catch,\n\u001b[1;32m     69\u001b[0m             callbacks,\n\u001b[1;32m     70\u001b[0m             gc_after_trial,\n\u001b[1;32m     71\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     72\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     73\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     74\u001b[0m         )\n\u001b[1;32m     75\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    161\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    198\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[12], line 303\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    262\u001b[0m gp_vae \u001b[39m=\u001b[39m GPVAE(\n\u001b[1;32m    263\u001b[0m     n_steps\u001b[39m=\u001b[39mdata_reshaped\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m],\n\u001b[1;32m    264\u001b[0m     n_features\u001b[39m=\u001b[39mdata_reshaped\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m     model_saving_strategy\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbest\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    297\u001b[0m )\n\u001b[1;32m    302\u001b[0m \u001b[39m# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\u001b[39;00m\n\u001b[0;32m--> 303\u001b[0m gp_vae\u001b[39m.\u001b[39;49mfit(train_set\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m: train_scaled}, val_set\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m: val_X_masked, \u001b[39m\"\u001b[39;49m\u001b[39mX_ori\u001b[39;49m\u001b[39m\"\u001b[39;49m: val_X_ori})\n\u001b[1;32m    304\u001b[0m gp_vae_results \u001b[39m=\u001b[39m gp_vae\u001b[39m.\u001b[39mpredict({\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m: test_X_masked}, n_sampling_times\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    305\u001b[0m gp_vae_imputation \u001b[39m=\u001b[39m gp_vae_results[\u001b[39m\"\u001b[39m\u001b[39mimputation\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/pypots/imputation/gpvae/model.py:269\u001b[0m, in \u001b[0;36mGPVAE.fit\u001b[0;34m(self, train_set, val_set, file_type)\u001b[0m\n\u001b[1;32m    261\u001b[0m     val_dataloader \u001b[39m=\u001b[39m DataLoader(\n\u001b[1;32m    262\u001b[0m         val_dataset,\n\u001b[1;32m    263\u001b[0m         batch_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size,\n\u001b[1;32m    264\u001b[0m         shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    265\u001b[0m         num_workers\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_workers,\n\u001b[1;32m    266\u001b[0m     )\n\u001b[1;32m    268\u001b[0m \u001b[39m# Step 2: train the model and freeze it\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_model(train_dataloader, val_dataloader)\n\u001b[1;32m    270\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mload_state_dict(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_model_dict)\n\u001b[1;32m    272\u001b[0m \u001b[39m# Step 3: save the model if necessary\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/pypots/base.py:814\u001b[0m, in \u001b[0;36mBaseNNModel._train_model\u001b[0;34m(self, train_dataloader, val_dataloader)\u001b[0m\n\u001b[1;32m    812\u001b[0m logger\u001b[39m.\u001b[39merror(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m❌ Exception: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_model_dict \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# if no best model, raise error\u001b[39;00m\n\u001b[0;32m--> 814\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    815\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTraining got interrupted. Model was not trained. Please investigate the error printed above.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    816\u001b[0m     )\n\u001b[1;32m    817\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    818\u001b[0m     \u001b[39mRuntimeWarning\u001b[39;00m(\n\u001b[1;32m    819\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTraining got interrupted. Please investigate the error printed above.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    820\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mModel got trained and will load the best checkpoint so far for testing.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    821\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf you don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt want it, please try fit() again.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    822\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Training got interrupted. Model was not trained. Please investigate the error printed above."
     ]
    }
   ],
   "source": [
    "#Import Pypots Library\n",
    "from pypots.optim import Adam\n",
    "from pypots.imputation import GPVAE\n",
    "#from pypots.utils.metrics import calc_mae\n",
    "from pypots.nn.functional import calc_mae\n",
    "\n",
    "\n",
    "import argparse\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import data_insight\n",
    "from data_insight import setup_duckdb\n",
    "from duckdb import DuckDBPyConnection as DuckDB\n",
    "from duckdb import DuckDBPyRelation as Relation\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "from duckdb import DuckDBPyConnection as DuckDB\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna \n",
    "from optuna.visualization import plot_optimization_history\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, Dataset\n",
    "from pygrinder.missing_completely_at_random import mcar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import sensor_imputation_thesis.shared.load_data as load\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "#PatchTST might be an ideal choise if SAITS is too slow \n",
    "\n",
    "##Drop columns with different indexes while loading data.. Or the mean values \n",
    "\n",
    "df=pd.read_parquet(\"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/ny_df_for_pypots.parquet\")\n",
    "\n",
    "len(df)\n",
    "\n",
    "#current length of the dataframe is 119439\n",
    "\n",
    "# Check nan values in each column\n",
    "for col in df.columns:\n",
    "    print(f\"Column {col} has {df[col].isna().sum()} NaN values\")\n",
    "    missing_rate=df[col].isna().sum()/len(df[col])\n",
    "    print(f\"Column {col} has {missing_rate} Missing_rate\")\n",
    "\n",
    "\n",
    "#Try with smaller dataset, size 4000\n",
    "##SAMPLE the percengtage of the dataset, df.sample (averagely pick samples)\n",
    "#not df.sample cuz it will randomly select \n",
    "original_size=len(df)\n",
    "desired_fraction=0.3 #Select data every 3 minutes \n",
    "step=int(1/desired_fraction) #step_size=10 (sample every 10th (3/10) minute)\n",
    "\n",
    "#Systematic sampling: Start at a random offset to avoid bias \n",
    "start=np.random.randint(0,step) #Random start between 0-9\n",
    "df1=df.iloc[start::step].reset_index(drop=True)\n",
    "\n",
    "print(f\"Original size:{len(df)}, Sampled size: {len(df1)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Custom Dataset class\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Data processing code\n",
    "sensor_cols = [col for col in df1.columns if col != \"time\"]\n",
    "data = df1[sensor_cols].values\n",
    "\n",
    "#¤get feature names for printing mae later \n",
    "feature_names=df1[sensor_cols].columns.tolist()\n",
    "\n",
    "## Convert data to 3D arrays of shape n_samples, n_timesteps, n_features, X_ori refers to the original data without missing values \n",
    "## Reconstruct all columns simultaneously  #num_features: 119\n",
    "n_features = data.shape[1]  # exclude the time column\n",
    "n_steps = 20 #60 (was 60 previously) #(TRY TO CHANGE HERE)  # # window length, 1440 steps = 24 hours of 1-minute data, but here is revised to 60 again\n",
    "#total_elements = data.shape[0] * data.shape[1]\n",
    "n_samples = data.shape[0] // n_steps \n",
    "\n",
    "\n",
    "\n",
    "# Reshape to (n_samples // n_steps, n_steps, n_features)\n",
    "#data_reshaped = data.reshape((n_samples, n_steps, n_features))\n",
    "data_reshaped=data[:n_samples*n_steps].reshape(n_samples,n_steps,n_features)\n",
    "print(f\"Reshaped data:{data.shape}\")\n",
    "\n",
    "#Split into train, test, val, fit scaler only on the train set (prevent data leakage)\n",
    "\n",
    "#train_size = int(0.6 * len(data))\n",
    "#val_size = int(0.2 * len(data))\n",
    "#test_size = len(data) - train_size - val_size\n",
    "\n",
    "#train_data = data_reshaped[:train_size]\n",
    "#val_data = data_reshaped[train_size:train_size + val_size]\n",
    "#test_data= data_reshaped[train_size + val_size:]\n",
    "\n",
    "\n",
    "#Apply time series split \n",
    "#Split into train(60%), val(20%), and test (20%)\n",
    "train_data, temp_data=train_test_split(data_reshaped,test_size=0.4,shuffle=True)\n",
    "val_data, test_data=train_test_split(temp_data, test_size=0.5, shuffle=False)\n",
    "\n",
    "##Normalization is important because of the nature of mse calculation of saits, columns with large \n",
    "#values dominate the loss, making metrics meaningless. SAITS computes MSE/MAE column-wise and averages \n",
    "#them across all columns \n",
    "#  Apply minmax scaler here \n",
    "#normalize each feature independently\n",
    "scalers={}\n",
    "\n",
    "\n",
    "#train_scaled = np.zeros_like(data_reshaped[train_size])  # Initialize the normalized data array\n",
    "#val_scaled=np.zeros_like(data_reshaped[train_size:train_size + val_size])\n",
    "#test_scaled=np.zeros_like(data_reshaped[train_size + val_size:])\n",
    "\n",
    "train_scaled = np.zeros_like(train_data)\n",
    "val_scaled = np.zeros_like(val_data)\n",
    "test_scaled = np.zeros_like(test_data)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(data_reshaped.shape[2]):\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1)) #changed to -1,1\n",
    "    # Flatten timesteps and samples for scaling\n",
    "    train_scaled[:, :, i] = scaler.fit_transform(train_data[:, :, i].reshape(-1, 1)).reshape(train_data.shape[0], train_data.shape[1])\n",
    "    val_scaled[:, :, i] = scaler.transform(val_data[:, :, i].reshape(-1, 1)).reshape(val_data.shape[0], val_data.shape[1])\n",
    "    test_scaled[:, :, i] = scaler.transform(test_data[:, :, i].reshape(-1, 1)).reshape(test_data.shape[0], test_data.shape[1])\n",
    "    scalers[i] = scaler  # Save scalers to inverse-transform later\n",
    "\n",
    "#Inverse Scale\n",
    "def inverse_scale(imputation, scalers):\n",
    "    n_samples, n_timesteps, n_features = imputation.shape\n",
    "    imputation_denorm = np.empty_like(imputation)\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        reshaped = imputation[:, :, i].reshape(-1, 1)\n",
    "        inversed = scalers[i].inverse_transform(reshaped)\n",
    "        imputation_denorm[:, :, i] = inversed.reshape(n_samples, n_timesteps)\n",
    "    \n",
    "    return imputation_denorm\n",
    "\n",
    "\n",
    "\n",
    "#Optional: Artificially mask. Mask 20% of the data (MIT part). Try masking 30% here \n",
    "def mcar_f(X, mask_ratio=0.3):\n",
    "    \"\"\"Apply MCAR only to observed values.\"\"\"\n",
    "    observed_mask=~np.isnan(X) #find observed positions\n",
    "    artificial_mask=mcar(X,mask_ratio).astype(bool) #generate MCAR mask, cast to boolean\n",
    "    #combine masks \n",
    "    combined_mask=observed_mask & artificial_mask\n",
    "\n",
    "    #Apply masking\n",
    "    X_masked=X.copy()\n",
    "    X_masked[combined_mask]=np.nan\n",
    "    return X_masked,combined_mask\n",
    "\n",
    "\n",
    "#Use mcar on validation data \n",
    "val_X_masked, val_mask =mcar_f(val_scaled)\n",
    "val_X_ori=val_scaled.copy() \n",
    "\n",
    "test_X_masked, test_mask =mcar_f(test_scaled)\n",
    "test_X_ori=test_scaled.copy() \n",
    "\n",
    "\n",
    "#?? Problem: Can't have the best input for testing\n",
    "#1.Create synthetic test_data cuz if I drop nan values for test set, there's basically nothing left\n",
    "#synthetic_data=np.random.randn(n_samples,n_steps,n_features)\n",
    "#test_X_masked,test_mask=mcar_f(synthetic_data)\n",
    "#test_X_ori=synthetic_data.copy() #Ground truth\n",
    "\n",
    "# 2, Ensure no NaN values in synthetic data\n",
    "#test_X_masked = np.nan_to_num(test_X_masked, nan=np.nanmean(test_X_masked))\n",
    "#test_X_ori = np.nan_to_num(test_X_ori, nan=np.nanmean(test_X_ori))\n",
    "\n",
    "\n",
    "\n",
    "class Config:\n",
    "    no_cuda = False\n",
    "    no_mps = False\n",
    "    seed = 1\n",
    "\n",
    "args=Config()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "use_mps = not args.no_mps and torch.backends.mps.is_available()\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "\n",
    "if args.cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA\")\n",
    "elif use_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "train_scaled = torch.tensor(train_scaled, dtype=torch.float32)\n",
    "val_X_masked = torch.tensor(val_X_masked, dtype=torch.float32)\n",
    "val_X_ori = torch.tensor(val_X_ori, dtype=torch.float32)\n",
    "\n",
    "train_scaled = train_scaled.to(device)\n",
    "val_X_masked = val_X_masked.to(device)\n",
    "val_X_ori = val_X_ori.to(device)\n",
    "\n",
    "\n",
    "#MLflow set up\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "mlflow.set_experiment(\"GP_VAE_2\")\n",
    "\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"lr\": trial.suggest_float(\"lr\", 1e-4, 1e-3, log=True),\n",
    "        \"epochs\": trial.suggest_int(\"epochs\", 10, 50),\n",
    "        \"batch_size\": trial.suggest_int(\"batch_size\", 32, 128, step=32),\n",
    "        \"length_scale\": trial.suggest_float(\"length_scale\",0.5,5.0),\n",
    "        \"beta\": trial.suggest_float(\"beta\",0.1,1.0),\n",
    "        \"kernel\":trial.suggest_categorical(\"kernel\",[\"cauchy\", \"diffusion\", \"rbf\", \"matern\"]),\n",
    "\n",
    "\n",
    "\n",
    "    }\n",
    "\n",
    "    with mlflow.start_run(run_name=\"GP-VAE-Trial\", nested=True):\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        gp_vae = GPVAE(\n",
    "            n_steps=data_reshaped.shape[1],\n",
    "            n_features=data_reshaped.shape[2],\n",
    "            latent_size=37, #should be the latent dimensions \n",
    "            encoder_sizes=(128,128), #should I change it here too?\n",
    "            decoder_sizes=(256,256), #should I change the model size?\n",
    "            kernel=params[\"kernel\"],\n",
    "            beta=params[\"beta\"], #The weight of KL divergence in ELBO\n",
    "            M=1,  #The number of Monte Carlo samples for ELBO estimation during training.\n",
    "            K=1,  #The number of importance weights for IWAE model training loss.\n",
    "            sigma=1.005, # The scale parameter for a kernel function\n",
    "            length_scale=params[\"length_scale\"], #The length scale parameter for a kernel function\n",
    "            kernel_scales=1, #The number of different length scales over latent space dimensions\n",
    "            window_size=24,  # Window size for the inference CNN.\n",
    "            batch_size=params[\"batch_size\"],\n",
    "            # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "            epochs=params[\"epochs\"],\n",
    "            # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "            # You can leave it to defualt as None to disable early stopping.\n",
    "            patience=3,\n",
    "            # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "            # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "            optimizer=Adam(lr=params[\"lr\"]),\n",
    "            # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "            # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "            # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "            num_workers=0,\n",
    "            # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "            # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "            device=device,\n",
    "            # set the path for saving tensorboard and trained model files \n",
    "            saving_path=\"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model\",\n",
    "            # only save the best model after training finished.\n",
    "            # You can also set it as \"better\" to save models performing better ever during training.\n",
    "            model_saving_strategy=\"best\",\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
    "        gp_vae.fit(train_set={\"X\": train_scaled}, val_set={\"X\": val_X_masked, \"X_ori\": val_X_ori})\n",
    "        gp_vae_results = gp_vae.predict({\"X\": test_X_masked}, n_sampling_times=2)\n",
    "        gp_vae_imputation = gp_vae_results[\"imputation\"]\n",
    "\n",
    "        print(f\"The shape of gp_vae_imputation is {gp_vae_imputation.shape}\")\n",
    "\n",
    "        # for error calculation, we need to take the mean value of the multiple samplings for each data sample\n",
    "        mean_gp_vae_imputation = gp_vae_imputation.mean(axis=1)\n",
    "\n",
    "        test_imputation_denorm = inverse_scale(mean_gp_vae_imputation, scalers)\n",
    "        test_ori_denorm = inverse_scale(test_X_ori, scalers)\n",
    "\n",
    "        mae_list, rmse_list = [], []\n",
    "        for i in range(n_features):\n",
    "            mask_i = test_mask[:, :, i]\n",
    "            pred = test_imputation_denorm[:, :, i][mask_i]\n",
    "            true = test_ori_denorm[:, :, i][mask_i]\n",
    "            if len(true) == 0: continue\n",
    "            mae = np.mean(np.abs(pred - true))\n",
    "            rmse = np.sqrt(mean_squared_error(true, pred))\n",
    "            mae_list.append(mae)\n",
    "            rmse_list.append(rmse)\n",
    "            mlflow.log_metric(f\"MAE_{i}\", mae)\n",
    "            mlflow.log_metric(f\"RMSE_{i}\", rmse)\n",
    "\n",
    "         # Calculate metrics\n",
    "        mae_per_feature = []\n",
    "        rmse_per_feature=[]\n",
    "        percentage_mae_per_feature = []\n",
    "\n",
    "        for i in range(n_features):\n",
    "            imputation_i = test_imputation_denorm[:, :, i]\n",
    "            ground_truth_i = test_ori_denorm[:, :, i]\n",
    "            mask_i = test_mask[:, :, i]\n",
    "            if np.isnan(imputation_i).any() or np.isnan(ground_truth_i).any():\n",
    "                continue\n",
    "            mae_i = calc_mae(imputation_i, ground_truth_i, mask_i)\n",
    "            mae_per_feature.append(mae_i)\n",
    "            rmse_i = np.sqrt(mean_squared_error(imputation_i, ground_truth_i))\n",
    "            rmse_per_feature.append(rmse)\n",
    "\n",
    "            #Calculate the original standard deviation for the feature\n",
    "            std_dev_i = np.std(ground_truth_i[mask_i == 1])\n",
    "             # Calculate the percentage of MAE relative to the standard deviation   \n",
    "            if std_dev_i != 0:\n",
    "                percentage_mae_i = (mae_i / std_dev_i) * 100\n",
    "                percentage_mae_per_feature.append(percentage_mae_i)\n",
    "            else:\n",
    "                 percentage_mae_i = float('inf')\n",
    "            \n",
    "            mlflow.log_metric(f\"MAE_{feature_names[i]}\", mae_i)\n",
    "            mlflow.log_metric(f\"RMSE_{feature_names[i]}\",rmse_i)\n",
    "            mlflow.log_metric(f\"Percentage_MAE_{feature_names[i]}\", percentage_mae_i)\n",
    "\n",
    "        avg_mae = np.mean(mae_per_feature)\n",
    "        #avg_rmse=np.mean(rmse_per_feature)\n",
    "       \n",
    "        mlflow.log_metric(\"avg_mae\", np.mean(mae_list))\n",
    "        mlflow.log_metric(\"avg_rmse\", np.mean(rmse_list))\n",
    "\n",
    "        trial.set_user_attr(\"model_state_dict\", GPVAE.state_dict())\n",
    "        trial.set_user_attr(\"mlflow_run_id\", run.info.run_id)\n",
    "\n",
    "        return avg_mae\n",
    "\n",
    "    print(\"MAE per feature:\", mae_per_feature)\n",
    "    print(\"RMSE per feature\",rmse_per_feature)\n",
    "    print(\"Percentage MAE per feature:\", percentage_mae_per_feature)\n",
    "   \n",
    "\n",
    "# Run Optuna study\n",
    "mlflow.set_experiment(\"GP-VAE-2\")\n",
    "with mlflow.start_run(run_name=\"GPVAE_Optuna_Study(2)\") as parent_run:\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "\n",
    "    best_params = study.best_trial.params\n",
    "    best_value = study.best_trial.value\n",
    "    best_run_id = study.best_trial.user_attrs[\"mlflow_run_id\"]\n",
    "\n",
    "\n",
    "    \n",
    "    # Log best parameters\n",
    "    mlflow.log_params(best_params)\n",
    "\n",
    "    # Log best metric(s)\n",
    "    mlflow.log_metric(\"best_objective_value\", best_value)\n",
    "    mlflow.log_param(\"best_run_id\", best_run_id)\n",
    "\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Best Objective Value:\", best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ea30cb-91f1-473b-a655-18525fe7e208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column time has 0 NaN values\n",
      "Column time has 0.0 Missing_rate\n",
      "Column fr_eng has 0 NaN values\n",
      "Column fr_eng has 0.0 Missing_rate\n",
      "Column te_exh_cyl_out__0 has 0 NaN values\n",
      "Column te_exh_cyl_out__0 has 0.0 Missing_rate\n",
      "Column pd_air_ic__0 has 0 NaN values\n",
      "Column pd_air_ic__0 has 0.0 Missing_rate\n",
      "Column pr_exh_turb_out__0 has 316581 NaN values\n",
      "Column pr_exh_turb_out__0 has 1.0 Missing_rate\n",
      "Column te_air_ic_out__0 has 0 NaN values\n",
      "Column te_air_ic_out__0 has 0.0 Missing_rate\n",
      "Column te_seawater has 0 NaN values\n",
      "Column te_seawater has 0.0 Missing_rate\n",
      "Column te_air_comp_in_a__0 has 316581 NaN values\n",
      "Column te_air_comp_in_a__0 has 1.0 Missing_rate\n",
      "Column te_air_comp_in_b__0 has 316581 NaN values\n",
      "Column te_air_comp_in_b__0 has 1.0 Missing_rate\n",
      "Column fr_tc__0 has 316581 NaN values\n",
      "Column fr_tc__0 has 1.0 Missing_rate\n",
      "Column pr_baro has 0 NaN values\n",
      "Column pr_baro has 0.0 Missing_rate\n",
      "Column pd_air_ic__0_1 has 0 NaN values\n",
      "Column pd_air_ic__0_1 has 0.0 Missing_rate\n",
      "Column pr_exh_rec has 0 NaN values\n",
      "Column pr_exh_rec has 0.0 Missing_rate\n",
      "Column te_exh_turb_in__0 has 316581 NaN values\n",
      "Column te_exh_turb_in__0 has 1.0 Missing_rate\n",
      "Column te_exh_turb_out__0 has 316581 NaN values\n",
      "Column te_exh_turb_out__0 has 1.0 Missing_rate\n",
      "Column bo_aux_blower_running has 0 NaN values\n",
      "Column bo_aux_blower_running has 0.0 Missing_rate\n",
      "Column re_eng_load has 426 NaN values\n",
      "Column re_eng_load has 0.0013456271854596453 Missing_rate\n",
      "Column pr_air_scav_ecs has 0 NaN values\n",
      "Column pr_air_scav_ecs has 0.0 Missing_rate\n",
      "Column pr_air_scav has 0 NaN values\n",
      "Column pr_air_scav has 0.0 Missing_rate\n",
      "Column te_air_scav_rec has 0 NaN values\n",
      "Column te_air_scav_rec has 0.0 Missing_rate\n",
      "Column te_air_ic_out__0_1 has 0 NaN values\n",
      "Column te_air_ic_out__0_1 has 0.0 Missing_rate\n",
      "Column pr_cyl_comp__0 has 426 NaN values\n",
      "Column pr_cyl_comp__0 has 0.0013456271854596453 Missing_rate\n",
      "Column pr_cyl_max__0 has 426 NaN values\n",
      "Column pr_cyl_max__0 has 0.0013456271854596453 Missing_rate\n",
      "Column se_mip__0 has 426 NaN values\n",
      "Column se_mip__0 has 0.0013456271854596453 Missing_rate\n",
      "Column te_exh_cyl_out__0_1 has 0 NaN values\n",
      "Column te_exh_cyl_out__0_1 has 0.0 Missing_rate\n",
      "Column fr_eng_setpoint has 0 NaN values\n",
      "Column fr_eng_setpoint has 0.0 Missing_rate\n",
      "Column te_air_scav_rec_iso has 126055 NaN values\n",
      "Column te_air_scav_rec_iso has 0.3981761381763277 Missing_rate\n",
      "Column pr_cyl_max_mv_iso has 128557 NaN values\n",
      "Column pr_cyl_max_mv_iso has 0.40607932882895686 Missing_rate\n",
      "Column pr_cyl_comp_mv_iso has 128557 NaN values\n",
      "Column pr_cyl_comp_mv_iso has 0.40607932882895686 Missing_rate\n",
      "Column fr_eng_ecs has 0 NaN values\n",
      "Column fr_eng_ecs has 0.0 Missing_rate\n",
      "Column pr_air_scav_iso has 128557 NaN values\n",
      "Column pr_air_scav_iso has 0.40607932882895686 Missing_rate\n",
      "Column engine_type_G80ME-C9.5-GI-LPSCR has 0 NaN values\n",
      "Column engine_type_G80ME-C9.5-GI-LPSCR has 0.0 Missing_rate\n",
      "Original size:316581, Sampled size: 105527\n",
      "Reshaped data:(105527, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "[I 2025-05-20 00:37:21,647] A new study created in memory with name: no-name-f68d0a3f-5771-496e-8567-d324eeb96eab\n",
      "2025-05-20 00:37:21 [INFO]: Using the given device: cuda\n",
      "2025-05-20 00:37:21 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T003721\n",
      "2025-05-20 00:37:21 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T003721/tensorboard\n",
      "2025-05-20 00:37:21 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Using CUDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 00:37:22 [INFO]: Epoch 001 - training loss (default): 29023.6647, validation loss: 36484.5052\n",
      "2025-05-20 00:37:22 [INFO]: Epoch 002 - training loss (default): 12432.6056, validation loss: 35268.0334\n",
      "2025-05-20 00:37:23 [INFO]: Epoch 003 - training loss (default): 10335.1963, validation loss: 34782.2183\n",
      "2025-05-20 00:37:24 [INFO]: Epoch 004 - training loss (default): 10241.5189, validation loss: 34596.8290\n",
      "2025-05-20 00:37:24 [INFO]: Epoch 005 - training loss (default): 10221.6641, validation loss: 34481.5977\n",
      "2025-05-20 00:37:25 [INFO]: Epoch 006 - training loss (default): 10211.6875, validation loss: 34380.6163\n",
      "2025-05-20 00:37:26 [INFO]: Epoch 007 - training loss (default): 10205.3616, validation loss: 34282.8368\n",
      "2025-05-20 00:37:26 [INFO]: Epoch 008 - training loss (default): 10200.9662, validation loss: 34190.5156\n",
      "2025-05-20 00:37:27 [INFO]: Epoch 009 - training loss (default): 10197.9365, validation loss: 34100.7474\n",
      "2025-05-20 00:37:28 [INFO]: Epoch 010 - training loss (default): 10195.8114, validation loss: 34018.6645\n",
      "2025-05-20 00:37:28 [INFO]: Epoch 011 - training loss (default): 10194.1307, validation loss: 33940.4479\n",
      "2025-05-20 00:37:29 [INFO]: Epoch 012 - training loss (default): 10192.7873, validation loss: 33869.2313\n",
      "2025-05-20 00:37:29 [INFO]: Epoch 013 - training loss (default): 10191.9400, validation loss: 33800.9991\n",
      "2025-05-20 00:37:30 [INFO]: Epoch 014 - training loss (default): 10190.9498, validation loss: 33737.1263\n",
      "2025-05-20 00:37:31 [INFO]: Epoch 015 - training loss (default): 10190.3446, validation loss: 33675.9983\n",
      "2025-05-20 00:37:31 [INFO]: Epoch 016 - training loss (default): 10189.7425, validation loss: 33619.9271\n",
      "2025-05-20 00:37:32 [INFO]: Epoch 017 - training loss (default): 10189.3702, validation loss: 33564.7643\n",
      "2025-05-20 00:37:33 [INFO]: Epoch 018 - training loss (default): 10188.9065, validation loss: 33512.9748\n",
      "2025-05-20 00:37:33 [INFO]: Epoch 019 - training loss (default): 10188.5576, validation loss: 33464.5130\n",
      "2025-05-20 00:37:34 [INFO]: Epoch 020 - training loss (default): 10188.2825, validation loss: 33418.5942\n",
      "2025-05-20 00:37:35 [INFO]: Epoch 021 - training loss (default): 10187.8232, validation loss: 33372.1198\n",
      "2025-05-20 00:37:35 [INFO]: Epoch 022 - training loss (default): 10187.5556, validation loss: 33328.9957\n",
      "2025-05-20 00:37:36 [INFO]: Epoch 023 - training loss (default): 10187.3365, validation loss: 33287.0699\n",
      "2025-05-20 00:37:36 [INFO]: Epoch 024 - training loss (default): 10187.0369, validation loss: 33247.9076\n",
      "2025-05-20 00:37:37 [INFO]: Epoch 025 - training loss (default): 10186.8944, validation loss: 33209.1836\n",
      "2025-05-20 00:37:38 [INFO]: Epoch 026 - training loss (default): 10186.7715, validation loss: 33172.0078\n",
      "2025-05-20 00:37:38 [INFO]: Epoch 027 - training loss (default): 10186.6078, validation loss: 33136.4601\n",
      "2025-05-20 00:37:39 [INFO]: Epoch 028 - training loss (default): 10186.5072, validation loss: 33102.2582\n",
      "2025-05-20 00:37:40 [INFO]: Epoch 029 - training loss (default): 10186.2527, validation loss: 33068.5838\n",
      "2025-05-20 00:37:40 [INFO]: Epoch 030 - training loss (default): 10186.2180, validation loss: 33036.5208\n",
      "2025-05-20 00:37:41 [INFO]: Epoch 031 - training loss (default): 10185.9451, validation loss: 33004.6250\n",
      "2025-05-20 00:37:42 [INFO]: Epoch 032 - training loss (default): 10185.9150, validation loss: 32973.9805\n",
      "2025-05-20 00:37:42 [INFO]: Epoch 033 - training loss (default): 10185.8380, validation loss: 32943.5195\n",
      "2025-05-20 00:37:42 [INFO]: Finished training. The best model is from epoch#33.\n",
      "2025-05-20 00:37:42 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T003721/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-05-20 00:37:43,270] Trial 0 failed with parameters: {'lr': 0.0003174471955074421, 'epochs': 33, 'batch_size': 128, 'length_scale': 1.9734374565972177, 'beta': 0.9182668977722848, 'kernel': 'rbf'} because of the following error: AttributeError(\"type object 'GPVAE' has no attribute 'state_dict'\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"<ipython-input-13-0ad50cddbc5f>\", line 363, in objective\n",
      "    trial.set_user_attr(\"model_state_dict\", GPVAE.state_dict())\n",
      "AttributeError: type object 'GPVAE' has no attribute 'state_dict'\n",
      "[W 2025-05-20 00:37:43,272] Trial 0 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/099574dbb9394265b2d09aab8318e34b\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n",
      "🏃 View run GPVAE_Optuna_Study(2) at: http://localhost:5000/#/experiments/832352739106302318/runs/63f9b9e323ea4559a36fb66b465aaa86\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'GPVAE' has no attribute 'state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 377\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[39mwith\u001b[39;00m mlflow\u001b[39m.\u001b[39mstart_run(run_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGPVAE_Optuna_Study(2)\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m parent_run:\n\u001b[1;32m    376\u001b[0m     study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 377\u001b[0m     study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\n\u001b[1;32m    379\u001b[0m     best_params \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_trial\u001b[39m.\u001b[39mparams\n\u001b[1;32m    380\u001b[0m     best_value \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_trial\u001b[39m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     _optimize(\n\u001b[1;32m    476\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    477\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    478\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    479\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    480\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    481\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    482\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    483\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    484\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    485\u001b[0m     )\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         _optimize_sequential(\n\u001b[1;32m     64\u001b[0m             study,\n\u001b[1;32m     65\u001b[0m             func,\n\u001b[1;32m     66\u001b[0m             n_trials,\n\u001b[1;32m     67\u001b[0m             timeout,\n\u001b[1;32m     68\u001b[0m             catch,\n\u001b[1;32m     69\u001b[0m             callbacks,\n\u001b[1;32m     70\u001b[0m             gc_after_trial,\n\u001b[1;32m     71\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     72\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     73\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     74\u001b[0m         )\n\u001b[1;32m     75\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    161\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    198\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[13], line 363\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    360\u001b[0m mlflow\u001b[39m.\u001b[39mlog_metric(\u001b[39m\"\u001b[39m\u001b[39mavg_mae\u001b[39m\u001b[39m\"\u001b[39m, np\u001b[39m.\u001b[39mmean(mae_list))\n\u001b[1;32m    361\u001b[0m mlflow\u001b[39m.\u001b[39mlog_metric(\u001b[39m\"\u001b[39m\u001b[39mavg_rmse\u001b[39m\u001b[39m\"\u001b[39m, np\u001b[39m.\u001b[39mmean(rmse_list))\n\u001b[0;32m--> 363\u001b[0m trial\u001b[39m.\u001b[39mset_user_attr(\u001b[39m\"\u001b[39m\u001b[39mmodel_state_dict\u001b[39m\u001b[39m\"\u001b[39m, GPVAE\u001b[39m.\u001b[39;49mstate_dict())\n\u001b[1;32m    364\u001b[0m trial\u001b[39m.\u001b[39mset_user_attr(\u001b[39m\"\u001b[39m\u001b[39mmlflow_run_id\u001b[39m\u001b[39m\"\u001b[39m, run\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mrun_id)\n\u001b[1;32m    366\u001b[0m \u001b[39mreturn\u001b[39;00m avg_mae\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'GPVAE' has no attribute 'state_dict'"
     ]
    }
   ],
   "source": [
    "#Import Pypots Library\n",
    "from pypots.optim import Adam\n",
    "from pypots.imputation import GPVAE\n",
    "#from pypots.utils.metrics import calc_mae\n",
    "from pypots.nn.functional import calc_mae\n",
    "\n",
    "\n",
    "import argparse\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import data_insight\n",
    "from data_insight import setup_duckdb\n",
    "from duckdb import DuckDBPyConnection as DuckDB\n",
    "from duckdb import DuckDBPyRelation as Relation\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "from duckdb import DuckDBPyConnection as DuckDB\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna \n",
    "from optuna.visualization import plot_optimization_history\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, Dataset\n",
    "from pygrinder.missing_completely_at_random import mcar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import sensor_imputation_thesis.shared.load_data as load\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "#PatchTST might be an ideal choise if SAITS is too slow \n",
    "\n",
    "##Drop columns with different indexes while loading data.. Or the mean values \n",
    "\n",
    "df=pd.read_parquet(\"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/ny_df_for_pypots.parquet\")\n",
    "\n",
    "len(df)\n",
    "\n",
    "#current length of the dataframe is 119439\n",
    "\n",
    "# Check nan values in each column\n",
    "for col in df.columns:\n",
    "    print(f\"Column {col} has {df[col].isna().sum()} NaN values\")\n",
    "    missing_rate=df[col].isna().sum()/len(df[col])\n",
    "    print(f\"Column {col} has {missing_rate} Missing_rate\")\n",
    "\n",
    "\n",
    "#Try with smaller dataset, size 4000\n",
    "##SAMPLE the percengtage of the dataset, df.sample (averagely pick samples)\n",
    "#not df.sample cuz it will randomly select \n",
    "original_size=len(df)\n",
    "desired_fraction=0.3 #Select data every 3 minutes \n",
    "step=int(1/desired_fraction) #step_size=10 (sample every 10th (3/10) minute)\n",
    "\n",
    "#Systematic sampling: Start at a random offset to avoid bias \n",
    "start=np.random.randint(0,step) #Random start between 0-9\n",
    "df1=df.iloc[start::step].reset_index(drop=True)\n",
    "\n",
    "print(f\"Original size:{len(df)}, Sampled size: {len(df1)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Custom Dataset class\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Data processing code\n",
    "sensor_cols = [col for col in df1.columns if col != \"time\"]\n",
    "data = df1[sensor_cols].values\n",
    "\n",
    "#¤get feature names for printing mae later \n",
    "feature_names=df1[sensor_cols].columns.tolist()\n",
    "\n",
    "## Convert data to 3D arrays of shape n_samples, n_timesteps, n_features, X_ori refers to the original data without missing values \n",
    "## Reconstruct all columns simultaneously  #num_features: 119\n",
    "n_features = data.shape[1]  # exclude the time column\n",
    "n_steps = 20 #60 (was 60 previously) #(TRY TO CHANGE HERE)  # # window length, 1440 steps = 24 hours of 1-minute data, but here is revised to 60 again\n",
    "#total_elements = data.shape[0] * data.shape[1]\n",
    "n_samples = data.shape[0] // n_steps \n",
    "\n",
    "\n",
    "\n",
    "# Reshape to (n_samples // n_steps, n_steps, n_features)\n",
    "#data_reshaped = data.reshape((n_samples, n_steps, n_features))\n",
    "data_reshaped=data[:n_samples*n_steps].reshape(n_samples,n_steps,n_features)\n",
    "print(f\"Reshaped data:{data.shape}\")\n",
    "\n",
    "#Split into train, test, val, fit scaler only on the train set (prevent data leakage)\n",
    "\n",
    "#train_size = int(0.6 * len(data))\n",
    "#val_size = int(0.2 * len(data))\n",
    "#test_size = len(data) - train_size - val_size\n",
    "\n",
    "#train_data = data_reshaped[:train_size]\n",
    "#val_data = data_reshaped[train_size:train_size + val_size]\n",
    "#test_data= data_reshaped[train_size + val_size:]\n",
    "\n",
    "\n",
    "#Apply time series split \n",
    "#Split into train(60%), val(20%), and test (20%)\n",
    "train_data, temp_data=train_test_split(data_reshaped,test_size=0.4,shuffle=True)\n",
    "val_data, test_data=train_test_split(temp_data, test_size=0.5, shuffle=False)\n",
    "\n",
    "##Normalization is important because of the nature of mse calculation of saits, columns with large \n",
    "#values dominate the loss, making metrics meaningless. SAITS computes MSE/MAE column-wise and averages \n",
    "#them across all columns \n",
    "#  Apply minmax scaler here \n",
    "#normalize each feature independently\n",
    "scalers={}\n",
    "\n",
    "\n",
    "#train_scaled = np.zeros_like(data_reshaped[train_size])  # Initialize the normalized data array\n",
    "#val_scaled=np.zeros_like(data_reshaped[train_size:train_size + val_size])\n",
    "#test_scaled=np.zeros_like(data_reshaped[train_size + val_size:])\n",
    "\n",
    "train_scaled = np.zeros_like(train_data)\n",
    "val_scaled = np.zeros_like(val_data)\n",
    "test_scaled = np.zeros_like(test_data)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(data_reshaped.shape[2]):\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1)) #changed to -1,1\n",
    "    # Flatten timesteps and samples for scaling\n",
    "    train_scaled[:, :, i] = scaler.fit_transform(train_data[:, :, i].reshape(-1, 1)).reshape(train_data.shape[0], train_data.shape[1])\n",
    "    val_scaled[:, :, i] = scaler.transform(val_data[:, :, i].reshape(-1, 1)).reshape(val_data.shape[0], val_data.shape[1])\n",
    "    test_scaled[:, :, i] = scaler.transform(test_data[:, :, i].reshape(-1, 1)).reshape(test_data.shape[0], test_data.shape[1])\n",
    "    scalers[i] = scaler  # Save scalers to inverse-transform later\n",
    "\n",
    "#Inverse Scale\n",
    "def inverse_scale(imputation, scalers):\n",
    "    n_samples, n_timesteps, n_features = imputation.shape\n",
    "    imputation_denorm = np.empty_like(imputation)\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        reshaped = imputation[:, :, i].reshape(-1, 1)\n",
    "        inversed = scalers[i].inverse_transform(reshaped)\n",
    "        imputation_denorm[:, :, i] = inversed.reshape(n_samples, n_timesteps)\n",
    "    \n",
    "    return imputation_denorm\n",
    "\n",
    "\n",
    "\n",
    "#Optional: Artificially mask. Mask 20% of the data (MIT part). Try masking 30% here \n",
    "def mcar_f(X, mask_ratio=0.3):\n",
    "    \"\"\"Apply MCAR only to observed values.\"\"\"\n",
    "    observed_mask=~np.isnan(X) #find observed positions\n",
    "    artificial_mask=mcar(X,mask_ratio).astype(bool) #generate MCAR mask, cast to boolean\n",
    "    #combine masks \n",
    "    combined_mask=observed_mask & artificial_mask\n",
    "\n",
    "    #Apply masking\n",
    "    X_masked=X.copy()\n",
    "    X_masked[combined_mask]=np.nan\n",
    "    return X_masked,combined_mask\n",
    "\n",
    "\n",
    "#Use mcar on validation data \n",
    "val_X_masked, val_mask =mcar_f(val_scaled)\n",
    "val_X_ori=val_scaled.copy() \n",
    "\n",
    "test_X_masked, test_mask =mcar_f(test_scaled)\n",
    "test_X_ori=test_scaled.copy() \n",
    "\n",
    "\n",
    "#?? Problem: Can't have the best input for testing\n",
    "#1.Create synthetic test_data cuz if I drop nan values for test set, there's basically nothing left\n",
    "#synthetic_data=np.random.randn(n_samples,n_steps,n_features)\n",
    "#test_X_masked,test_mask=mcar_f(synthetic_data)\n",
    "#test_X_ori=synthetic_data.copy() #Ground truth\n",
    "\n",
    "# 2, Ensure no NaN values in synthetic data\n",
    "#test_X_masked = np.nan_to_num(test_X_masked, nan=np.nanmean(test_X_masked))\n",
    "#test_X_ori = np.nan_to_num(test_X_ori, nan=np.nanmean(test_X_ori))\n",
    "\n",
    "\n",
    "\n",
    "class Config:\n",
    "    no_cuda = False\n",
    "    no_mps = False\n",
    "    seed = 1\n",
    "\n",
    "args=Config()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "use_mps = not args.no_mps and torch.backends.mps.is_available()\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "\n",
    "if args.cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA\")\n",
    "elif use_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "train_scaled = torch.tensor(train_scaled, dtype=torch.float32)\n",
    "val_X_masked = torch.tensor(val_X_masked, dtype=torch.float32)\n",
    "val_X_ori = torch.tensor(val_X_ori, dtype=torch.float32)\n",
    "\n",
    "train_scaled = train_scaled.to(device)\n",
    "val_X_masked = val_X_masked.to(device)\n",
    "val_X_ori = val_X_ori.to(device)\n",
    "\n",
    "\n",
    "#MLflow set up\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "mlflow.set_experiment(\"GP_VAE_2\")\n",
    "\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"lr\": trial.suggest_float(\"lr\", 1e-4, 1e-3, log=True),\n",
    "        \"epochs\": trial.suggest_int(\"epochs\", 10, 50),\n",
    "        \"batch_size\": trial.suggest_int(\"batch_size\", 32, 128, step=32),\n",
    "        \"length_scale\": trial.suggest_float(\"length_scale\",0.5,5.0),\n",
    "        \"beta\": trial.suggest_float(\"beta\",0.1,1.0),\n",
    "        \"kernel\":trial.suggest_categorical(\"kernel\",[\"cauchy\", \"diffusion\", \"rbf\"]),\n",
    "\n",
    "\n",
    "\n",
    "    }\n",
    "\n",
    "    with mlflow.start_run(run_name=\"GP-VAE-Trial\", nested=True):\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        gp_vae = GPVAE(\n",
    "            n_steps=data_reshaped.shape[1],\n",
    "            n_features=data_reshaped.shape[2],\n",
    "            latent_size=37, #should be the latent dimensions \n",
    "            encoder_sizes=(128,128), #should I change it here too?\n",
    "            decoder_sizes=(256,256), #should I change the model size?\n",
    "            kernel=params[\"kernel\"],\n",
    "            beta=params[\"beta\"], #The weight of KL divergence in ELBO\n",
    "            M=1,  #The number of Monte Carlo samples for ELBO estimation during training.\n",
    "            K=1,  #The number of importance weights for IWAE model training loss.\n",
    "            sigma=1.005, # The scale parameter for a kernel function\n",
    "            length_scale=params[\"length_scale\"], #The length scale parameter for a kernel function\n",
    "            kernel_scales=1, #The number of different length scales over latent space dimensions\n",
    "            window_size=24,  # Window size for the inference CNN.\n",
    "            batch_size=params[\"batch_size\"],\n",
    "            # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "            epochs=params[\"epochs\"],\n",
    "            # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "            # You can leave it to defualt as None to disable early stopping.\n",
    "            patience=3,\n",
    "            # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "            # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "            optimizer=Adam(lr=params[\"lr\"]),\n",
    "            # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "            # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "            # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "            num_workers=0,\n",
    "            # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "            # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "            device=device,\n",
    "            # set the path for saving tensorboard and trained model files \n",
    "            saving_path=\"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model\",\n",
    "            # only save the best model after training finished.\n",
    "            # You can also set it as \"better\" to save models performing better ever during training.\n",
    "            model_saving_strategy=\"best\",\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
    "        gp_vae.fit(train_set={\"X\": train_scaled}, val_set={\"X\": val_X_masked, \"X_ori\": val_X_ori})\n",
    "        gp_vae_results = gp_vae.predict({\"X\": test_X_masked}, n_sampling_times=2)\n",
    "        gp_vae_imputation = gp_vae_results[\"imputation\"]\n",
    "\n",
    "        print(f\"The shape of gp_vae_imputation is {gp_vae_imputation.shape}\")\n",
    "\n",
    "        # for error calculation, we need to take the mean value of the multiple samplings for each data sample\n",
    "        mean_gp_vae_imputation = gp_vae_imputation.mean(axis=1)\n",
    "\n",
    "        test_imputation_denorm = inverse_scale(mean_gp_vae_imputation, scalers)\n",
    "        test_ori_denorm = inverse_scale(test_X_ori, scalers)\n",
    "\n",
    "        mae_list, rmse_list = [], []\n",
    "        for i in range(n_features):\n",
    "            mask_i = test_mask[:, :, i]\n",
    "            pred = test_imputation_denorm[:, :, i][mask_i]\n",
    "            true = test_ori_denorm[:, :, i][mask_i]\n",
    "            if len(true) == 0: continue\n",
    "            mae = np.mean(np.abs(pred - true))\n",
    "            rmse = np.sqrt(mean_squared_error(true, pred))\n",
    "            mae_list.append(mae)\n",
    "            rmse_list.append(rmse)\n",
    "            mlflow.log_metric(f\"MAE_{i}\", mae)\n",
    "            mlflow.log_metric(f\"RMSE_{i}\", rmse)\n",
    "\n",
    "         # Calculate metrics\n",
    "        mae_per_feature = []\n",
    "        rmse_per_feature=[]\n",
    "        percentage_mae_per_feature = []\n",
    "\n",
    "        for i in range(n_features):\n",
    "            imputation_i = test_imputation_denorm[:, :, i]\n",
    "            ground_truth_i = test_ori_denorm[:, :, i]\n",
    "            mask_i = test_mask[:, :, i]\n",
    "            if np.isnan(imputation_i).any() or np.isnan(ground_truth_i).any():\n",
    "                continue\n",
    "            mae_i = calc_mae(imputation_i, ground_truth_i, mask_i)\n",
    "            mae_per_feature.append(mae_i)\n",
    "            rmse_i = np.sqrt(mean_squared_error(imputation_i, ground_truth_i))\n",
    "            rmse_per_feature.append(rmse)\n",
    "\n",
    "            #Calculate the original standard deviation for the feature\n",
    "            std_dev_i = np.std(ground_truth_i[mask_i == 1])\n",
    "             # Calculate the percentage of MAE relative to the standard deviation   \n",
    "            if std_dev_i != 0:\n",
    "                percentage_mae_i = (mae_i / std_dev_i) * 100\n",
    "                percentage_mae_per_feature.append(percentage_mae_i)\n",
    "            else:\n",
    "                 percentage_mae_i = float('inf')\n",
    "            \n",
    "            mlflow.log_metric(f\"MAE_{feature_names[i]}\", mae_i)\n",
    "            mlflow.log_metric(f\"RMSE_{feature_names[i]}\",rmse_i)\n",
    "            mlflow.log_metric(f\"Percentage_MAE_{feature_names[i]}\", percentage_mae_i)\n",
    "\n",
    "        avg_mae = np.mean(mae_per_feature)\n",
    "        #avg_rmse=np.mean(rmse_per_feature)\n",
    "       \n",
    "        mlflow.log_metric(\"avg_mae\", np.mean(mae_list))\n",
    "        mlflow.log_metric(\"avg_rmse\", np.mean(rmse_list))\n",
    "\n",
    "        trial.set_user_attr(\"model_state_dict\", GPVAE.state_dict())\n",
    "        trial.set_user_attr(\"mlflow_run_id\", run.info.run_id)\n",
    "\n",
    "        return avg_mae\n",
    "\n",
    "    print(\"MAE per feature:\", mae_per_feature)\n",
    "    print(\"RMSE per feature\",rmse_per_feature)\n",
    "    print(\"Percentage MAE per feature:\", percentage_mae_per_feature)\n",
    "   \n",
    "\n",
    "# Run Optuna study\n",
    "mlflow.set_experiment(\"GP-VAE-2\")\n",
    "with mlflow.start_run(run_name=\"GPVAE_Optuna_Study(2)\") as parent_run:\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "\n",
    "    best_params = study.best_trial.params\n",
    "    best_value = study.best_trial.value\n",
    "    best_run_id = study.best_trial.user_attrs[\"mlflow_run_id\"]\n",
    "\n",
    "\n",
    "    \n",
    "    # Log best parameters\n",
    "    mlflow.log_params(best_params)\n",
    "\n",
    "    # Log best metric(s)\n",
    "    mlflow.log_metric(\"best_objective_value\", best_value)\n",
    "    mlflow.log_param(\"best_run_id\", best_run_id)\n",
    "\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Best Objective Value:\", best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bd997e-8528-4087-84d9-20ded92f3486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column time has 0 NaN values\n",
      "Column time has 0.0 Missing_rate\n",
      "Column fr_eng has 0 NaN values\n",
      "Column fr_eng has 0.0 Missing_rate\n",
      "Column te_exh_cyl_out__0 has 0 NaN values\n",
      "Column te_exh_cyl_out__0 has 0.0 Missing_rate\n",
      "Column pd_air_ic__0 has 0 NaN values\n",
      "Column pd_air_ic__0 has 0.0 Missing_rate\n",
      "Column pr_exh_turb_out__0 has 316581 NaN values\n",
      "Column pr_exh_turb_out__0 has 1.0 Missing_rate\n",
      "Column te_air_ic_out__0 has 0 NaN values\n",
      "Column te_air_ic_out__0 has 0.0 Missing_rate\n",
      "Column te_seawater has 0 NaN values\n",
      "Column te_seawater has 0.0 Missing_rate\n",
      "Column te_air_comp_in_a__0 has 316581 NaN values\n",
      "Column te_air_comp_in_a__0 has 1.0 Missing_rate\n",
      "Column te_air_comp_in_b__0 has 316581 NaN values\n",
      "Column te_air_comp_in_b__0 has 1.0 Missing_rate\n",
      "Column fr_tc__0 has 316581 NaN values\n",
      "Column fr_tc__0 has 1.0 Missing_rate\n",
      "Column pr_baro has 0 NaN values\n",
      "Column pr_baro has 0.0 Missing_rate\n",
      "Column pd_air_ic__0_1 has 0 NaN values\n",
      "Column pd_air_ic__0_1 has 0.0 Missing_rate\n",
      "Column pr_exh_rec has 0 NaN values\n",
      "Column pr_exh_rec has 0.0 Missing_rate\n",
      "Column te_exh_turb_in__0 has 316581 NaN values\n",
      "Column te_exh_turb_in__0 has 1.0 Missing_rate\n",
      "Column te_exh_turb_out__0 has 316581 NaN values\n",
      "Column te_exh_turb_out__0 has 1.0 Missing_rate\n",
      "Column bo_aux_blower_running has 0 NaN values\n",
      "Column bo_aux_blower_running has 0.0 Missing_rate\n",
      "Column re_eng_load has 426 NaN values\n",
      "Column re_eng_load has 0.0013456271854596453 Missing_rate\n",
      "Column pr_air_scav_ecs has 0 NaN values\n",
      "Column pr_air_scav_ecs has 0.0 Missing_rate\n",
      "Column pr_air_scav has 0 NaN values\n",
      "Column pr_air_scav has 0.0 Missing_rate\n",
      "Column te_air_scav_rec has 0 NaN values\n",
      "Column te_air_scav_rec has 0.0 Missing_rate\n",
      "Column te_air_ic_out__0_1 has 0 NaN values\n",
      "Column te_air_ic_out__0_1 has 0.0 Missing_rate\n",
      "Column pr_cyl_comp__0 has 426 NaN values\n",
      "Column pr_cyl_comp__0 has 0.0013456271854596453 Missing_rate\n",
      "Column pr_cyl_max__0 has 426 NaN values\n",
      "Column pr_cyl_max__0 has 0.0013456271854596453 Missing_rate\n",
      "Column se_mip__0 has 426 NaN values\n",
      "Column se_mip__0 has 0.0013456271854596453 Missing_rate\n",
      "Column te_exh_cyl_out__0_1 has 0 NaN values\n",
      "Column te_exh_cyl_out__0_1 has 0.0 Missing_rate\n",
      "Column fr_eng_setpoint has 0 NaN values\n",
      "Column fr_eng_setpoint has 0.0 Missing_rate\n",
      "Column te_air_scav_rec_iso has 126055 NaN values\n",
      "Column te_air_scav_rec_iso has 0.3981761381763277 Missing_rate\n",
      "Column pr_cyl_max_mv_iso has 128557 NaN values\n",
      "Column pr_cyl_max_mv_iso has 0.40607932882895686 Missing_rate\n",
      "Column pr_cyl_comp_mv_iso has 128557 NaN values\n",
      "Column pr_cyl_comp_mv_iso has 0.40607932882895686 Missing_rate\n",
      "Column fr_eng_ecs has 0 NaN values\n",
      "Column fr_eng_ecs has 0.0 Missing_rate\n",
      "Column pr_air_scav_iso has 128557 NaN values\n",
      "Column pr_air_scav_iso has 0.40607932882895686 Missing_rate\n",
      "Column engine_type_G80ME-C9.5-GI-LPSCR has 0 NaN values\n",
      "Column engine_type_G80ME-C9.5-GI-LPSCR has 0.0 Missing_rate\n",
      "Original size:316581, Sampled size: 105527\n",
      "Reshaped data:(105527, 31)\n",
      "CUDA available: True\n",
      "Using CUDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "[I 2025-05-20 00:38:31,085] A new study created in memory with name: no-name-6b973a1c-b40c-40eb-a08e-1ca5ab5381a4\n",
      "2025-05-20 00:38:31 [INFO]: Using the given device: cuda\n",
      "2025-05-20 00:38:31 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T003831\n",
      "2025-05-20 00:38:31 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T003831/tensorboard\n",
      "2025-05-20 00:38:31 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n",
      "2025-05-20 00:38:31 [INFO]: Epoch 001 - training loss (default): 43641.4343, validation loss: 55112.3918\n",
      "2025-05-20 00:38:32 [INFO]: Epoch 002 - training loss (default): 18827.6110, validation loss: 53753.6310\n",
      "2025-05-20 00:38:33 [INFO]: Epoch 003 - training loss (default): 15589.0953, validation loss: 53205.1482\n",
      "2025-05-20 00:38:34 [INFO]: Epoch 004 - training loss (default): 15305.7711, validation loss: 52891.6227\n",
      "2025-05-20 00:38:35 [INFO]: Epoch 005 - training loss (default): 15227.5837, validation loss: 52640.9623\n",
      "2025-05-20 00:38:35 [INFO]: Epoch 006 - training loss (default): 15195.1775, validation loss: 52423.8883\n",
      "2025-05-20 00:38:36 [INFO]: Epoch 007 - training loss (default): 15178.0318, validation loss: 52239.1636\n",
      "2025-05-20 00:38:37 [INFO]: Epoch 008 - training loss (default): 15168.2211, validation loss: 52083.0248\n",
      "2025-05-20 00:38:38 [INFO]: Epoch 009 - training loss (default): 15162.1380, validation loss: 51940.5048\n",
      "2025-05-20 00:38:39 [INFO]: Epoch 010 - training loss (default): 15158.0266, validation loss: 51810.4074\n",
      "2025-05-20 00:38:39 [INFO]: Epoch 011 - training loss (default): 15154.8999, validation loss: 51685.6363\n",
      "2025-05-20 00:38:40 [INFO]: Epoch 012 - training loss (default): 15152.8748, validation loss: 51574.9676\n",
      "2025-05-20 00:38:41 [INFO]: Epoch 013 - training loss (default): 15151.0711, validation loss: 51468.6202\n",
      "2025-05-20 00:38:41 [INFO]: Finished training. The best model is from epoch#13.\n",
      "2025-05-20 00:38:41 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T003831/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-05-20 00:38:42,179] Trial 0 failed with parameters: {'lr': 0.00014138309898143063, 'epochs': 13, 'batch_size': 64, 'length_scale': 2.0986642679908574, 'beta': 0.5303956187801935, 'kernel': 'rbf'} because of the following error: NameError(\"name 'run' is not defined\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"<ipython-input-14-321dd7e3a26f>\", line 363, in objective\n",
      "    trial.set_user_attr(\"mlflow_run_id\", run.info.run_id)\n",
      "NameError: name 'run' is not defined\n",
      "[W 2025-05-20 00:38:42,181] Trial 0 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/0ab070eef2004f2992b99541db2825f4\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n",
      "🏃 View run GPVAE_Optuna_Study(2) at: http://localhost:5000/#/experiments/832352739106302318/runs/13d977d4fd4a4eddba68fcd03986c20a\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'run' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 376\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[39mwith\u001b[39;00m mlflow\u001b[39m.\u001b[39mstart_run(run_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGPVAE_Optuna_Study(2)\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m parent_run:\n\u001b[1;32m    375\u001b[0m     study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 376\u001b[0m     study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\n\u001b[1;32m    378\u001b[0m     best_params \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_trial\u001b[39m.\u001b[39mparams\n\u001b[1;32m    379\u001b[0m     best_value \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_trial\u001b[39m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     _optimize(\n\u001b[1;32m    476\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    477\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    478\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    479\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    480\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    481\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    482\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    483\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    484\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    485\u001b[0m     )\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         _optimize_sequential(\n\u001b[1;32m     64\u001b[0m             study,\n\u001b[1;32m     65\u001b[0m             func,\n\u001b[1;32m     66\u001b[0m             n_trials,\n\u001b[1;32m     67\u001b[0m             timeout,\n\u001b[1;32m     68\u001b[0m             catch,\n\u001b[1;32m     69\u001b[0m             callbacks,\n\u001b[1;32m     70\u001b[0m             gc_after_trial,\n\u001b[1;32m     71\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     72\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     73\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     74\u001b[0m         )\n\u001b[1;32m     75\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    161\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    198\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[14], line 363\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    360\u001b[0m     mlflow\u001b[39m.\u001b[39mlog_metric(\u001b[39m\"\u001b[39m\u001b[39mavg_mae\u001b[39m\u001b[39m\"\u001b[39m, np\u001b[39m.\u001b[39mmean(mae_list))\n\u001b[1;32m    361\u001b[0m     mlflow\u001b[39m.\u001b[39mlog_metric(\u001b[39m\"\u001b[39m\u001b[39mavg_rmse\u001b[39m\u001b[39m\"\u001b[39m, np\u001b[39m.\u001b[39mmean(rmse_list))\n\u001b[0;32m--> 363\u001b[0m     trial\u001b[39m.\u001b[39mset_user_attr(\u001b[39m\"\u001b[39m\u001b[39mmlflow_run_id\u001b[39m\u001b[39m\"\u001b[39m, run\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mrun_id)\n\u001b[1;32m    365\u001b[0m     \u001b[39mreturn\u001b[39;00m avg_mae\n\u001b[1;32m    367\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMAE per feature:\u001b[39m\u001b[39m\"\u001b[39m, mae_per_feature)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run' is not defined"
     ]
    }
   ],
   "source": [
    "#Import Pypots Library\n",
    "from pypots.optim import Adam\n",
    "from pypots.imputation import GPVAE\n",
    "#from pypots.utils.metrics import calc_mae\n",
    "from pypots.nn.functional import calc_mae\n",
    "\n",
    "\n",
    "import argparse\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import data_insight\n",
    "from data_insight import setup_duckdb\n",
    "from duckdb import DuckDBPyConnection as DuckDB\n",
    "from duckdb import DuckDBPyRelation as Relation\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "from duckdb import DuckDBPyConnection as DuckDB\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna \n",
    "from optuna.visualization import plot_optimization_history\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, Dataset\n",
    "from pygrinder.missing_completely_at_random import mcar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import sensor_imputation_thesis.shared.load_data as load\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "#PatchTST might be an ideal choise if SAITS is too slow \n",
    "\n",
    "##Drop columns with different indexes while loading data.. Or the mean values \n",
    "\n",
    "df=pd.read_parquet(\"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/ny_df_for_pypots.parquet\")\n",
    "\n",
    "len(df)\n",
    "\n",
    "#current length of the dataframe is 119439\n",
    "\n",
    "# Check nan values in each column\n",
    "for col in df.columns:\n",
    "    print(f\"Column {col} has {df[col].isna().sum()} NaN values\")\n",
    "    missing_rate=df[col].isna().sum()/len(df[col])\n",
    "    print(f\"Column {col} has {missing_rate} Missing_rate\")\n",
    "\n",
    "\n",
    "#Try with smaller dataset, size 4000\n",
    "##SAMPLE the percengtage of the dataset, df.sample (averagely pick samples)\n",
    "#not df.sample cuz it will randomly select \n",
    "original_size=len(df)\n",
    "desired_fraction=0.3 #Select data every 3 minutes \n",
    "step=int(1/desired_fraction) #step_size=10 (sample every 10th (3/10) minute)\n",
    "\n",
    "#Systematic sampling: Start at a random offset to avoid bias \n",
    "start=np.random.randint(0,step) #Random start between 0-9\n",
    "df1=df.iloc[start::step].reset_index(drop=True)\n",
    "\n",
    "print(f\"Original size:{len(df)}, Sampled size: {len(df1)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Custom Dataset class\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Data processing code\n",
    "sensor_cols = [col for col in df1.columns if col != \"time\"]\n",
    "data = df1[sensor_cols].values\n",
    "\n",
    "#¤get feature names for printing mae later \n",
    "feature_names=df1[sensor_cols].columns.tolist()\n",
    "\n",
    "## Convert data to 3D arrays of shape n_samples, n_timesteps, n_features, X_ori refers to the original data without missing values \n",
    "## Reconstruct all columns simultaneously  #num_features: 119\n",
    "n_features = data.shape[1]  # exclude the time column\n",
    "n_steps = 20 #60 (was 60 previously) #(TRY TO CHANGE HERE)  # # window length, 1440 steps = 24 hours of 1-minute data, but here is revised to 60 again\n",
    "#total_elements = data.shape[0] * data.shape[1]\n",
    "n_samples = data.shape[0] // n_steps \n",
    "\n",
    "\n",
    "\n",
    "# Reshape to (n_samples // n_steps, n_steps, n_features)\n",
    "#data_reshaped = data.reshape((n_samples, n_steps, n_features))\n",
    "data_reshaped=data[:n_samples*n_steps].reshape(n_samples,n_steps,n_features)\n",
    "print(f\"Reshaped data:{data.shape}\")\n",
    "\n",
    "#Split into train, test, val, fit scaler only on the train set (prevent data leakage)\n",
    "\n",
    "#train_size = int(0.6 * len(data))\n",
    "#val_size = int(0.2 * len(data))\n",
    "#test_size = len(data) - train_size - val_size\n",
    "\n",
    "#train_data = data_reshaped[:train_size]\n",
    "#val_data = data_reshaped[train_size:train_size + val_size]\n",
    "#test_data= data_reshaped[train_size + val_size:]\n",
    "\n",
    "\n",
    "#Apply time series split \n",
    "#Split into train(60%), val(20%), and test (20%)\n",
    "train_data, temp_data=train_test_split(data_reshaped,test_size=0.4,shuffle=True)\n",
    "val_data, test_data=train_test_split(temp_data, test_size=0.5, shuffle=False)\n",
    "\n",
    "##Normalization is important because of the nature of mse calculation of saits, columns with large \n",
    "#values dominate the loss, making metrics meaningless. SAITS computes MSE/MAE column-wise and averages \n",
    "#them across all columns \n",
    "#  Apply minmax scaler here \n",
    "#normalize each feature independently\n",
    "scalers={}\n",
    "\n",
    "\n",
    "#train_scaled = np.zeros_like(data_reshaped[train_size])  # Initialize the normalized data array\n",
    "#val_scaled=np.zeros_like(data_reshaped[train_size:train_size + val_size])\n",
    "#test_scaled=np.zeros_like(data_reshaped[train_size + val_size:])\n",
    "\n",
    "train_scaled = np.zeros_like(train_data)\n",
    "val_scaled = np.zeros_like(val_data)\n",
    "test_scaled = np.zeros_like(test_data)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(data_reshaped.shape[2]):\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1)) #changed to -1,1\n",
    "    # Flatten timesteps and samples for scaling\n",
    "    train_scaled[:, :, i] = scaler.fit_transform(train_data[:, :, i].reshape(-1, 1)).reshape(train_data.shape[0], train_data.shape[1])\n",
    "    val_scaled[:, :, i] = scaler.transform(val_data[:, :, i].reshape(-1, 1)).reshape(val_data.shape[0], val_data.shape[1])\n",
    "    test_scaled[:, :, i] = scaler.transform(test_data[:, :, i].reshape(-1, 1)).reshape(test_data.shape[0], test_data.shape[1])\n",
    "    scalers[i] = scaler  # Save scalers to inverse-transform later\n",
    "\n",
    "#Inverse Scale\n",
    "def inverse_scale(imputation, scalers):\n",
    "    n_samples, n_timesteps, n_features = imputation.shape\n",
    "    imputation_denorm = np.empty_like(imputation)\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        reshaped = imputation[:, :, i].reshape(-1, 1)\n",
    "        inversed = scalers[i].inverse_transform(reshaped)\n",
    "        imputation_denorm[:, :, i] = inversed.reshape(n_samples, n_timesteps)\n",
    "    \n",
    "    return imputation_denorm\n",
    "\n",
    "\n",
    "\n",
    "#Optional: Artificially mask. Mask 20% of the data (MIT part). Try masking 30% here \n",
    "def mcar_f(X, mask_ratio=0.3):\n",
    "    \"\"\"Apply MCAR only to observed values.\"\"\"\n",
    "    observed_mask=~np.isnan(X) #find observed positions\n",
    "    artificial_mask=mcar(X,mask_ratio).astype(bool) #generate MCAR mask, cast to boolean\n",
    "    #combine masks \n",
    "    combined_mask=observed_mask & artificial_mask\n",
    "\n",
    "    #Apply masking\n",
    "    X_masked=X.copy()\n",
    "    X_masked[combined_mask]=np.nan\n",
    "    return X_masked,combined_mask\n",
    "\n",
    "\n",
    "#Use mcar on validation data \n",
    "val_X_masked, val_mask =mcar_f(val_scaled)\n",
    "val_X_ori=val_scaled.copy() \n",
    "\n",
    "test_X_masked, test_mask =mcar_f(test_scaled)\n",
    "test_X_ori=test_scaled.copy() \n",
    "\n",
    "\n",
    "#?? Problem: Can't have the best input for testing\n",
    "#1.Create synthetic test_data cuz if I drop nan values for test set, there's basically nothing left\n",
    "#synthetic_data=np.random.randn(n_samples,n_steps,n_features)\n",
    "#test_X_masked,test_mask=mcar_f(synthetic_data)\n",
    "#test_X_ori=synthetic_data.copy() #Ground truth\n",
    "\n",
    "# 2, Ensure no NaN values in synthetic data\n",
    "#test_X_masked = np.nan_to_num(test_X_masked, nan=np.nanmean(test_X_masked))\n",
    "#test_X_ori = np.nan_to_num(test_X_ori, nan=np.nanmean(test_X_ori))\n",
    "\n",
    "\n",
    "\n",
    "class Config:\n",
    "    no_cuda = False\n",
    "    no_mps = False\n",
    "    seed = 1\n",
    "\n",
    "args=Config()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "use_mps = not args.no_mps and torch.backends.mps.is_available()\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "\n",
    "if args.cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA\")\n",
    "elif use_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "train_scaled = torch.tensor(train_scaled, dtype=torch.float32)\n",
    "val_X_masked = torch.tensor(val_X_masked, dtype=torch.float32)\n",
    "val_X_ori = torch.tensor(val_X_ori, dtype=torch.float32)\n",
    "\n",
    "train_scaled = train_scaled.to(device)\n",
    "val_X_masked = val_X_masked.to(device)\n",
    "val_X_ori = val_X_ori.to(device)\n",
    "\n",
    "\n",
    "#MLflow set up\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "mlflow.set_experiment(\"GP_VAE_2\")\n",
    "\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"lr\": trial.suggest_float(\"lr\", 1e-4, 1e-3, log=True),\n",
    "        \"epochs\": trial.suggest_int(\"epochs\", 10, 50),\n",
    "        \"batch_size\": trial.suggest_int(\"batch_size\", 32, 128, step=32),\n",
    "        \"length_scale\": trial.suggest_float(\"length_scale\",0.5,5.0),\n",
    "        \"beta\": trial.suggest_float(\"beta\",0.1,1.0),\n",
    "        \"kernel\":trial.suggest_categorical(\"kernel\",[\"cauchy\", \"diffusion\", \"rbf\"]),\n",
    "\n",
    "\n",
    "\n",
    "    }\n",
    "\n",
    "    with mlflow.start_run(run_name=\"GP-VAE-Trial\", nested=True):\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        gp_vae = GPVAE(\n",
    "            n_steps=data_reshaped.shape[1],\n",
    "            n_features=data_reshaped.shape[2],\n",
    "            latent_size=37, #should be the latent dimensions \n",
    "            encoder_sizes=(128,128), #should I change it here too?\n",
    "            decoder_sizes=(256,256), #should I change the model size?\n",
    "            kernel=params[\"kernel\"],\n",
    "            beta=params[\"beta\"], #The weight of KL divergence in ELBO\n",
    "            M=1,  #The number of Monte Carlo samples for ELBO estimation during training.\n",
    "            K=1,  #The number of importance weights for IWAE model training loss.\n",
    "            sigma=1.005, # The scale parameter for a kernel function\n",
    "            length_scale=params[\"length_scale\"], #The length scale parameter for a kernel function\n",
    "            kernel_scales=1, #The number of different length scales over latent space dimensions\n",
    "            window_size=24,  # Window size for the inference CNN.\n",
    "            batch_size=params[\"batch_size\"],\n",
    "            # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "            epochs=params[\"epochs\"],\n",
    "            # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "            # You can leave it to defualt as None to disable early stopping.\n",
    "            patience=3,\n",
    "            # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "            # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "            optimizer=Adam(lr=params[\"lr\"]),\n",
    "            # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "            # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "            # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "            num_workers=0,\n",
    "            # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "            # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "            device=device,\n",
    "            # set the path for saving tensorboard and trained model files \n",
    "            saving_path=\"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model\",\n",
    "            # only save the best model after training finished.\n",
    "            # You can also set it as \"better\" to save models performing better ever during training.\n",
    "            model_saving_strategy=\"best\",\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
    "        gp_vae.fit(train_set={\"X\": train_scaled}, val_set={\"X\": val_X_masked, \"X_ori\": val_X_ori})\n",
    "        gp_vae_results = gp_vae.predict({\"X\": test_X_masked}, n_sampling_times=2)\n",
    "        gp_vae_imputation = gp_vae_results[\"imputation\"]\n",
    "\n",
    "        print(f\"The shape of gp_vae_imputation is {gp_vae_imputation.shape}\")\n",
    "\n",
    "        # for error calculation, we need to take the mean value of the multiple samplings for each data sample\n",
    "        mean_gp_vae_imputation = gp_vae_imputation.mean(axis=1)\n",
    "\n",
    "        test_imputation_denorm = inverse_scale(mean_gp_vae_imputation, scalers)\n",
    "        test_ori_denorm = inverse_scale(test_X_ori, scalers)\n",
    "\n",
    "        mae_list, rmse_list = [], []\n",
    "        for i in range(n_features):\n",
    "            mask_i = test_mask[:, :, i]\n",
    "            pred = test_imputation_denorm[:, :, i][mask_i]\n",
    "            true = test_ori_denorm[:, :, i][mask_i]\n",
    "            if len(true) == 0: continue\n",
    "            mae = np.mean(np.abs(pred - true))\n",
    "            rmse = np.sqrt(mean_squared_error(true, pred))\n",
    "            mae_list.append(mae)\n",
    "            rmse_list.append(rmse)\n",
    "            mlflow.log_metric(f\"MAE_{i}\", mae)\n",
    "            mlflow.log_metric(f\"RMSE_{i}\", rmse)\n",
    "\n",
    "         # Calculate metrics\n",
    "        mae_per_feature = []\n",
    "        rmse_per_feature=[]\n",
    "        percentage_mae_per_feature = []\n",
    "\n",
    "        for i in range(n_features):\n",
    "            imputation_i = test_imputation_denorm[:, :, i]\n",
    "            ground_truth_i = test_ori_denorm[:, :, i]\n",
    "            mask_i = test_mask[:, :, i]\n",
    "            if np.isnan(imputation_i).any() or np.isnan(ground_truth_i).any():\n",
    "                continue\n",
    "            mae_i = calc_mae(imputation_i, ground_truth_i, mask_i)\n",
    "            mae_per_feature.append(mae_i)\n",
    "            rmse_i = np.sqrt(mean_squared_error(imputation_i, ground_truth_i))\n",
    "            rmse_per_feature.append(rmse)\n",
    "\n",
    "            #Calculate the original standard deviation for the feature\n",
    "            std_dev_i = np.std(ground_truth_i[mask_i == 1])\n",
    "             # Calculate the percentage of MAE relative to the standard deviation   \n",
    "            if std_dev_i != 0:\n",
    "                percentage_mae_i = (mae_i / std_dev_i) * 100\n",
    "                percentage_mae_per_feature.append(percentage_mae_i)\n",
    "            else:\n",
    "                 percentage_mae_i = float('inf')\n",
    "            \n",
    "            mlflow.log_metric(f\"MAE_{feature_names[i]}\", mae_i)\n",
    "            mlflow.log_metric(f\"RMSE_{feature_names[i]}\",rmse_i)\n",
    "            mlflow.log_metric(f\"Percentage_MAE_{feature_names[i]}\", percentage_mae_i)\n",
    "\n",
    "        avg_mae = np.mean(mae_per_feature)\n",
    "        #avg_rmse=np.mean(rmse_per_feature)\n",
    "       \n",
    "        mlflow.log_metric(\"avg_mae\", np.mean(mae_list))\n",
    "        mlflow.log_metric(\"avg_rmse\", np.mean(rmse_list))\n",
    "\n",
    "        trial.set_user_attr(\"mlflow_run_id\", run.info.run_id)\n",
    "\n",
    "        return avg_mae\n",
    "\n",
    "    print(\"MAE per feature:\", mae_per_feature)\n",
    "    print(\"RMSE per feature\",rmse_per_feature)\n",
    "    print(\"Percentage MAE per feature:\", percentage_mae_per_feature)\n",
    "   \n",
    "\n",
    "# Run Optuna study\n",
    "mlflow.set_experiment(\"GP-VAE-2\")\n",
    "with mlflow.start_run(run_name=\"GPVAE_Optuna_Study(2)\") as parent_run:\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "\n",
    "    best_params = study.best_trial.params\n",
    "    best_value = study.best_trial.value\n",
    "    best_run_id = study.best_trial.user_attrs[\"mlflow_run_id\"]\n",
    "\n",
    "\n",
    "    \n",
    "    # Log best parameters\n",
    "    mlflow.log_params(best_params)\n",
    "\n",
    "    # Log best metric(s)\n",
    "    mlflow.log_metric(\"best_objective_value\", best_value)\n",
    "    mlflow.log_param(\"best_run_id\", best_run_id)\n",
    "\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Best Objective Value:\", best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76a5e1e-dbe8-411c-a7dd-7be10b8e2082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "[I 2025-05-20 00:40:02,727] A new study created in memory with name: no-name-61fc82e3-2735-47bf-99fe-ce81fd7afe95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column time has 0 NaN values\n",
      "Column time has 0.0 Missing_rate\n",
      "Column fr_eng has 0 NaN values\n",
      "Column fr_eng has 0.0 Missing_rate\n",
      "Column te_exh_cyl_out__0 has 0 NaN values\n",
      "Column te_exh_cyl_out__0 has 0.0 Missing_rate\n",
      "Column pd_air_ic__0 has 0 NaN values\n",
      "Column pd_air_ic__0 has 0.0 Missing_rate\n",
      "Column pr_exh_turb_out__0 has 316581 NaN values\n",
      "Column pr_exh_turb_out__0 has 1.0 Missing_rate\n",
      "Column te_air_ic_out__0 has 0 NaN values\n",
      "Column te_air_ic_out__0 has 0.0 Missing_rate\n",
      "Column te_seawater has 0 NaN values\n",
      "Column te_seawater has 0.0 Missing_rate\n",
      "Column te_air_comp_in_a__0 has 316581 NaN values\n",
      "Column te_air_comp_in_a__0 has 1.0 Missing_rate\n",
      "Column te_air_comp_in_b__0 has 316581 NaN values\n",
      "Column te_air_comp_in_b__0 has 1.0 Missing_rate\n",
      "Column fr_tc__0 has 316581 NaN values\n",
      "Column fr_tc__0 has 1.0 Missing_rate\n",
      "Column pr_baro has 0 NaN values\n",
      "Column pr_baro has 0.0 Missing_rate\n",
      "Column pd_air_ic__0_1 has 0 NaN values\n",
      "Column pd_air_ic__0_1 has 0.0 Missing_rate\n",
      "Column pr_exh_rec has 0 NaN values\n",
      "Column pr_exh_rec has 0.0 Missing_rate\n",
      "Column te_exh_turb_in__0 has 316581 NaN values\n",
      "Column te_exh_turb_in__0 has 1.0 Missing_rate\n",
      "Column te_exh_turb_out__0 has 316581 NaN values\n",
      "Column te_exh_turb_out__0 has 1.0 Missing_rate\n",
      "Column bo_aux_blower_running has 0 NaN values\n",
      "Column bo_aux_blower_running has 0.0 Missing_rate\n",
      "Column re_eng_load has 426 NaN values\n",
      "Column re_eng_load has 0.0013456271854596453 Missing_rate\n",
      "Column pr_air_scav_ecs has 0 NaN values\n",
      "Column pr_air_scav_ecs has 0.0 Missing_rate\n",
      "Column pr_air_scav has 0 NaN values\n",
      "Column pr_air_scav has 0.0 Missing_rate\n",
      "Column te_air_scav_rec has 0 NaN values\n",
      "Column te_air_scav_rec has 0.0 Missing_rate\n",
      "Column te_air_ic_out__0_1 has 0 NaN values\n",
      "Column te_air_ic_out__0_1 has 0.0 Missing_rate\n",
      "Column pr_cyl_comp__0 has 426 NaN values\n",
      "Column pr_cyl_comp__0 has 0.0013456271854596453 Missing_rate\n",
      "Column pr_cyl_max__0 has 426 NaN values\n",
      "Column pr_cyl_max__0 has 0.0013456271854596453 Missing_rate\n",
      "Column se_mip__0 has 426 NaN values\n",
      "Column se_mip__0 has 0.0013456271854596453 Missing_rate\n",
      "Column te_exh_cyl_out__0_1 has 0 NaN values\n",
      "Column te_exh_cyl_out__0_1 has 0.0 Missing_rate\n",
      "Column fr_eng_setpoint has 0 NaN values\n",
      "Column fr_eng_setpoint has 0.0 Missing_rate\n",
      "Column te_air_scav_rec_iso has 126055 NaN values\n",
      "Column te_air_scav_rec_iso has 0.3981761381763277 Missing_rate\n",
      "Column pr_cyl_max_mv_iso has 128557 NaN values\n",
      "Column pr_cyl_max_mv_iso has 0.40607932882895686 Missing_rate\n",
      "Column pr_cyl_comp_mv_iso has 128557 NaN values\n",
      "Column pr_cyl_comp_mv_iso has 0.40607932882895686 Missing_rate\n",
      "Column fr_eng_ecs has 0 NaN values\n",
      "Column fr_eng_ecs has 0.0 Missing_rate\n",
      "Column pr_air_scav_iso has 128557 NaN values\n",
      "Column pr_air_scav_iso has 0.40607932882895686 Missing_rate\n",
      "Column engine_type_G80ME-C9.5-GI-LPSCR has 0 NaN values\n",
      "Column engine_type_G80ME-C9.5-GI-LPSCR has 0.0 Missing_rate\n",
      "Original size:316581, Sampled size: 105527\n",
      "Reshaped data:(105527, 31)\n",
      "CUDA available: True\n",
      "Using CUDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 00:40:02 [INFO]: Using the given device: cuda\n",
      "2025-05-20 00:40:02 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004002\n",
      "2025-05-20 00:40:02 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004002/tensorboard\n",
      "2025-05-20 00:40:02 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n",
      "2025-05-20 00:40:03 [INFO]: Epoch 001 - training loss (default): 722.8419, validation loss: 354.8317\n",
      "2025-05-20 00:40:04 [INFO]: Epoch 002 - training loss (default): 565.1318, validation loss: 343.4969\n",
      "2025-05-20 00:40:04 [INFO]: Epoch 003 - training loss (default): 552.5871, validation loss: 340.6999\n",
      "2025-05-20 00:40:05 [INFO]: Epoch 004 - training loss (default): 550.3879, validation loss: 339.4278\n",
      "2025-05-20 00:40:06 [INFO]: Epoch 005 - training loss (default): 549.4127, validation loss: 338.4612\n",
      "2025-05-20 00:40:06 [INFO]: Epoch 006 - training loss (default): 548.6879, validation loss: 337.6381\n",
      "2025-05-20 00:40:07 [INFO]: Epoch 007 - training loss (default): 548.2508, validation loss: 336.8501\n",
      "2025-05-20 00:40:07 [INFO]: Epoch 008 - training loss (default): 547.9204, validation loss: 336.1405\n",
      "2025-05-20 00:40:08 [INFO]: Epoch 009 - training loss (default): 547.6309, validation loss: 335.5385\n",
      "2025-05-20 00:40:09 [INFO]: Epoch 010 - training loss (default): 547.3990, validation loss: 334.9457\n",
      "2025-05-20 00:40:09 [INFO]: Epoch 011 - training loss (default): 547.1049, validation loss: 334.4384\n",
      "2025-05-20 00:40:10 [INFO]: Epoch 012 - training loss (default): 546.7890, validation loss: 333.9422\n",
      "2025-05-20 00:40:11 [INFO]: Epoch 013 - training loss (default): 546.7314, validation loss: 333.5182\n",
      "2025-05-20 00:40:11 [INFO]: Epoch 014 - training loss (default): 546.4590, validation loss: 333.1352\n",
      "2025-05-20 00:40:12 [INFO]: Epoch 015 - training loss (default): 546.3201, validation loss: 332.7303\n",
      "2025-05-20 00:40:13 [INFO]: Epoch 016 - training loss (default): 546.1738, validation loss: 332.4476\n",
      "2025-05-20 00:40:13 [INFO]: Finished training. The best model is from epoch#16.\n",
      "2025-05-20 00:40:13 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004002/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-20 00:40:13,677] Trial 0 finished with value: 7675.04213732518 and parameters: {'lr': 0.00048532276594338386, 'epochs': 16, 'batch_size': 128, 'length_scale': 1.6539034791803948, 'beta': 0.38234690972619034, 'kernel': 'cauchy'}. Best is trial 0 with value: 7675.04213732518.\n",
      "2025-05-20 00:40:13 [INFO]: Using the given device: cuda\n",
      "2025-05-20 00:40:13 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004013\n",
      "2025-05-20 00:40:13 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004013/tensorboard\n",
      "2025-05-20 00:40:13 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n",
      "2025-05-20 00:40:13 [ERROR]: ❌ Exception: Expected parameter covariance_matrix (Tensor of shape (37, 20, 20)) of distribution MultivariateNormal(loc: torch.Size([37, 20]), covariance_matrix: torch.Size([37, 20, 20])) to satisfy the constraint PositiveDefinite(), but found invalid values:\n",
      "tensor([[[1.0000e+00, 9.4433e-01, 7.9524e-01,  ..., 6.4707e-08,\n",
      "          8.7158e-09, 1.0469e-09],\n",
      "         [9.4433e-01, 1.0000e+00, 9.4433e-01,  ..., 4.2840e-07,\n",
      "          6.4707e-08, 8.7158e-09],\n",
      "         [7.9524e-01, 9.4433e-01, 1.0000e+00,  ..., 2.5292e-06,\n",
      "          4.2840e-07, 6.4707e-08],\n",
      "         ...,\n",
      "         [6.4707e-08, 4.2840e-07, 2.5292e-06,  ..., 1.0000e+00,\n",
      "          9.4433e-01, 7.9524e-01],\n",
      "         [8.7158e-09, 6.4707e-08, 4.2840e-07,  ..., 9.4433e-01,\n",
      "          1.0000e+00, 9.4433e-01],\n",
      "         [1.0469e-09, 8.7158e-09, 6.4707e-08,  ..., 7.9524e-01,\n",
      "          9.4433e-01, 1.0000e+00]],\n",
      "\n",
      "        [[1.0000e+00, 9.4433e-01, 7.9524e-01,  ..., 6.4707e-08,\n",
      "          8.7158e-09, 1.0469e-09],\n",
      "         [9.4433e-01, 1.0000e+00, 9.4433e-01,  ..., 4.2840e-07,\n",
      "          6.4707e-08, 8.7158e-09],\n",
      "         [7.9524e-01, 9.4433e-01, 1.0000e+00,  ..., 2.5292e-06,\n",
      "          4.2840e-07, 6.4707e-08],\n",
      "         ...,\n",
      "         [6.4707e-08, 4.2840e-07, 2.5292e-06,  ..., 1.0000e+00,\n",
      "          9.4433e-01, 7.9524e-01],\n",
      "         [8.7158e-09, 6.4707e-08, 4.2840e-07,  ..., 9.4433e-01,\n",
      "          1.0000e+00, 9.4433e-01],\n",
      "         [1.0469e-09, 8.7158e-09, 6.4707e-08,  ..., 7.9524e-01,\n",
      "          9.4433e-01, 1.0000e+00]],\n",
      "\n",
      "        [[1.0000e+00, 9.4433e-01, 7.9524e-01,  ..., 6.4707e-08,\n",
      "          8.7158e-09, 1.0469e-09],\n",
      "         [9.4433e-01, 1.0000e+00, 9.4433e-01,  ..., 4.2840e-07,\n",
      "          6.4707e-08, 8.7158e-09],\n",
      "         [7.9524e-01, 9.4433e-01, 1.0000e+00,  ..., 2.5292e-06,\n",
      "          4.2840e-07, 6.4707e-08],\n",
      "         ...,\n",
      "         [6.4707e-08, 4.2840e-07, 2.5292e-06,  ..., 1.0000e+00,\n",
      "          9.4433e-01, 7.9524e-01],\n",
      "         [8.7158e-09, 6.4707e-08, 4.2840e-07,  ..., 9.4433e-01,\n",
      "          1.0000e+00, 9.4433e-01],\n",
      "         [1.0469e-09, 8.7158e-09, 6.4707e-08,  ..., 7.9524e-01,\n",
      "          9.4433e-01, 1.0000e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1.0000e+00, 9.4433e-01, 7.9524e-01,  ..., 6.4707e-08,\n",
      "          8.7158e-09, 1.0469e-09],\n",
      "         [9.4433e-01, 1.0000e+00, 9.4433e-01,  ..., 4.2840e-07,\n",
      "          6.4707e-08, 8.7158e-09],\n",
      "         [7.9524e-01, 9.4433e-01, 1.0000e+00,  ..., 2.5292e-06,\n",
      "          4.2840e-07, 6.4707e-08],\n",
      "         ...,\n",
      "         [6.4707e-08, 4.2840e-07, 2.5292e-06,  ..., 1.0000e+00,\n",
      "          9.4433e-01, 7.9524e-01],\n",
      "         [8.7158e-09, 6.4707e-08, 4.2840e-07,  ..., 9.4433e-01,\n",
      "          1.0000e+00, 9.4433e-01],\n",
      "         [1.0469e-09, 8.7158e-09, 6.4707e-08,  ..., 7.9524e-01,\n",
      "          9.4433e-01, 1.0000e+00]],\n",
      "\n",
      "        [[1.0000e+00, 9.4433e-01, 7.9524e-01,  ..., 6.4707e-08,\n",
      "          8.7158e-09, 1.0469e-09],\n",
      "         [9.4433e-01, 1.0000e+00, 9.4433e-01,  ..., 4.2840e-07,\n",
      "          6.4707e-08, 8.7158e-09],\n",
      "         [7.9524e-01, 9.4433e-01, 1.0000e+00,  ..., 2.5292e-06,\n",
      "          4.2840e-07, 6.4707e-08],\n",
      "         ...,\n",
      "         [6.4707e-08, 4.2840e-07, 2.5292e-06,  ..., 1.0000e+00,\n",
      "          9.4433e-01, 7.9524e-01],\n",
      "         [8.7158e-09, 6.4707e-08, 4.2840e-07,  ..., 9.4433e-01,\n",
      "          1.0000e+00, 9.4433e-01],\n",
      "         [1.0469e-09, 8.7158e-09, 6.4707e-08,  ..., 7.9524e-01,\n",
      "          9.4433e-01, 1.0000e+00]],\n",
      "\n",
      "        [[1.0000e+00, 9.4433e-01, 7.9524e-01,  ..., 6.4707e-08,\n",
      "          8.7158e-09, 1.0469e-09],\n",
      "         [9.4433e-01, 1.0000e+00, 9.4433e-01,  ..., 4.2840e-07,\n",
      "          6.4707e-08, 8.7158e-09],\n",
      "         [7.9524e-01, 9.4433e-01, 1.0000e+00,  ..., 2.5292e-06,\n",
      "          4.2840e-07, 6.4707e-08],\n",
      "         ...,\n",
      "         [6.4707e-08, 4.2840e-07, 2.5292e-06,  ..., 1.0000e+00,\n",
      "          9.4433e-01, 7.9524e-01],\n",
      "         [8.7158e-09, 6.4707e-08, 4.2840e-07,  ..., 9.4433e-01,\n",
      "          1.0000e+00, 9.4433e-01],\n",
      "         [1.0469e-09, 8.7158e-09, 6.4707e-08,  ..., 7.9524e-01,\n",
      "          9.4433e-01, 1.0000e+00]]], device='cuda:0')\n",
      "[W 2025-05-20 00:40:13,774] Trial 1 failed with parameters: {'lr': 0.0007112014309759345, 'epochs': 19, 'batch_size': 128, 'length_scale': 4.178355661702597, 'beta': 0.13901619801508014, 'kernel': 'rbf'} because of the following error: RuntimeError('Training got interrupted. Model was not trained. Please investigate the error printed above.').\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/pypots/base.py\", line 737, in _train_model\n",
      "    results = self.model(inputs, calc_criterion=True)\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/pypots/imputation/gpvae/core.py\", line 101, in forward\n",
      "    elbo_loss = self.backbone(X, missing_mask)\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/pypots/nn/modules/gpvae/backbone.py\", line 160, in forward\n",
      "    self.prior = self._init_prior(device=X.device)\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/pypots/nn/modules/gpvae/backbone.py\", line 137, in _init_prior\n",
      "    prior = torch.distributions.MultivariateNormal(\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/torch/distributions/multivariate_normal.py\", line 180, in __init__\n",
      "    super().__init__(batch_shape, event_shape, validate_args=validate_args)\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/torch/distributions/distribution.py\", line 71, in __init__\n",
      "    raise ValueError(\n",
      "ValueError: Expected parameter covariance_matrix (Tensor of shape (37, 20, 20)) of distribution MultivariateNormal(loc: torch.Size([37, 20]), covariance_matrix: torch.Size([37, 20, 20])) to satisfy the constraint PositiveDefinite(), but found invalid values:\n",
      "tensor([[[1.0000e+00, 9.4433e-01, 7.9524e-01,  ..., 6.4707e-08,\n",
      "          8.7158e-09, 1.0469e-09],\n",
      "         [9.4433e-01, 1.0000e+00, 9.4433e-01,  ..., 4.2840e-07,\n",
      "          6.4707e-08, 8.7158e-09],\n",
      "         [7.9524e-01, 9.4433e-01, 1.0000e+00,  ..., 2.5292e-06,\n",
      "          4.2840e-07, 6.4707e-08],\n",
      "         ...,\n",
      "         [6.4707e-08, 4.2840e-07, 2.5292e-06,  ..., 1.0000e+00,\n",
      "          9.4433e-01, 7.9524e-01],\n",
      "         [8.7158e-09, 6.4707e-08, 4.2840e-07,  ..., 9.4433e-01,\n",
      "          1.0000e+00, 9.4433e-01],\n",
      "         [1.0469e-09, 8.7158e-09, 6.4707e-08,  ..., 7.9524e-01,\n",
      "          9.4433e-01, 1.0000e+00]],\n",
      "\n",
      "        [[1.0000e+00, 9.4433e-01, 7.9524e-01,  ..., 6.4707e-08,\n",
      "          8.7158e-09, 1.0469e-09],\n",
      "         [9.4433e-01, 1.0000e+00, 9.4433e-01,  ..., 4.2840e-07,\n",
      "          6.4707e-08, 8.7158e-09],\n",
      "         [7.9524e-01, 9.4433e-01, 1.0000e+00,  ..., 2.5292e-06,\n",
      "          4.2840e-07, 6.4707e-08],\n",
      "         ...,\n",
      "         [6.4707e-08, 4.2840e-07, 2.5292e-06,  ..., 1.0000e+00,\n",
      "          9.4433e-01, 7.9524e-01],\n",
      "         [8.7158e-09, 6.4707e-08, 4.2840e-07,  ..., 9.4433e-01,\n",
      "          1.0000e+00, 9.4433e-01],\n",
      "         [1.0469e-09, 8.7158e-09, 6.4707e-08,  ..., 7.9524e-01,\n",
      "          9.4433e-01, 1.0000e+00]],\n",
      "\n",
      "        [[1.0000e+00, 9.4433e-01, 7.9524e-01,  ..., 6.4707e-08,\n",
      "          8.7158e-09, 1.0469e-09],\n",
      "         [9.4433e-01, 1.0000e+00, 9.4433e-01,  ..., 4.2840e-07,\n",
      "          6.4707e-08, 8.7158e-09],\n",
      "         [7.9524e-01, 9.4433e-01, 1.0000e+00,  ..., 2.5292e-06,\n",
      "          4.2840e-07, 6.4707e-08],\n",
      "         ...,\n",
      "         [6.4707e-08, 4.2840e-07, 2.5292e-06,  ..., 1.0000e+00,\n",
      "          9.4433e-01, 7.9524e-01],\n",
      "         [8.7158e-09, 6.4707e-08, 4.2840e-07,  ..., 9.4433e-01,\n",
      "          1.0000e+00, 9.4433e-01],\n",
      "         [1.0469e-09, 8.7158e-09, 6.4707e-08,  ..., 7.9524e-01,\n",
      "          9.4433e-01, 1.0000e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1.0000e+00, 9.4433e-01, 7.9524e-01,  ..., 6.4707e-08,\n",
      "          8.7158e-09, 1.0469e-09],\n",
      "         [9.4433e-01, 1.0000e+00, 9.4433e-01,  ..., 4.2840e-07,\n",
      "          6.4707e-08, 8.7158e-09],\n",
      "         [7.9524e-01, 9.4433e-01, 1.0000e+00,  ..., 2.5292e-06,\n",
      "          4.2840e-07, 6.4707e-08],\n",
      "         ...,\n",
      "         [6.4707e-08, 4.2840e-07, 2.5292e-06,  ..., 1.0000e+00,\n",
      "          9.4433e-01, 7.9524e-01],\n",
      "         [8.7158e-09, 6.4707e-08, 4.2840e-07,  ..., 9.4433e-01,\n",
      "          1.0000e+00, 9.4433e-01],\n",
      "         [1.0469e-09, 8.7158e-09, 6.4707e-08,  ..., 7.9524e-01,\n",
      "          9.4433e-01, 1.0000e+00]],\n",
      "\n",
      "        [[1.0000e+00, 9.4433e-01, 7.9524e-01,  ..., 6.4707e-08,\n",
      "          8.7158e-09, 1.0469e-09],\n",
      "         [9.4433e-01, 1.0000e+00, 9.4433e-01,  ..., 4.2840e-07,\n",
      "          6.4707e-08, 8.7158e-09],\n",
      "         [7.9524e-01, 9.4433e-01, 1.0000e+00,  ..., 2.5292e-06,\n",
      "          4.2840e-07, 6.4707e-08],\n",
      "         ...,\n",
      "         [6.4707e-08, 4.2840e-07, 2.5292e-06,  ..., 1.0000e+00,\n",
      "          9.4433e-01, 7.9524e-01],\n",
      "         [8.7158e-09, 6.4707e-08, 4.2840e-07,  ..., 9.4433e-01,\n",
      "          1.0000e+00, 9.4433e-01],\n",
      "         [1.0469e-09, 8.7158e-09, 6.4707e-08,  ..., 7.9524e-01,\n",
      "          9.4433e-01, 1.0000e+00]],\n",
      "\n",
      "        [[1.0000e+00, 9.4433e-01, 7.9524e-01,  ..., 6.4707e-08,\n",
      "          8.7158e-09, 1.0469e-09],\n",
      "         [9.4433e-01, 1.0000e+00, 9.4433e-01,  ..., 4.2840e-07,\n",
      "          6.4707e-08, 8.7158e-09],\n",
      "         [7.9524e-01, 9.4433e-01, 1.0000e+00,  ..., 2.5292e-06,\n",
      "          4.2840e-07, 6.4707e-08],\n",
      "         ...,\n",
      "         [6.4707e-08, 4.2840e-07, 2.5292e-06,  ..., 1.0000e+00,\n",
      "          9.4433e-01, 7.9524e-01],\n",
      "         [8.7158e-09, 6.4707e-08, 4.2840e-07,  ..., 9.4433e-01,\n",
      "          1.0000e+00, 9.4433e-01],\n",
      "         [1.0469e-09, 8.7158e-09, 6.4707e-08,  ..., 7.9524e-01,\n",
      "          9.4433e-01, 1.0000e+00]]], device='cuda:0')\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"<ipython-input-15-be148be5d8fe>\", line 300, in objective\n",
      "    gp_vae.fit(train_set={\"X\": train_scaled}, val_set={\"X\": val_X_masked, \"X_ori\": val_X_ori})\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/pypots/imputation/gpvae/model.py\", line 269, in fit\n",
      "    self._train_model(train_dataloader, val_dataloader)\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/pypots/base.py\", line 814, in _train_model\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.\n",
      "[W 2025-05-20 00:40:13,776] Trial 1 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/f007d5e783e64b42a7bd6742a6301f95\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n",
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/2d67f3f8141842a58203908cd6ec7f57\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n",
      "🏃 View run GPVAE_Optuna_Study(2) at: http://localhost:5000/#/experiments/832352739106302318/runs/c81e5b54f26545c38c34f7e47d9768cd\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Training got interrupted. Model was not trained. Please investigate the error printed above.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/pypots/base.py:737\u001b[0m, in \u001b[0;36mBaseNNModel._train_model\u001b[0;34m(self, train_dataloader, val_dataloader)\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> 737\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(inputs, calc_criterion\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    738\u001b[0m loss \u001b[39m=\u001b[39m results[\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39msum()\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/pypots/imputation/gpvae/core.py:101\u001b[0m, in \u001b[0;36m_GPVAE.forward\u001b[0;34m(self, inputs, calc_criterion, n_sampling_times)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39mif\u001b[39;00m calc_criterion:\n\u001b[0;32m--> 101\u001b[0m     elbo_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone(X, missing_mask)\n\u001b[1;32m    102\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:  \u001b[39m# if in the training mode (the training stage), return loss result from training_loss\u001b[39;00m\n\u001b[1;32m    103\u001b[0m         \u001b[39m# `loss` is always the item for backward propagating to update the model\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/pypots/nn/modules/gpvae/backbone.py:160\u001b[0m, in \u001b[0;36mBackboneGPVAE.forward\u001b[0;34m(self, X, missing_mask)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprior \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprior \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init_prior(device\u001b[39m=\u001b[39;49mX\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    162\u001b[0m qz_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode(X)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/pypots/nn/modules/gpvae/backbone.py:137\u001b[0m, in \u001b[0;36mBackboneGPVAE._init_prior\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(kernel_matrix_tiled) \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlatent_dim\n\u001b[0;32m--> 137\u001b[0m prior \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mdistributions\u001b[39m.\u001b[39;49mMultivariateNormal(\n\u001b[1;32m    138\u001b[0m     loc\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mzeros(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlatent_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtime_length, device\u001b[39m=\u001b[39;49mdevice),\n\u001b[1;32m    139\u001b[0m     covariance_matrix\u001b[39m=\u001b[39;49mkernel_matrix_tiled\u001b[39m.\u001b[39;49mto(device),\n\u001b[1;32m    140\u001b[0m )\n\u001b[1;32m    141\u001b[0m \u001b[39mreturn\u001b[39;00m prior\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/torch/distributions/multivariate_normal.py:180\u001b[0m, in \u001b[0;36mMultivariateNormal.__init__\u001b[0;34m(self, loc, covariance_matrix, precision_matrix, scale_tril, validate_args)\u001b[0m\n\u001b[1;32m    179\u001b[0m event_shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloc\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 180\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(batch_shape, event_shape, validate_args\u001b[39m=\u001b[39;49mvalidate_args)\n\u001b[1;32m    182\u001b[0m \u001b[39mif\u001b[39;00m scale_tril \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/torch/distributions/distribution.py:71\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m valid\u001b[39m.\u001b[39mall():\n\u001b[0;32m---> 71\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     72\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected parameter \u001b[39m\u001b[39m{\u001b[39;00mparam\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(value)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m of shape \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(value\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mof distribution \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mto satisfy the constraint \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(constraint)\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut found invalid values:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     77\u001b[0m             )\n\u001b[1;32m     78\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter covariance_matrix (Tensor of shape (37, 20, 20)) of distribution MultivariateNormal(loc: torch.Size([37, 20]), covariance_matrix: torch.Size([37, 20, 20])) to satisfy the constraint PositiveDefinite(), but found invalid values:\ntensor([[[1.0000e+00, 9.4433e-01, 7.9524e-01,  ..., 6.4707e-08,\n          8.7158e-09, 1.0469e-09],\n         [9.4433e-01, 1.0000e+00, 9.4433e-01,  ..., 4.2840e-07,\n          6.4707e-08, 8.7158e-09],\n         [7.9524e-01, 9.4433e-01, 1.0000e+00,  ..., 2.5292e-06,\n          4.2840e-07, 6.4707e-08],\n         ...,\n         [6.4707e-08, 4.2840e-07, 2.5292e-06,  ..., 1.0000e+00,\n          9.4433e-01, 7.9524e-01],\n         [8.7158e-09, 6.4707e-08, 4.2840e-07,  ..., 9.4433e-01,\n          1.0000e+00, 9.4433e-01],\n         [1.0469e-09, 8.7158e-09, 6.4707e-08,  ..., 7.9524e-01,\n          9.4433e-01, 1.0000e+00]],\n\n        [[1.0000e+00, 9.4433e-01, 7.9524e-01,  ..., 6.4707e-08,\n          8.7158e-09, 1.0469e-09],\n         [9.4433e-01, 1.0000e+00, 9.4433e-01,  ..., 4.2840e-07,\n          6.4707e-08, 8.7158e-09],\n         [7.9524e-01, 9.4433e-01, 1.0000e+00,  ..., 2.5292e-06,\n          4.2840e-07, 6.4707e-08],\n         ...,\n         [6.4707e-08, 4.2840e-07, 2.5292e-06,  ..., 1.0000e+00,\n          9.4433e-01, 7.9524e-01],\n         [8.7158e-09, 6.4707e-08, 4.2840e-07,  ..., 9.4433e-01,\n          1.0000e+00, 9.4433e-01],\n         [1.0469e-09, 8.7158e-09, 6.4707e-08,  ..., 7.9524e-01,\n          9.4433e-01, 1.0000e+00]],\n\n        [[1.0000e+00, 9.4433e-01, 7.9524e-01,  ..., 6.4707e-08,\n          8.7158e-09, 1.0469e-09],\n         [9.4433e-01, 1.0000e+00, 9.4433e-01,  ..., 4.2840e-07,\n          6.4707e-08, 8.7158e-09],\n         [7.9524e-01, 9.4433e-01, 1.0000e+00,  ..., 2.5292e-06,\n          4.2840e-07, 6.4707e-08],\n         ...,\n         [6.4707e-08, 4.2840e-07, 2.5292e-06,  ..., 1.0000e+00,\n          9.4433e-01, 7.9524e-01],\n         [8.7158e-09, 6.4707e-08, 4.2840e-07,  ..., 9.4433e-01,\n          1.0000e+00, 9.4433e-01],\n         [1.0469e-09, 8.7158e-09, 6.4707e-08,  ..., 7.9524e-01,\n          9.4433e-01, 1.0000e+00]],\n\n        ...,\n\n        [[1.0000e+00, 9.4433e-01, 7.9524e-01,  ..., 6.4707e-08,\n          8.7158e-09, 1.0469e-09],\n         [9.4433e-01, 1.0000e+00, 9.4433e-01,  ..., 4.2840e-07,\n          6.4707e-08, 8.7158e-09],\n         [7.9524e-01, 9.4433e-01, 1.0000e+00,  ..., 2.5292e-06,\n          4.2840e-07, 6.4707e-08],\n         ...,\n         [6.4707e-08, 4.2840e-07, 2.5292e-06,  ..., 1.0000e+00,\n          9.4433e-01, 7.9524e-01],\n         [8.7158e-09, 6.4707e-08, 4.2840e-07,  ..., 9.4433e-01,\n          1.0000e+00, 9.4433e-01],\n         [1.0469e-09, 8.7158e-09, 6.4707e-08,  ..., 7.9524e-01,\n          9.4433e-01, 1.0000e+00]],\n\n        [[1.0000e+00, 9.4433e-01, 7.9524e-01,  ..., 6.4707e-08,\n          8.7158e-09, 1.0469e-09],\n         [9.4433e-01, 1.0000e+00, 9.4433e-01,  ..., 4.2840e-07,\n          6.4707e-08, 8.7158e-09],\n         [7.9524e-01, 9.4433e-01, 1.0000e+00,  ..., 2.5292e-06,\n          4.2840e-07, 6.4707e-08],\n         ...,\n         [6.4707e-08, 4.2840e-07, 2.5292e-06,  ..., 1.0000e+00,\n          9.4433e-01, 7.9524e-01],\n         [8.7158e-09, 6.4707e-08, 4.2840e-07,  ..., 9.4433e-01,\n          1.0000e+00, 9.4433e-01],\n         [1.0469e-09, 8.7158e-09, 6.4707e-08,  ..., 7.9524e-01,\n          9.4433e-01, 1.0000e+00]],\n\n        [[1.0000e+00, 9.4433e-01, 7.9524e-01,  ..., 6.4707e-08,\n          8.7158e-09, 1.0469e-09],\n         [9.4433e-01, 1.0000e+00, 9.4433e-01,  ..., 4.2840e-07,\n          6.4707e-08, 8.7158e-09],\n         [7.9524e-01, 9.4433e-01, 1.0000e+00,  ..., 2.5292e-06,\n          4.2840e-07, 6.4707e-08],\n         ...,\n         [6.4707e-08, 4.2840e-07, 2.5292e-06,  ..., 1.0000e+00,\n          9.4433e-01, 7.9524e-01],\n         [8.7158e-09, 6.4707e-08, 4.2840e-07,  ..., 9.4433e-01,\n          1.0000e+00, 9.4433e-01],\n         [1.0469e-09, 8.7158e-09, 6.4707e-08,  ..., 7.9524e-01,\n          9.4433e-01, 1.0000e+00]]], device='cuda:0')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 373\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[39mwith\u001b[39;00m mlflow\u001b[39m.\u001b[39mstart_run(run_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGPVAE_Optuna_Study(2)\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m parent_run:\n\u001b[1;32m    372\u001b[0m     study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 373\u001b[0m     study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\n\u001b[1;32m    375\u001b[0m     best_params \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_trial\u001b[39m.\u001b[39mparams\n\u001b[1;32m    376\u001b[0m     best_value \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_trial\u001b[39m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     _optimize(\n\u001b[1;32m    476\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    477\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    478\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    479\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    480\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    481\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    482\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    483\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    484\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    485\u001b[0m     )\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         _optimize_sequential(\n\u001b[1;32m     64\u001b[0m             study,\n\u001b[1;32m     65\u001b[0m             func,\n\u001b[1;32m     66\u001b[0m             n_trials,\n\u001b[1;32m     67\u001b[0m             timeout,\n\u001b[1;32m     68\u001b[0m             catch,\n\u001b[1;32m     69\u001b[0m             callbacks,\n\u001b[1;32m     70\u001b[0m             gc_after_trial,\n\u001b[1;32m     71\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     72\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     73\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     74\u001b[0m         )\n\u001b[1;32m     75\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    161\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    198\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[15], line 300\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    259\u001b[0m gp_vae \u001b[39m=\u001b[39m GPVAE(\n\u001b[1;32m    260\u001b[0m     n_steps\u001b[39m=\u001b[39mdata_reshaped\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m],\n\u001b[1;32m    261\u001b[0m     n_features\u001b[39m=\u001b[39mdata_reshaped\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    293\u001b[0m     model_saving_strategy\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbest\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    294\u001b[0m )\n\u001b[1;32m    299\u001b[0m \u001b[39m# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m gp_vae\u001b[39m.\u001b[39;49mfit(train_set\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m: train_scaled}, val_set\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m: val_X_masked, \u001b[39m\"\u001b[39;49m\u001b[39mX_ori\u001b[39;49m\u001b[39m\"\u001b[39;49m: val_X_ori})\n\u001b[1;32m    301\u001b[0m gp_vae_results \u001b[39m=\u001b[39m gp_vae\u001b[39m.\u001b[39mpredict({\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m: test_X_masked}, n_sampling_times\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    302\u001b[0m gp_vae_imputation \u001b[39m=\u001b[39m gp_vae_results[\u001b[39m\"\u001b[39m\u001b[39mimputation\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/pypots/imputation/gpvae/model.py:269\u001b[0m, in \u001b[0;36mGPVAE.fit\u001b[0;34m(self, train_set, val_set, file_type)\u001b[0m\n\u001b[1;32m    261\u001b[0m     val_dataloader \u001b[39m=\u001b[39m DataLoader(\n\u001b[1;32m    262\u001b[0m         val_dataset,\n\u001b[1;32m    263\u001b[0m         batch_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size,\n\u001b[1;32m    264\u001b[0m         shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    265\u001b[0m         num_workers\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_workers,\n\u001b[1;32m    266\u001b[0m     )\n\u001b[1;32m    268\u001b[0m \u001b[39m# Step 2: train the model and freeze it\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_model(train_dataloader, val_dataloader)\n\u001b[1;32m    270\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mload_state_dict(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_model_dict)\n\u001b[1;32m    272\u001b[0m \u001b[39m# Step 3: save the model if necessary\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/pypots/base.py:814\u001b[0m, in \u001b[0;36mBaseNNModel._train_model\u001b[0;34m(self, train_dataloader, val_dataloader)\u001b[0m\n\u001b[1;32m    812\u001b[0m logger\u001b[39m.\u001b[39merror(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m❌ Exception: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_model_dict \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# if no best model, raise error\u001b[39;00m\n\u001b[0;32m--> 814\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    815\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTraining got interrupted. Model was not trained. Please investigate the error printed above.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    816\u001b[0m     )\n\u001b[1;32m    817\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    818\u001b[0m     \u001b[39mRuntimeWarning\u001b[39;00m(\n\u001b[1;32m    819\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTraining got interrupted. Please investigate the error printed above.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    820\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mModel got trained and will load the best checkpoint so far for testing.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    821\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf you don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt want it, please try fit() again.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    822\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Training got interrupted. Model was not trained. Please investigate the error printed above."
     ]
    }
   ],
   "source": [
    "#Import Pypots Library\n",
    "from pypots.optim import Adam\n",
    "from pypots.imputation import GPVAE\n",
    "#from pypots.utils.metrics import calc_mae\n",
    "from pypots.nn.functional import calc_mae\n",
    "\n",
    "\n",
    "import argparse\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import data_insight\n",
    "from data_insight import setup_duckdb\n",
    "from duckdb import DuckDBPyConnection as DuckDB\n",
    "from duckdb import DuckDBPyRelation as Relation\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "from duckdb import DuckDBPyConnection as DuckDB\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna \n",
    "from optuna.visualization import plot_optimization_history\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, Dataset\n",
    "from pygrinder.missing_completely_at_random import mcar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import sensor_imputation_thesis.shared.load_data as load\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "#PatchTST might be an ideal choise if SAITS is too slow \n",
    "\n",
    "##Drop columns with different indexes while loading data.. Or the mean values \n",
    "\n",
    "df=pd.read_parquet(\"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/ny_df_for_pypots.parquet\")\n",
    "\n",
    "len(df)\n",
    "\n",
    "#current length of the dataframe is 119439\n",
    "\n",
    "# Check nan values in each column\n",
    "for col in df.columns:\n",
    "    print(f\"Column {col} has {df[col].isna().sum()} NaN values\")\n",
    "    missing_rate=df[col].isna().sum()/len(df[col])\n",
    "    print(f\"Column {col} has {missing_rate} Missing_rate\")\n",
    "\n",
    "\n",
    "#Try with smaller dataset, size 4000\n",
    "##SAMPLE the percengtage of the dataset, df.sample (averagely pick samples)\n",
    "#not df.sample cuz it will randomly select \n",
    "original_size=len(df)\n",
    "desired_fraction=0.3 #Select data every 3 minutes \n",
    "step=int(1/desired_fraction) #step_size=10 (sample every 10th (3/10) minute)\n",
    "\n",
    "#Systematic sampling: Start at a random offset to avoid bias \n",
    "start=np.random.randint(0,step) #Random start between 0-9\n",
    "df1=df.iloc[start::step].reset_index(drop=True)\n",
    "\n",
    "print(f\"Original size:{len(df)}, Sampled size: {len(df1)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Custom Dataset class\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Data processing code\n",
    "sensor_cols = [col for col in df1.columns if col != \"time\"]\n",
    "data = df1[sensor_cols].values\n",
    "\n",
    "#¤get feature names for printing mae later \n",
    "feature_names=df1[sensor_cols].columns.tolist()\n",
    "\n",
    "## Convert data to 3D arrays of shape n_samples, n_timesteps, n_features, X_ori refers to the original data without missing values \n",
    "## Reconstruct all columns simultaneously  #num_features: 119\n",
    "n_features = data.shape[1]  # exclude the time column\n",
    "n_steps = 20 #60 (was 60 previously) #(TRY TO CHANGE HERE)  # # window length, 1440 steps = 24 hours of 1-minute data, but here is revised to 60 again\n",
    "#total_elements = data.shape[0] * data.shape[1]\n",
    "n_samples = data.shape[0] // n_steps \n",
    "\n",
    "\n",
    "\n",
    "# Reshape to (n_samples // n_steps, n_steps, n_features)\n",
    "#data_reshaped = data.reshape((n_samples, n_steps, n_features))\n",
    "data_reshaped=data[:n_samples*n_steps].reshape(n_samples,n_steps,n_features)\n",
    "print(f\"Reshaped data:{data.shape}\")\n",
    "\n",
    "#Split into train, test, val, fit scaler only on the train set (prevent data leakage)\n",
    "\n",
    "#train_size = int(0.6 * len(data))\n",
    "#val_size = int(0.2 * len(data))\n",
    "#test_size = len(data) - train_size - val_size\n",
    "\n",
    "#train_data = data_reshaped[:train_size]\n",
    "#val_data = data_reshaped[train_size:train_size + val_size]\n",
    "#test_data= data_reshaped[train_size + val_size:]\n",
    "\n",
    "\n",
    "#Apply time series split \n",
    "#Split into train(60%), val(20%), and test (20%)\n",
    "train_data, temp_data=train_test_split(data_reshaped,test_size=0.4,shuffle=True)\n",
    "val_data, test_data=train_test_split(temp_data, test_size=0.5, shuffle=False)\n",
    "\n",
    "##Normalization is important because of the nature of mse calculation of saits, columns with large \n",
    "#values dominate the loss, making metrics meaningless. SAITS computes MSE/MAE column-wise and averages \n",
    "#them across all columns \n",
    "#  Apply minmax scaler here \n",
    "#normalize each feature independently\n",
    "scalers={}\n",
    "\n",
    "\n",
    "#train_scaled = np.zeros_like(data_reshaped[train_size])  # Initialize the normalized data array\n",
    "#val_scaled=np.zeros_like(data_reshaped[train_size:train_size + val_size])\n",
    "#test_scaled=np.zeros_like(data_reshaped[train_size + val_size:])\n",
    "\n",
    "train_scaled = np.zeros_like(train_data)\n",
    "val_scaled = np.zeros_like(val_data)\n",
    "test_scaled = np.zeros_like(test_data)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(data_reshaped.shape[2]):\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1)) #changed to -1,1\n",
    "    # Flatten timesteps and samples for scaling\n",
    "    train_scaled[:, :, i] = scaler.fit_transform(train_data[:, :, i].reshape(-1, 1)).reshape(train_data.shape[0], train_data.shape[1])\n",
    "    val_scaled[:, :, i] = scaler.transform(val_data[:, :, i].reshape(-1, 1)).reshape(val_data.shape[0], val_data.shape[1])\n",
    "    test_scaled[:, :, i] = scaler.transform(test_data[:, :, i].reshape(-1, 1)).reshape(test_data.shape[0], test_data.shape[1])\n",
    "    scalers[i] = scaler  # Save scalers to inverse-transform later\n",
    "\n",
    "#Inverse Scale\n",
    "def inverse_scale(imputation, scalers):\n",
    "    n_samples, n_timesteps, n_features = imputation.shape\n",
    "    imputation_denorm = np.empty_like(imputation)\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        reshaped = imputation[:, :, i].reshape(-1, 1)\n",
    "        inversed = scalers[i].inverse_transform(reshaped)\n",
    "        imputation_denorm[:, :, i] = inversed.reshape(n_samples, n_timesteps)\n",
    "    \n",
    "    return imputation_denorm\n",
    "\n",
    "\n",
    "\n",
    "#Optional: Artificially mask. Mask 20% of the data (MIT part). Try masking 30% here \n",
    "def mcar_f(X, mask_ratio=0.3):\n",
    "    \"\"\"Apply MCAR only to observed values.\"\"\"\n",
    "    observed_mask=~np.isnan(X) #find observed positions\n",
    "    artificial_mask=mcar(X,mask_ratio).astype(bool) #generate MCAR mask, cast to boolean\n",
    "    #combine masks \n",
    "    combined_mask=observed_mask & artificial_mask\n",
    "\n",
    "    #Apply masking\n",
    "    X_masked=X.copy()\n",
    "    X_masked[combined_mask]=np.nan\n",
    "    return X_masked,combined_mask\n",
    "\n",
    "\n",
    "#Use mcar on validation data \n",
    "val_X_masked, val_mask =mcar_f(val_scaled)\n",
    "val_X_ori=val_scaled.copy() \n",
    "\n",
    "test_X_masked, test_mask =mcar_f(test_scaled)\n",
    "test_X_ori=test_scaled.copy() \n",
    "\n",
    "\n",
    "#?? Problem: Can't have the best input for testing\n",
    "#1.Create synthetic test_data cuz if I drop nan values for test set, there's basically nothing left\n",
    "#synthetic_data=np.random.randn(n_samples,n_steps,n_features)\n",
    "#test_X_masked,test_mask=mcar_f(synthetic_data)\n",
    "#test_X_ori=synthetic_data.copy() #Ground truth\n",
    "\n",
    "# 2, Ensure no NaN values in synthetic data\n",
    "#test_X_masked = np.nan_to_num(test_X_masked, nan=np.nanmean(test_X_masked))\n",
    "#test_X_ori = np.nan_to_num(test_X_ori, nan=np.nanmean(test_X_ori))\n",
    "\n",
    "\n",
    "\n",
    "class Config:\n",
    "    no_cuda = False\n",
    "    no_mps = False\n",
    "    seed = 1\n",
    "\n",
    "args=Config()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "use_mps = not args.no_mps and torch.backends.mps.is_available()\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "\n",
    "if args.cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA\")\n",
    "elif use_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "train_scaled = torch.tensor(train_scaled, dtype=torch.float32)\n",
    "val_X_masked = torch.tensor(val_X_masked, dtype=torch.float32)\n",
    "val_X_ori = torch.tensor(val_X_ori, dtype=torch.float32)\n",
    "\n",
    "train_scaled = train_scaled.to(device)\n",
    "val_X_masked = val_X_masked.to(device)\n",
    "val_X_ori = val_X_ori.to(device)\n",
    "\n",
    "\n",
    "#MLflow set up\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "mlflow.set_experiment(\"GP_VAE_2\")\n",
    "\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"lr\": trial.suggest_float(\"lr\", 1e-4, 1e-3, log=True),\n",
    "        \"epochs\": trial.suggest_int(\"epochs\", 10, 50),\n",
    "        \"batch_size\": trial.suggest_int(\"batch_size\", 32, 128, step=32),\n",
    "        \"length_scale\": trial.suggest_float(\"length_scale\",0.5,5.0),\n",
    "        \"beta\": trial.suggest_float(\"beta\",0.1,1.0),\n",
    "        \"kernel\":trial.suggest_categorical(\"kernel\",[\"cauchy\", \"diffusion\", \"rbf\"]),\n",
    " }\n",
    "\n",
    "    with mlflow.start_run(run_name=\"GP-VAE-Trial\", nested=True) as run:\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        gp_vae = GPVAE(\n",
    "            n_steps=data_reshaped.shape[1],\n",
    "            n_features=data_reshaped.shape[2],\n",
    "            latent_size=37, #should be the latent dimensions \n",
    "            encoder_sizes=(128,128), #should I change it here too?\n",
    "            decoder_sizes=(256,256), #should I change the model size?\n",
    "            kernel=params[\"kernel\"],\n",
    "            beta=params[\"beta\"], #The weight of KL divergence in ELBO\n",
    "            M=1,  #The number of Monte Carlo samples for ELBO estimation during training.\n",
    "            K=1,  #The number of importance weights for IWAE model training loss.\n",
    "            sigma=1.005, # The scale parameter for a kernel function\n",
    "            length_scale=params[\"length_scale\"], #The length scale parameter for a kernel function\n",
    "            kernel_scales=1, #The number of different length scales over latent space dimensions\n",
    "            window_size=24,  # Window size for the inference CNN.\n",
    "            batch_size=params[\"batch_size\"],\n",
    "            # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "            epochs=params[\"epochs\"],\n",
    "            # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "            # You can leave it to defualt as None to disable early stopping.\n",
    "            patience=3,\n",
    "            # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "            # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "            optimizer=Adam(lr=params[\"lr\"]),\n",
    "            # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "            # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "            # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "            num_workers=0,\n",
    "            # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "            # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "            device=device,\n",
    "            # set the path for saving tensorboard and trained model files \n",
    "            saving_path=\"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model\",\n",
    "            # only save the best model after training finished.\n",
    "            # You can also set it as \"better\" to save models performing better ever during training.\n",
    "            model_saving_strategy=\"best\",\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
    "        gp_vae.fit(train_set={\"X\": train_scaled}, val_set={\"X\": val_X_masked, \"X_ori\": val_X_ori})\n",
    "        gp_vae_results = gp_vae.predict({\"X\": test_X_masked}, n_sampling_times=2)\n",
    "        gp_vae_imputation = gp_vae_results[\"imputation\"]\n",
    "\n",
    "        print(f\"The shape of gp_vae_imputation is {gp_vae_imputation.shape}\")\n",
    "\n",
    "        # for error calculation, we need to take the mean value of the multiple samplings for each data sample\n",
    "        mean_gp_vae_imputation = gp_vae_imputation.mean(axis=1)\n",
    "\n",
    "        test_imputation_denorm = inverse_scale(mean_gp_vae_imputation, scalers)\n",
    "        test_ori_denorm = inverse_scale(test_X_ori, scalers)\n",
    "\n",
    "        mae_list, rmse_list = [], []\n",
    "        for i in range(n_features):\n",
    "            mask_i = test_mask[:, :, i]\n",
    "            pred = test_imputation_denorm[:, :, i][mask_i]\n",
    "            true = test_ori_denorm[:, :, i][mask_i]\n",
    "            if len(true) == 0: continue\n",
    "            mae = np.mean(np.abs(pred - true))\n",
    "            rmse = np.sqrt(mean_squared_error(true, pred))\n",
    "            mae_list.append(mae)\n",
    "            rmse_list.append(rmse)\n",
    "            mlflow.log_metric(f\"MAE_{i}\", mae)\n",
    "            mlflow.log_metric(f\"RMSE_{i}\", rmse)\n",
    "\n",
    "         # Calculate metrics\n",
    "        mae_per_feature = []\n",
    "        rmse_per_feature=[]\n",
    "        percentage_mae_per_feature = []\n",
    "\n",
    "        for i in range(n_features):\n",
    "            imputation_i = test_imputation_denorm[:, :, i]\n",
    "            ground_truth_i = test_ori_denorm[:, :, i]\n",
    "            mask_i = test_mask[:, :, i]\n",
    "            if np.isnan(imputation_i).any() or np.isnan(ground_truth_i).any():\n",
    "                continue\n",
    "            mae_i = calc_mae(imputation_i, ground_truth_i, mask_i)\n",
    "            mae_per_feature.append(mae_i)\n",
    "            rmse_i = np.sqrt(mean_squared_error(imputation_i, ground_truth_i))\n",
    "            rmse_per_feature.append(rmse)\n",
    "\n",
    "            #Calculate the original standard deviation for the feature\n",
    "            std_dev_i = np.std(ground_truth_i[mask_i == 1])\n",
    "             # Calculate the percentage of MAE relative to the standard deviation   \n",
    "            if std_dev_i != 0:\n",
    "                percentage_mae_i = (mae_i / std_dev_i) * 100\n",
    "                percentage_mae_per_feature.append(percentage_mae_i)\n",
    "            else:\n",
    "                 percentage_mae_i = float('inf')\n",
    "            \n",
    "            mlflow.log_metric(f\"MAE_{feature_names[i]}\", mae_i)\n",
    "            mlflow.log_metric(f\"RMSE_{feature_names[i]}\",rmse_i)\n",
    "            mlflow.log_metric(f\"Percentage_MAE_{feature_names[i]}\", percentage_mae_i)\n",
    "\n",
    "        avg_mae = np.mean(mae_per_feature)\n",
    "        #avg_rmse=np.mean(rmse_per_feature)\n",
    "       \n",
    "        mlflow.log_metric(\"avg_mae\", np.mean(mae_list))\n",
    "        mlflow.log_metric(\"avg_rmse\", np.mean(rmse_list))\n",
    "\n",
    "        trial.set_user_attr(\"mlflow_run_id\", run.info.run_id)\n",
    "\n",
    "        return avg_mae\n",
    "\n",
    "    print(\"MAE per feature:\", mae_per_feature)\n",
    "    print(\"RMSE per feature\",rmse_per_feature)\n",
    "    print(\"Percentage MAE per feature:\", percentage_mae_per_feature)\n",
    "   \n",
    "\n",
    "# Run Optuna study\n",
    "mlflow.set_experiment(\"GP-VAE-2\")\n",
    "with mlflow.start_run(run_name=\"GPVAE_Optuna_Study(2)\") as parent_run:\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "\n",
    "    best_params = study.best_trial.params\n",
    "    best_value = study.best_trial.value\n",
    "    best_run_id = study.best_trial.user_attrs[\"mlflow_run_id\"]\n",
    "\n",
    "\n",
    "    \n",
    "    # Log best parameters\n",
    "    mlflow.log_params(best_params)\n",
    "\n",
    "    # Log best metric(s)\n",
    "    mlflow.log_metric(\"best_objective_value\", best_value)\n",
    "    mlflow.log_param(\"best_run_id\", best_run_id)\n",
    "\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Best Objective Value:\", best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a611031-c8ce-4fcd-87b9-8a31350efc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "[I 2025-05-20 00:43:13,791] A new study created in memory with name: no-name-1e6f6177-6264-400c-b102-a2982cc10411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column time has 0 NaN values\n",
      "Column time has 0.0 Missing_rate\n",
      "Column fr_eng has 0 NaN values\n",
      "Column fr_eng has 0.0 Missing_rate\n",
      "Column te_exh_cyl_out__0 has 0 NaN values\n",
      "Column te_exh_cyl_out__0 has 0.0 Missing_rate\n",
      "Column pd_air_ic__0 has 0 NaN values\n",
      "Column pd_air_ic__0 has 0.0 Missing_rate\n",
      "Column pr_exh_turb_out__0 has 316581 NaN values\n",
      "Column pr_exh_turb_out__0 has 1.0 Missing_rate\n",
      "Column te_air_ic_out__0 has 0 NaN values\n",
      "Column te_air_ic_out__0 has 0.0 Missing_rate\n",
      "Column te_seawater has 0 NaN values\n",
      "Column te_seawater has 0.0 Missing_rate\n",
      "Column te_air_comp_in_a__0 has 316581 NaN values\n",
      "Column te_air_comp_in_a__0 has 1.0 Missing_rate\n",
      "Column te_air_comp_in_b__0 has 316581 NaN values\n",
      "Column te_air_comp_in_b__0 has 1.0 Missing_rate\n",
      "Column fr_tc__0 has 316581 NaN values\n",
      "Column fr_tc__0 has 1.0 Missing_rate\n",
      "Column pr_baro has 0 NaN values\n",
      "Column pr_baro has 0.0 Missing_rate\n",
      "Column pd_air_ic__0_1 has 0 NaN values\n",
      "Column pd_air_ic__0_1 has 0.0 Missing_rate\n",
      "Column pr_exh_rec has 0 NaN values\n",
      "Column pr_exh_rec has 0.0 Missing_rate\n",
      "Column te_exh_turb_in__0 has 316581 NaN values\n",
      "Column te_exh_turb_in__0 has 1.0 Missing_rate\n",
      "Column te_exh_turb_out__0 has 316581 NaN values\n",
      "Column te_exh_turb_out__0 has 1.0 Missing_rate\n",
      "Column bo_aux_blower_running has 0 NaN values\n",
      "Column bo_aux_blower_running has 0.0 Missing_rate\n",
      "Column re_eng_load has 426 NaN values\n",
      "Column re_eng_load has 0.0013456271854596453 Missing_rate\n",
      "Column pr_air_scav_ecs has 0 NaN values\n",
      "Column pr_air_scav_ecs has 0.0 Missing_rate\n",
      "Column pr_air_scav has 0 NaN values\n",
      "Column pr_air_scav has 0.0 Missing_rate\n",
      "Column te_air_scav_rec has 0 NaN values\n",
      "Column te_air_scav_rec has 0.0 Missing_rate\n",
      "Column te_air_ic_out__0_1 has 0 NaN values\n",
      "Column te_air_ic_out__0_1 has 0.0 Missing_rate\n",
      "Column pr_cyl_comp__0 has 426 NaN values\n",
      "Column pr_cyl_comp__0 has 0.0013456271854596453 Missing_rate\n",
      "Column pr_cyl_max__0 has 426 NaN values\n",
      "Column pr_cyl_max__0 has 0.0013456271854596453 Missing_rate\n",
      "Column se_mip__0 has 426 NaN values\n",
      "Column se_mip__0 has 0.0013456271854596453 Missing_rate\n",
      "Column te_exh_cyl_out__0_1 has 0 NaN values\n",
      "Column te_exh_cyl_out__0_1 has 0.0 Missing_rate\n",
      "Column fr_eng_setpoint has 0 NaN values\n",
      "Column fr_eng_setpoint has 0.0 Missing_rate\n",
      "Column te_air_scav_rec_iso has 126055 NaN values\n",
      "Column te_air_scav_rec_iso has 0.3981761381763277 Missing_rate\n",
      "Column pr_cyl_max_mv_iso has 128557 NaN values\n",
      "Column pr_cyl_max_mv_iso has 0.40607932882895686 Missing_rate\n",
      "Column pr_cyl_comp_mv_iso has 128557 NaN values\n",
      "Column pr_cyl_comp_mv_iso has 0.40607932882895686 Missing_rate\n",
      "Column fr_eng_ecs has 0 NaN values\n",
      "Column fr_eng_ecs has 0.0 Missing_rate\n",
      "Column pr_air_scav_iso has 128557 NaN values\n",
      "Column pr_air_scav_iso has 0.40607932882895686 Missing_rate\n",
      "Column engine_type_G80ME-C9.5-GI-LPSCR has 0 NaN values\n",
      "Column engine_type_G80ME-C9.5-GI-LPSCR has 0.0 Missing_rate\n",
      "Original size:316581, Sampled size: 105527\n",
      "Reshaped data:(105527, 31)\n",
      "CUDA available: True\n",
      "Using CUDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 00:43:13 [INFO]: Using the given device: cuda\n",
      "2025-05-20 00:43:13 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004313\n",
      "2025-05-20 00:43:13 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004313/tensorboard\n",
      "2025-05-20 00:43:13 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n",
      "2025-05-20 00:43:15 [INFO]: Epoch 001 - training loss (default): 1974.3210, validation loss: 4101.7576\n",
      "2025-05-20 00:43:16 [INFO]: Epoch 002 - training loss (default): 1702.0264, validation loss: 4097.6497\n",
      "2025-05-20 00:43:17 [INFO]: Epoch 003 - training loss (default): 1700.6533, validation loss: 4094.6074\n",
      "2025-05-20 00:43:18 [INFO]: Epoch 004 - training loss (default): 1699.8676, validation loss: 4090.9239\n",
      "2025-05-20 00:43:19 [INFO]: Epoch 005 - training loss (default): 1699.6737, validation loss: 4092.5267\n",
      "2025-05-20 00:43:21 [INFO]: Epoch 006 - training loss (default): 1699.3644, validation loss: 4096.0181\n",
      "2025-05-20 00:43:22 [INFO]: Epoch 007 - training loss (default): 1700.8446, validation loss: 4128.5475\n",
      "2025-05-20 00:43:22 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-05-20 00:43:22 [INFO]: Finished training. The best model is from epoch#4.\n",
      "2025-05-20 00:43:22 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004313/GPVAE.pypots\n",
      "[W 2025-05-20 00:43:22,502] Trial 0 failed with parameters: {'lr': 0.000992372189515649, 'epochs': 38, 'batch_size': 32, 'length_scale': 2.439542596742112, 'beta': 0.7576863238401589} because of the following error: NameError(\"name 'rmse' is not defined\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"<ipython-input-16-1f5294201586>\", line 326, in objective\n",
      "    rmse_per_feature.append(rmse)\n",
      "NameError: name 'rmse' is not defined\n",
      "[W 2025-05-20 00:43:22,504] Trial 0 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n",
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/d2cd207c3fbb495ca3e178b6c618899f\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n",
      "🏃 View run GPVAE_Optuna_Study(2) at: http://localhost:5000/#/experiments/832352739106302318/runs/63c95a2dd80d44c28599e6f3d7c72d94\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'rmse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 360\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[39mwith\u001b[39;00m mlflow\u001b[39m.\u001b[39mstart_run(run_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGPVAE_Optuna_Study(2)\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m parent_run:\n\u001b[1;32m    359\u001b[0m     study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 360\u001b[0m     study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\n\u001b[1;32m    362\u001b[0m     best_params \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_trial\u001b[39m.\u001b[39mparams\n\u001b[1;32m    363\u001b[0m     best_value \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_trial\u001b[39m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     _optimize(\n\u001b[1;32m    476\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    477\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    478\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    479\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    480\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    481\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    482\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    483\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    484\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    485\u001b[0m     )\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         _optimize_sequential(\n\u001b[1;32m     64\u001b[0m             study,\n\u001b[1;32m     65\u001b[0m             func,\n\u001b[1;32m     66\u001b[0m             n_trials,\n\u001b[1;32m     67\u001b[0m             timeout,\n\u001b[1;32m     68\u001b[0m             catch,\n\u001b[1;32m     69\u001b[0m             callbacks,\n\u001b[1;32m     70\u001b[0m             gc_after_trial,\n\u001b[1;32m     71\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     72\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     73\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     74\u001b[0m         )\n\u001b[1;32m     75\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    161\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    198\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[16], line 326\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    324\u001b[0m mae_per_feature\u001b[39m.\u001b[39mappend(mae_i)\n\u001b[1;32m    325\u001b[0m rmse_i \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msqrt(mean_squared_error(imputation_i, ground_truth_i))\n\u001b[0;32m--> 326\u001b[0m rmse_per_feature\u001b[39m.\u001b[39mappend(rmse)\n\u001b[1;32m    328\u001b[0m \u001b[39m#Calculate the original standard deviation for the feature\u001b[39;00m\n\u001b[1;32m    329\u001b[0m std_dev_i \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstd(ground_truth_i[mask_i \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rmse' is not defined"
     ]
    }
   ],
   "source": [
    "#Import Pypots Library\n",
    "from pypots.optim import Adam\n",
    "from pypots.imputation import GPVAE\n",
    "#from pypots.utils.metrics import calc_mae\n",
    "from pypots.nn.functional import calc_mae\n",
    "\n",
    "\n",
    "import argparse\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import data_insight\n",
    "from data_insight import setup_duckdb\n",
    "from duckdb import DuckDBPyConnection as DuckDB\n",
    "from duckdb import DuckDBPyRelation as Relation\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "from duckdb import DuckDBPyConnection as DuckDB\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna \n",
    "from optuna.visualization import plot_optimization_history\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, Dataset\n",
    "from pygrinder.missing_completely_at_random import mcar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import sensor_imputation_thesis.shared.load_data as load\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "#PatchTST might be an ideal choise if SAITS is too slow \n",
    "\n",
    "##Drop columns with different indexes while loading data.. Or the mean values \n",
    "\n",
    "df=pd.read_parquet(\"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/ny_df_for_pypots.parquet\")\n",
    "\n",
    "len(df)\n",
    "\n",
    "#current length of the dataframe is 119439\n",
    "\n",
    "# Check nan values in each column\n",
    "for col in df.columns:\n",
    "    print(f\"Column {col} has {df[col].isna().sum()} NaN values\")\n",
    "    missing_rate=df[col].isna().sum()/len(df[col])\n",
    "    print(f\"Column {col} has {missing_rate} Missing_rate\")\n",
    "\n",
    "\n",
    "#Try with smaller dataset, size 4000\n",
    "##SAMPLE the percengtage of the dataset, df.sample (averagely pick samples)\n",
    "#not df.sample cuz it will randomly select \n",
    "original_size=len(df)\n",
    "desired_fraction=0.3 #Select data every 3 minutes \n",
    "step=int(1/desired_fraction) #step_size=10 (sample every 10th (3/10) minute)\n",
    "\n",
    "#Systematic sampling: Start at a random offset to avoid bias \n",
    "start=np.random.randint(0,step) #Random start between 0-9\n",
    "df1=df.iloc[start::step].reset_index(drop=True)\n",
    "\n",
    "print(f\"Original size:{len(df)}, Sampled size: {len(df1)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Custom Dataset class\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Data processing code\n",
    "sensor_cols = [col for col in df1.columns if col != \"time\"]\n",
    "data = df1[sensor_cols].values\n",
    "\n",
    "#¤get feature names for printing mae later \n",
    "feature_names=df1[sensor_cols].columns.tolist()\n",
    "\n",
    "## Convert data to 3D arrays of shape n_samples, n_timesteps, n_features, X_ori refers to the original data without missing values \n",
    "## Reconstruct all columns simultaneously  #num_features: 119\n",
    "n_features = data.shape[1]  # exclude the time column\n",
    "n_steps = 20 #60 (was 60 previously) #(TRY TO CHANGE HERE)  # # window length, 1440 steps = 24 hours of 1-minute data, but here is revised to 60 again\n",
    "#total_elements = data.shape[0] * data.shape[1]\n",
    "n_samples = data.shape[0] // n_steps \n",
    "\n",
    "\n",
    "\n",
    "# Reshape to (n_samples // n_steps, n_steps, n_features)\n",
    "#data_reshaped = data.reshape((n_samples, n_steps, n_features))\n",
    "data_reshaped=data[:n_samples*n_steps].reshape(n_samples,n_steps,n_features)\n",
    "print(f\"Reshaped data:{data.shape}\")\n",
    "\n",
    "#Split into train, test, val, fit scaler only on the train set (prevent data leakage)\n",
    "\n",
    "#train_size = int(0.6 * len(data))\n",
    "#val_size = int(0.2 * len(data))\n",
    "#test_size = len(data) - train_size - val_size\n",
    "\n",
    "#train_data = data_reshaped[:train_size]\n",
    "#val_data = data_reshaped[train_size:train_size + val_size]\n",
    "#test_data= data_reshaped[train_size + val_size:]\n",
    "\n",
    "\n",
    "#Apply time series split \n",
    "#Split into train(60%), val(20%), and test (20%)\n",
    "train_data, temp_data=train_test_split(data_reshaped,test_size=0.4,shuffle=True)\n",
    "val_data, test_data=train_test_split(temp_data, test_size=0.5, shuffle=False)\n",
    "\n",
    "##Normalization is important because of the nature of mse calculation of saits, columns with large \n",
    "#values dominate the loss, making metrics meaningless. SAITS computes MSE/MAE column-wise and averages \n",
    "#them across all columns \n",
    "#  Apply minmax scaler here \n",
    "#normalize each feature independently\n",
    "scalers={}\n",
    "\n",
    "\n",
    "#train_scaled = np.zeros_like(data_reshaped[train_size])  # Initialize the normalized data array\n",
    "#val_scaled=np.zeros_like(data_reshaped[train_size:train_size + val_size])\n",
    "#test_scaled=np.zeros_like(data_reshaped[train_size + val_size:])\n",
    "\n",
    "train_scaled = np.zeros_like(train_data)\n",
    "val_scaled = np.zeros_like(val_data)\n",
    "test_scaled = np.zeros_like(test_data)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(data_reshaped.shape[2]):\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1)) #changed to -1,1\n",
    "    # Flatten timesteps and samples for scaling\n",
    "    train_scaled[:, :, i] = scaler.fit_transform(train_data[:, :, i].reshape(-1, 1)).reshape(train_data.shape[0], train_data.shape[1])\n",
    "    val_scaled[:, :, i] = scaler.transform(val_data[:, :, i].reshape(-1, 1)).reshape(val_data.shape[0], val_data.shape[1])\n",
    "    test_scaled[:, :, i] = scaler.transform(test_data[:, :, i].reshape(-1, 1)).reshape(test_data.shape[0], test_data.shape[1])\n",
    "    scalers[i] = scaler  # Save scalers to inverse-transform later\n",
    "\n",
    "#Inverse Scale\n",
    "def inverse_scale(imputation, scalers):\n",
    "    n_samples, n_timesteps, n_features = imputation.shape\n",
    "    imputation_denorm = np.empty_like(imputation)\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        reshaped = imputation[:, :, i].reshape(-1, 1)\n",
    "        inversed = scalers[i].inverse_transform(reshaped)\n",
    "        imputation_denorm[:, :, i] = inversed.reshape(n_samples, n_timesteps)\n",
    "    \n",
    "    return imputation_denorm\n",
    "\n",
    "\n",
    "\n",
    "#Optional: Artificially mask. Mask 20% of the data (MIT part). Try masking 30% here \n",
    "def mcar_f(X, mask_ratio=0.3):\n",
    "    \"\"\"Apply MCAR only to observed values.\"\"\"\n",
    "    observed_mask=~np.isnan(X) #find observed positions\n",
    "    artificial_mask=mcar(X,mask_ratio).astype(bool) #generate MCAR mask, cast to boolean\n",
    "    #combine masks \n",
    "    combined_mask=observed_mask & artificial_mask\n",
    "\n",
    "    #Apply masking\n",
    "    X_masked=X.copy()\n",
    "    X_masked[combined_mask]=np.nan\n",
    "    return X_masked,combined_mask\n",
    "\n",
    "\n",
    "#Use mcar on validation data \n",
    "val_X_masked, val_mask =mcar_f(val_scaled)\n",
    "val_X_ori=val_scaled.copy() \n",
    "\n",
    "test_X_masked, test_mask =mcar_f(test_scaled)\n",
    "test_X_ori=test_scaled.copy() \n",
    "\n",
    "\n",
    "#?? Problem: Can't have the best input for testing\n",
    "#1.Create synthetic test_data cuz if I drop nan values for test set, there's basically nothing left\n",
    "#synthetic_data=np.random.randn(n_samples,n_steps,n_features)\n",
    "#test_X_masked,test_mask=mcar_f(synthetic_data)\n",
    "#test_X_ori=synthetic_data.copy() #Ground truth\n",
    "\n",
    "# 2, Ensure no NaN values in synthetic data\n",
    "#test_X_masked = np.nan_to_num(test_X_masked, nan=np.nanmean(test_X_masked))\n",
    "#test_X_ori = np.nan_to_num(test_X_ori, nan=np.nanmean(test_X_ori))\n",
    "\n",
    "\n",
    "\n",
    "class Config:\n",
    "    no_cuda = False\n",
    "    no_mps = False\n",
    "    seed = 1\n",
    "\n",
    "args=Config()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "use_mps = not args.no_mps and torch.backends.mps.is_available()\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "\n",
    "if args.cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA\")\n",
    "elif use_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "train_scaled = torch.tensor(train_scaled, dtype=torch.float32)\n",
    "val_X_masked = torch.tensor(val_X_masked, dtype=torch.float32)\n",
    "val_X_ori = torch.tensor(val_X_ori, dtype=torch.float32)\n",
    "\n",
    "train_scaled = train_scaled.to(device)\n",
    "val_X_masked = val_X_masked.to(device)\n",
    "val_X_ori = val_X_ori.to(device)\n",
    "\n",
    "\n",
    "#MLflow set up\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "mlflow.set_experiment(\"GP_VAE_2\")\n",
    "\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"lr\": trial.suggest_float(\"lr\", 1e-4, 1e-3, log=True),\n",
    "        \"epochs\": trial.suggest_int(\"epochs\", 10, 50),\n",
    "        \"batch_size\": trial.suggest_int(\"batch_size\", 32, 128, step=32),\n",
    "        \"length_scale\": trial.suggest_float(\"length_scale\",0.5,5.0),\n",
    "        \"beta\": trial.suggest_float(\"beta\",0.1,1.0)\n",
    " }\n",
    "\n",
    "    with mlflow.start_run(run_name=\"GP-VAE-Trial\", nested=True) as run:\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        gp_vae = GPVAE(\n",
    "            n_steps=data_reshaped.shape[1],\n",
    "            n_features=data_reshaped.shape[2],\n",
    "            latent_size=37, #should be the latent dimensions \n",
    "            encoder_sizes=(128,128), #should I change it here too?\n",
    "            decoder_sizes=(256,256), #should I change the model size?\n",
    "            kernel=\"cauchy\",\n",
    "            beta=params[\"beta\"], #The weight of KL divergence in ELBO\n",
    "            M=1,  #The number of Monte Carlo samples for ELBO estimation during training.\n",
    "            K=1,  #The number of importance weights for IWAE model training loss.\n",
    "            sigma=1.005, # The scale parameter for a kernel function\n",
    "            length_scale=params[\"length_scale\"], #The length scale parameter for a kernel function\n",
    "            kernel_scales=1, #The number of different length scales over latent space dimensions\n",
    "            window_size=24,  # Window size for the inference CNN.\n",
    "            batch_size=params[\"batch_size\"],\n",
    "            # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "            epochs=params[\"epochs\"],\n",
    "            # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "            # You can leave it to defualt as None to disable early stopping.\n",
    "            patience=3,\n",
    "            # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "            # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "            optimizer=Adam(lr=params[\"lr\"]),\n",
    "            # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "            # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "            # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "            num_workers=0,\n",
    "            # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "            # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "            device=device,\n",
    "            # set the path for saving tensorboard and trained model files \n",
    "            saving_path=\"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model\",\n",
    "            # only save the best model after training finished.\n",
    "            # You can also set it as \"better\" to save models performing better ever during training.\n",
    "            model_saving_strategy=\"best\",\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
    "        gp_vae.fit(train_set={\"X\": train_scaled}, val_set={\"X\": val_X_masked, \"X_ori\": val_X_ori})\n",
    "        gp_vae_results = gp_vae.predict({\"X\": test_X_masked}, n_sampling_times=2)\n",
    "        gp_vae_imputation = gp_vae_results[\"imputation\"]\n",
    "\n",
    "        print(f\"The shape of gp_vae_imputation is {gp_vae_imputation.shape}\")\n",
    "\n",
    "        # for error calculation, we need to take the mean value of the multiple samplings for each data sample\n",
    "        mean_gp_vae_imputation = gp_vae_imputation.mean(axis=1)\n",
    "\n",
    "        test_imputation_denorm = inverse_scale(mean_gp_vae_imputation, scalers)\n",
    "        test_ori_denorm = inverse_scale(test_X_ori, scalers)\n",
    "\n",
    "\n",
    "         # Calculate metrics\n",
    "        mae_per_feature = []\n",
    "        rmse_per_feature=[]\n",
    "        percentage_mae_per_feature = []\n",
    "\n",
    "        for i in range(n_features):\n",
    "            imputation_i = test_imputation_denorm[:, :, i]\n",
    "            ground_truth_i = test_ori_denorm[:, :, i]\n",
    "            mask_i = test_mask[:, :, i]\n",
    "            if np.isnan(imputation_i).any() or np.isnan(ground_truth_i).any():\n",
    "                continue\n",
    "            mae_i = calc_mae(imputation_i, ground_truth_i, mask_i)\n",
    "            mae_per_feature.append(mae_i)\n",
    "            rmse_i = np.sqrt(mean_squared_error(imputation_i, ground_truth_i))\n",
    "            rmse_per_feature.append(rmse)\n",
    "\n",
    "            #Calculate the original standard deviation for the feature\n",
    "            std_dev_i = np.std(ground_truth_i[mask_i == 1])\n",
    "             # Calculate the percentage of MAE relative to the standard deviation   \n",
    "            if std_dev_i != 0:\n",
    "                percentage_mae_i = (mae_i / std_dev_i) * 100\n",
    "                percentage_mae_per_feature.append(percentage_mae_i)\n",
    "            else:\n",
    "                 percentage_mae_i = float('inf')\n",
    "            \n",
    "            mlflow.log_metric(f\"MAE_{feature_names[i]}\", mae_i)\n",
    "            mlflow.log_metric(f\"RMSE_{feature_names[i]}\",rmse_i)\n",
    "            mlflow.log_metric(f\"Percentage_MAE_{feature_names[i]}\", percentage_mae_i)\n",
    "\n",
    "        avg_mae = np.mean(mae_per_feature)\n",
    "        #avg_rmse=np.mean(rmse_per_feature)\n",
    "       \n",
    "        mlflow.log_metric(\"avg_mae\", np.mean(mae_list))\n",
    "        mlflow.log_metric(\"avg_rmse\", np.mean(rmse_list))\n",
    "\n",
    "        trial.set_user_attr(\"mlflow_run_id\", run.info.run_id)\n",
    "\n",
    "        return avg_mae\n",
    "\n",
    "    print(\"MAE per feature:\", mae_per_feature)\n",
    "    print(\"RMSE per feature\",rmse_per_feature)\n",
    "    print(\"Percentage MAE per feature:\", percentage_mae_per_feature)\n",
    "   \n",
    "\n",
    "# Run Optuna study\n",
    "mlflow.set_experiment(\"GP-VAE-2\")\n",
    "with mlflow.start_run(run_name=\"GPVAE_Optuna_Study(2)\") as parent_run:\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "\n",
    "    best_params = study.best_trial.params\n",
    "    best_value = study.best_trial.value\n",
    "    best_run_id = study.best_trial.user_attrs[\"mlflow_run_id\"]\n",
    "\n",
    "\n",
    "    \n",
    "    # Log best parameters\n",
    "    mlflow.log_params(best_params)\n",
    "\n",
    "    # Log best metric(s)\n",
    "    mlflow.log_metric(\"best_objective_value\", best_value)\n",
    "    mlflow.log_param(\"best_run_id\", best_run_id)\n",
    "\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Best Objective Value:\", best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15b652c-7b59-470e-aadf-daadabcf14e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "[I 2025-05-20 00:44:00,741] A new study created in memory with name: no-name-4a8855c3-887e-4d62-850b-d435bcacc905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column time has 0 NaN values\n",
      "Column time has 0.0 Missing_rate\n",
      "Column fr_eng has 0 NaN values\n",
      "Column fr_eng has 0.0 Missing_rate\n",
      "Column te_exh_cyl_out__0 has 0 NaN values\n",
      "Column te_exh_cyl_out__0 has 0.0 Missing_rate\n",
      "Column pd_air_ic__0 has 0 NaN values\n",
      "Column pd_air_ic__0 has 0.0 Missing_rate\n",
      "Column pr_exh_turb_out__0 has 316581 NaN values\n",
      "Column pr_exh_turb_out__0 has 1.0 Missing_rate\n",
      "Column te_air_ic_out__0 has 0 NaN values\n",
      "Column te_air_ic_out__0 has 0.0 Missing_rate\n",
      "Column te_seawater has 0 NaN values\n",
      "Column te_seawater has 0.0 Missing_rate\n",
      "Column te_air_comp_in_a__0 has 316581 NaN values\n",
      "Column te_air_comp_in_a__0 has 1.0 Missing_rate\n",
      "Column te_air_comp_in_b__0 has 316581 NaN values\n",
      "Column te_air_comp_in_b__0 has 1.0 Missing_rate\n",
      "Column fr_tc__0 has 316581 NaN values\n",
      "Column fr_tc__0 has 1.0 Missing_rate\n",
      "Column pr_baro has 0 NaN values\n",
      "Column pr_baro has 0.0 Missing_rate\n",
      "Column pd_air_ic__0_1 has 0 NaN values\n",
      "Column pd_air_ic__0_1 has 0.0 Missing_rate\n",
      "Column pr_exh_rec has 0 NaN values\n",
      "Column pr_exh_rec has 0.0 Missing_rate\n",
      "Column te_exh_turb_in__0 has 316581 NaN values\n",
      "Column te_exh_turb_in__0 has 1.0 Missing_rate\n",
      "Column te_exh_turb_out__0 has 316581 NaN values\n",
      "Column te_exh_turb_out__0 has 1.0 Missing_rate\n",
      "Column bo_aux_blower_running has 0 NaN values\n",
      "Column bo_aux_blower_running has 0.0 Missing_rate\n",
      "Column re_eng_load has 426 NaN values\n",
      "Column re_eng_load has 0.0013456271854596453 Missing_rate\n",
      "Column pr_air_scav_ecs has 0 NaN values\n",
      "Column pr_air_scav_ecs has 0.0 Missing_rate\n",
      "Column pr_air_scav has 0 NaN values\n",
      "Column pr_air_scav has 0.0 Missing_rate\n",
      "Column te_air_scav_rec has 0 NaN values\n",
      "Column te_air_scav_rec has 0.0 Missing_rate\n",
      "Column te_air_ic_out__0_1 has 0 NaN values\n",
      "Column te_air_ic_out__0_1 has 0.0 Missing_rate\n",
      "Column pr_cyl_comp__0 has 426 NaN values\n",
      "Column pr_cyl_comp__0 has 0.0013456271854596453 Missing_rate\n",
      "Column pr_cyl_max__0 has 426 NaN values\n",
      "Column pr_cyl_max__0 has 0.0013456271854596453 Missing_rate\n",
      "Column se_mip__0 has 426 NaN values\n",
      "Column se_mip__0 has 0.0013456271854596453 Missing_rate\n",
      "Column te_exh_cyl_out__0_1 has 0 NaN values\n",
      "Column te_exh_cyl_out__0_1 has 0.0 Missing_rate\n",
      "Column fr_eng_setpoint has 0 NaN values\n",
      "Column fr_eng_setpoint has 0.0 Missing_rate\n",
      "Column te_air_scav_rec_iso has 126055 NaN values\n",
      "Column te_air_scav_rec_iso has 0.3981761381763277 Missing_rate\n",
      "Column pr_cyl_max_mv_iso has 128557 NaN values\n",
      "Column pr_cyl_max_mv_iso has 0.40607932882895686 Missing_rate\n",
      "Column pr_cyl_comp_mv_iso has 128557 NaN values\n",
      "Column pr_cyl_comp_mv_iso has 0.40607932882895686 Missing_rate\n",
      "Column fr_eng_ecs has 0 NaN values\n",
      "Column fr_eng_ecs has 0.0 Missing_rate\n",
      "Column pr_air_scav_iso has 128557 NaN values\n",
      "Column pr_air_scav_iso has 0.40607932882895686 Missing_rate\n",
      "Column engine_type_G80ME-C9.5-GI-LPSCR has 0 NaN values\n",
      "Column engine_type_G80ME-C9.5-GI-LPSCR has 0.0 Missing_rate\n",
      "Original size:316581, Sampled size: 105527\n",
      "Reshaped data:(105527, 31)\n",
      "CUDA available: True\n",
      "Using CUDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 00:44:00 [INFO]: Using the given device: cuda\n",
      "2025-05-20 00:44:00 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004400\n",
      "2025-05-20 00:44:00 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004400/tensorboard\n",
      "2025-05-20 00:44:00 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n",
      "2025-05-20 00:44:01 [INFO]: Epoch 001 - training loss (default): 869.0620, validation loss: 609.1907\n",
      "2025-05-20 00:44:03 [INFO]: Epoch 002 - training loss (default): 632.1673, validation loss: 598.7252\n",
      "2025-05-20 00:44:04 [INFO]: Epoch 003 - training loss (default): 622.1954, validation loss: 593.6872\n",
      "2025-05-20 00:44:05 [INFO]: Epoch 004 - training loss (default): 619.6878, validation loss: 590.0230\n",
      "2025-05-20 00:44:06 [INFO]: Epoch 005 - training loss (default): 618.5876, validation loss: 587.2158\n",
      "2025-05-20 00:44:07 [INFO]: Epoch 006 - training loss (default): 617.9601, validation loss: 584.9006\n",
      "2025-05-20 00:44:09 [INFO]: Epoch 007 - training loss (default): 617.4860, validation loss: 582.9144\n",
      "2025-05-20 00:44:10 [INFO]: Epoch 008 - training loss (default): 617.1302, validation loss: 581.1771\n",
      "2025-05-20 00:44:11 [INFO]: Epoch 009 - training loss (default): 616.7653, validation loss: 579.5841\n",
      "2025-05-20 00:44:13 [INFO]: Epoch 010 - training loss (default): 616.4522, validation loss: 578.1334\n",
      "2025-05-20 00:44:14 [INFO]: Epoch 011 - training loss (default): 616.2224, validation loss: 576.7829\n",
      "2025-05-20 00:44:15 [INFO]: Epoch 012 - training loss (default): 616.0071, validation loss: 575.6979\n",
      "2025-05-20 00:44:16 [INFO]: Epoch 013 - training loss (default): 615.7742, validation loss: 574.5905\n",
      "2025-05-20 00:44:17 [INFO]: Epoch 014 - training loss (default): 615.5501, validation loss: 573.5765\n",
      "2025-05-20 00:44:19 [INFO]: Epoch 015 - training loss (default): 615.4004, validation loss: 572.6480\n",
      "2025-05-20 00:44:20 [INFO]: Epoch 016 - training loss (default): 615.2417, validation loss: 571.7732\n",
      "2025-05-20 00:44:21 [INFO]: Epoch 017 - training loss (default): 615.1193, validation loss: 570.9813\n",
      "2025-05-20 00:44:22 [INFO]: Epoch 018 - training loss (default): 615.0463, validation loss: 570.2407\n",
      "2025-05-20 00:44:23 [INFO]: Epoch 019 - training loss (default): 614.8837, validation loss: 569.5142\n",
      "2025-05-20 00:44:25 [INFO]: Epoch 020 - training loss (default): 614.7821, validation loss: 568.7592\n",
      "2025-05-20 00:44:26 [INFO]: Epoch 021 - training loss (default): 614.6894, validation loss: 568.0169\n",
      "2025-05-20 00:44:27 [INFO]: Epoch 022 - training loss (default): 614.6055, validation loss: 567.2682\n",
      "2025-05-20 00:44:28 [INFO]: Epoch 023 - training loss (default): 614.5842, validation loss: 566.5545\n",
      "2025-05-20 00:44:30 [INFO]: Epoch 024 - training loss (default): 614.5879, validation loss: 566.0992\n",
      "2025-05-20 00:44:31 [INFO]: Epoch 025 - training loss (default): 614.4479, validation loss: 565.4506\n",
      "2025-05-20 00:44:32 [INFO]: Epoch 026 - training loss (default): 614.4844, validation loss: 564.8221\n",
      "2025-05-20 00:44:33 [INFO]: Epoch 027 - training loss (default): 614.4617, validation loss: 564.3305\n",
      "2025-05-20 00:44:34 [INFO]: Epoch 028 - training loss (default): 614.3524, validation loss: 563.6780\n",
      "2025-05-20 00:44:36 [INFO]: Epoch 029 - training loss (default): 614.3199, validation loss: 563.0897\n",
      "2025-05-20 00:44:37 [INFO]: Epoch 030 - training loss (default): 614.3042, validation loss: 562.6152\n",
      "2025-05-20 00:44:38 [INFO]: Epoch 031 - training loss (default): 614.2916, validation loss: 562.0366\n",
      "2025-05-20 00:44:39 [INFO]: Epoch 032 - training loss (default): 614.2946, validation loss: 561.6875\n",
      "2025-05-20 00:44:40 [INFO]: Epoch 033 - training loss (default): 614.2421, validation loss: 561.2597\n",
      "2025-05-20 00:44:42 [INFO]: Epoch 034 - training loss (default): 614.2076, validation loss: 560.7216\n",
      "2025-05-20 00:44:43 [INFO]: Epoch 035 - training loss (default): 614.1953, validation loss: 560.2592\n",
      "2025-05-20 00:44:43 [INFO]: Finished training. The best model is from epoch#35.\n",
      "2025-05-20 00:44:43 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004400/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-05-20 00:44:43,686] Trial 0 failed with parameters: {'lr': 0.0001126821627210784, 'epochs': 35, 'batch_size': 32, 'length_scale': 1.7300271829834157, 'beta': 0.5425219729252396} because of the following error: NameError(\"name 'mae_list' is not defined\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"<ipython-input-17-6cab5f49d7d7>\", line 344, in objective\n",
      "    mlflow.log_metric(\"avg_mae\", np.mean(mae_list))\n",
      "NameError: name 'mae_list' is not defined\n",
      "[W 2025-05-20 00:44:43,690] Trial 0 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/dbb34cdee13246a899d1d963a34c5323\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n",
      "🏃 View run GPVAE_Optuna_Study(2) at: http://localhost:5000/#/experiments/832352739106302318/runs/1c25fd28eb544ff5ab730d7c69e4c280\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mae_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 360\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[39mwith\u001b[39;00m mlflow\u001b[39m.\u001b[39mstart_run(run_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGPVAE_Optuna_Study(2)\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m parent_run:\n\u001b[1;32m    359\u001b[0m     study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 360\u001b[0m     study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\n\u001b[1;32m    362\u001b[0m     best_params \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_trial\u001b[39m.\u001b[39mparams\n\u001b[1;32m    363\u001b[0m     best_value \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_trial\u001b[39m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     _optimize(\n\u001b[1;32m    476\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    477\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    478\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    479\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    480\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    481\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    482\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    483\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    484\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    485\u001b[0m     )\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         _optimize_sequential(\n\u001b[1;32m     64\u001b[0m             study,\n\u001b[1;32m     65\u001b[0m             func,\n\u001b[1;32m     66\u001b[0m             n_trials,\n\u001b[1;32m     67\u001b[0m             timeout,\n\u001b[1;32m     68\u001b[0m             catch,\n\u001b[1;32m     69\u001b[0m             callbacks,\n\u001b[1;32m     70\u001b[0m             gc_after_trial,\n\u001b[1;32m     71\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     72\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     73\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     74\u001b[0m         )\n\u001b[1;32m     75\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    161\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    198\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[17], line 344\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    341\u001b[0m avg_mae \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(mae_per_feature)\n\u001b[1;32m    342\u001b[0m \u001b[39m#avg_rmse=np.mean(rmse_per_feature)\u001b[39;00m\n\u001b[0;32m--> 344\u001b[0m mlflow\u001b[39m.\u001b[39mlog_metric(\u001b[39m\"\u001b[39m\u001b[39mavg_mae\u001b[39m\u001b[39m\"\u001b[39m, np\u001b[39m.\u001b[39mmean(mae_list))\n\u001b[1;32m    345\u001b[0m mlflow\u001b[39m.\u001b[39mlog_metric(\u001b[39m\"\u001b[39m\u001b[39mavg_rmse\u001b[39m\u001b[39m\"\u001b[39m, np\u001b[39m.\u001b[39mmean(rmse_list))\n\u001b[1;32m    347\u001b[0m trial\u001b[39m.\u001b[39mset_user_attr(\u001b[39m\"\u001b[39m\u001b[39mmlflow_run_id\u001b[39m\u001b[39m\"\u001b[39m, run\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mrun_id)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mae_list' is not defined"
     ]
    }
   ],
   "source": [
    "#Import Pypots Library\n",
    "from pypots.optim import Adam\n",
    "from pypots.imputation import GPVAE\n",
    "#from pypots.utils.metrics import calc_mae\n",
    "from pypots.nn.functional import calc_mae\n",
    "\n",
    "\n",
    "import argparse\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import data_insight\n",
    "from data_insight import setup_duckdb\n",
    "from duckdb import DuckDBPyConnection as DuckDB\n",
    "from duckdb import DuckDBPyRelation as Relation\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "from duckdb import DuckDBPyConnection as DuckDB\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna \n",
    "from optuna.visualization import plot_optimization_history\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, Dataset\n",
    "from pygrinder.missing_completely_at_random import mcar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import sensor_imputation_thesis.shared.load_data as load\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "#PatchTST might be an ideal choise if SAITS is too slow \n",
    "\n",
    "##Drop columns with different indexes while loading data.. Or the mean values \n",
    "\n",
    "df=pd.read_parquet(\"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/ny_df_for_pypots.parquet\")\n",
    "\n",
    "len(df)\n",
    "\n",
    "#current length of the dataframe is 119439\n",
    "\n",
    "# Check nan values in each column\n",
    "for col in df.columns:\n",
    "    print(f\"Column {col} has {df[col].isna().sum()} NaN values\")\n",
    "    missing_rate=df[col].isna().sum()/len(df[col])\n",
    "    print(f\"Column {col} has {missing_rate} Missing_rate\")\n",
    "\n",
    "\n",
    "#Try with smaller dataset, size 4000\n",
    "##SAMPLE the percengtage of the dataset, df.sample (averagely pick samples)\n",
    "#not df.sample cuz it will randomly select \n",
    "original_size=len(df)\n",
    "desired_fraction=0.3 #Select data every 3 minutes \n",
    "step=int(1/desired_fraction) #step_size=10 (sample every 10th (3/10) minute)\n",
    "\n",
    "#Systematic sampling: Start at a random offset to avoid bias \n",
    "start=np.random.randint(0,step) #Random start between 0-9\n",
    "df1=df.iloc[start::step].reset_index(drop=True)\n",
    "\n",
    "print(f\"Original size:{len(df)}, Sampled size: {len(df1)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Custom Dataset class\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Data processing code\n",
    "sensor_cols = [col for col in df1.columns if col != \"time\"]\n",
    "data = df1[sensor_cols].values\n",
    "\n",
    "#¤get feature names for printing mae later \n",
    "feature_names=df1[sensor_cols].columns.tolist()\n",
    "\n",
    "## Convert data to 3D arrays of shape n_samples, n_timesteps, n_features, X_ori refers to the original data without missing values \n",
    "## Reconstruct all columns simultaneously  #num_features: 119\n",
    "n_features = data.shape[1]  # exclude the time column\n",
    "n_steps = 20 #60 (was 60 previously) #(TRY TO CHANGE HERE)  # # window length, 1440 steps = 24 hours of 1-minute data, but here is revised to 60 again\n",
    "#total_elements = data.shape[0] * data.shape[1]\n",
    "n_samples = data.shape[0] // n_steps \n",
    "\n",
    "\n",
    "\n",
    "# Reshape to (n_samples // n_steps, n_steps, n_features)\n",
    "#data_reshaped = data.reshape((n_samples, n_steps, n_features))\n",
    "data_reshaped=data[:n_samples*n_steps].reshape(n_samples,n_steps,n_features)\n",
    "print(f\"Reshaped data:{data.shape}\")\n",
    "\n",
    "#Split into train, test, val, fit scaler only on the train set (prevent data leakage)\n",
    "\n",
    "#train_size = int(0.6 * len(data))\n",
    "#val_size = int(0.2 * len(data))\n",
    "#test_size = len(data) - train_size - val_size\n",
    "\n",
    "#train_data = data_reshaped[:train_size]\n",
    "#val_data = data_reshaped[train_size:train_size + val_size]\n",
    "#test_data= data_reshaped[train_size + val_size:]\n",
    "\n",
    "\n",
    "#Apply time series split \n",
    "#Split into train(60%), val(20%), and test (20%)\n",
    "train_data, temp_data=train_test_split(data_reshaped,test_size=0.4,shuffle=True)\n",
    "val_data, test_data=train_test_split(temp_data, test_size=0.5, shuffle=False)\n",
    "\n",
    "##Normalization is important because of the nature of mse calculation of saits, columns with large \n",
    "#values dominate the loss, making metrics meaningless. SAITS computes MSE/MAE column-wise and averages \n",
    "#them across all columns \n",
    "#  Apply minmax scaler here \n",
    "#normalize each feature independently\n",
    "scalers={}\n",
    "\n",
    "\n",
    "#train_scaled = np.zeros_like(data_reshaped[train_size])  # Initialize the normalized data array\n",
    "#val_scaled=np.zeros_like(data_reshaped[train_size:train_size + val_size])\n",
    "#test_scaled=np.zeros_like(data_reshaped[train_size + val_size:])\n",
    "\n",
    "train_scaled = np.zeros_like(train_data)\n",
    "val_scaled = np.zeros_like(val_data)\n",
    "test_scaled = np.zeros_like(test_data)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(data_reshaped.shape[2]):\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1)) #changed to -1,1\n",
    "    # Flatten timesteps and samples for scaling\n",
    "    train_scaled[:, :, i] = scaler.fit_transform(train_data[:, :, i].reshape(-1, 1)).reshape(train_data.shape[0], train_data.shape[1])\n",
    "    val_scaled[:, :, i] = scaler.transform(val_data[:, :, i].reshape(-1, 1)).reshape(val_data.shape[0], val_data.shape[1])\n",
    "    test_scaled[:, :, i] = scaler.transform(test_data[:, :, i].reshape(-1, 1)).reshape(test_data.shape[0], test_data.shape[1])\n",
    "    scalers[i] = scaler  # Save scalers to inverse-transform later\n",
    "\n",
    "#Inverse Scale\n",
    "def inverse_scale(imputation, scalers):\n",
    "    n_samples, n_timesteps, n_features = imputation.shape\n",
    "    imputation_denorm = np.empty_like(imputation)\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        reshaped = imputation[:, :, i].reshape(-1, 1)\n",
    "        inversed = scalers[i].inverse_transform(reshaped)\n",
    "        imputation_denorm[:, :, i] = inversed.reshape(n_samples, n_timesteps)\n",
    "    \n",
    "    return imputation_denorm\n",
    "\n",
    "\n",
    "\n",
    "#Optional: Artificially mask. Mask 20% of the data (MIT part). Try masking 30% here \n",
    "def mcar_f(X, mask_ratio=0.3):\n",
    "    \"\"\"Apply MCAR only to observed values.\"\"\"\n",
    "    observed_mask=~np.isnan(X) #find observed positions\n",
    "    artificial_mask=mcar(X,mask_ratio).astype(bool) #generate MCAR mask, cast to boolean\n",
    "    #combine masks \n",
    "    combined_mask=observed_mask & artificial_mask\n",
    "\n",
    "    #Apply masking\n",
    "    X_masked=X.copy()\n",
    "    X_masked[combined_mask]=np.nan\n",
    "    return X_masked,combined_mask\n",
    "\n",
    "\n",
    "#Use mcar on validation data \n",
    "val_X_masked, val_mask =mcar_f(val_scaled)\n",
    "val_X_ori=val_scaled.copy() \n",
    "\n",
    "test_X_masked, test_mask =mcar_f(test_scaled)\n",
    "test_X_ori=test_scaled.copy() \n",
    "\n",
    "\n",
    "#?? Problem: Can't have the best input for testing\n",
    "#1.Create synthetic test_data cuz if I drop nan values for test set, there's basically nothing left\n",
    "#synthetic_data=np.random.randn(n_samples,n_steps,n_features)\n",
    "#test_X_masked,test_mask=mcar_f(synthetic_data)\n",
    "#test_X_ori=synthetic_data.copy() #Ground truth\n",
    "\n",
    "# 2, Ensure no NaN values in synthetic data\n",
    "#test_X_masked = np.nan_to_num(test_X_masked, nan=np.nanmean(test_X_masked))\n",
    "#test_X_ori = np.nan_to_num(test_X_ori, nan=np.nanmean(test_X_ori))\n",
    "\n",
    "\n",
    "\n",
    "class Config:\n",
    "    no_cuda = False\n",
    "    no_mps = False\n",
    "    seed = 1\n",
    "\n",
    "args=Config()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "use_mps = not args.no_mps and torch.backends.mps.is_available()\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "\n",
    "if args.cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA\")\n",
    "elif use_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "train_scaled = torch.tensor(train_scaled, dtype=torch.float32)\n",
    "val_X_masked = torch.tensor(val_X_masked, dtype=torch.float32)\n",
    "val_X_ori = torch.tensor(val_X_ori, dtype=torch.float32)\n",
    "\n",
    "train_scaled = train_scaled.to(device)\n",
    "val_X_masked = val_X_masked.to(device)\n",
    "val_X_ori = val_X_ori.to(device)\n",
    "\n",
    "\n",
    "#MLflow set up\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "mlflow.set_experiment(\"GP_VAE_2\")\n",
    "\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"lr\": trial.suggest_float(\"lr\", 1e-4, 1e-3, log=True),\n",
    "        \"epochs\": trial.suggest_int(\"epochs\", 10, 50),\n",
    "        \"batch_size\": trial.suggest_int(\"batch_size\", 32, 128, step=32),\n",
    "        \"length_scale\": trial.suggest_float(\"length_scale\",0.5,5.0),\n",
    "        \"beta\": trial.suggest_float(\"beta\",0.1,1.0)\n",
    " }\n",
    "\n",
    "    with mlflow.start_run(run_name=\"GP-VAE-Trial\", nested=True) as run:\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        gp_vae = GPVAE(\n",
    "            n_steps=data_reshaped.shape[1],\n",
    "            n_features=data_reshaped.shape[2],\n",
    "            latent_size=37, #should be the latent dimensions \n",
    "            encoder_sizes=(128,128), #should I change it here too?\n",
    "            decoder_sizes=(256,256), #should I change the model size?\n",
    "            kernel=\"cauchy\",\n",
    "            beta=params[\"beta\"], #The weight of KL divergence in ELBO\n",
    "            M=1,  #The number of Monte Carlo samples for ELBO estimation during training.\n",
    "            K=1,  #The number of importance weights for IWAE model training loss.\n",
    "            sigma=1.005, # The scale parameter for a kernel function\n",
    "            length_scale=params[\"length_scale\"], #The length scale parameter for a kernel function\n",
    "            kernel_scales=1, #The number of different length scales over latent space dimensions\n",
    "            window_size=24,  # Window size for the inference CNN.\n",
    "            batch_size=params[\"batch_size\"],\n",
    "            # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "            epochs=params[\"epochs\"],\n",
    "            # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "            # You can leave it to defualt as None to disable early stopping.\n",
    "            patience=3,\n",
    "            # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "            # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "            optimizer=Adam(lr=params[\"lr\"]),\n",
    "            # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "            # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "            # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "            num_workers=0,\n",
    "            # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "            # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "            device=device,\n",
    "            # set the path for saving tensorboard and trained model files \n",
    "            saving_path=\"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model\",\n",
    "            # only save the best model after training finished.\n",
    "            # You can also set it as \"better\" to save models performing better ever during training.\n",
    "            model_saving_strategy=\"best\",\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
    "        gp_vae.fit(train_set={\"X\": train_scaled}, val_set={\"X\": val_X_masked, \"X_ori\": val_X_ori})\n",
    "        gp_vae_results = gp_vae.predict({\"X\": test_X_masked}, n_sampling_times=2)\n",
    "        gp_vae_imputation = gp_vae_results[\"imputation\"]\n",
    "\n",
    "        print(f\"The shape of gp_vae_imputation is {gp_vae_imputation.shape}\")\n",
    "\n",
    "        # for error calculation, we need to take the mean value of the multiple samplings for each data sample\n",
    "        mean_gp_vae_imputation = gp_vae_imputation.mean(axis=1)\n",
    "\n",
    "        test_imputation_denorm = inverse_scale(mean_gp_vae_imputation, scalers)\n",
    "        test_ori_denorm = inverse_scale(test_X_ori, scalers)\n",
    "\n",
    "\n",
    "         # Calculate metrics\n",
    "        mae_per_feature = []\n",
    "        rmse_per_feature=[]\n",
    "        percentage_mae_per_feature = []\n",
    "\n",
    "        for i in range(n_features):\n",
    "            imputation_i = test_imputation_denorm[:, :, i]\n",
    "            ground_truth_i = test_ori_denorm[:, :, i]\n",
    "            mask_i = test_mask[:, :, i]\n",
    "            if np.isnan(imputation_i).any() or np.isnan(ground_truth_i).any():\n",
    "                continue\n",
    "            mae_i = calc_mae(imputation_i, ground_truth_i, mask_i)\n",
    "            mae_per_feature.append(mae_i)\n",
    "            rmse_i = np.sqrt(mean_squared_error(imputation_i, ground_truth_i))\n",
    "            rmse_per_feature.append(rmse_i)\n",
    "\n",
    "            #Calculate the original standard deviation for the feature\n",
    "            std_dev_i = np.std(ground_truth_i[mask_i == 1])\n",
    "             # Calculate the percentage of MAE relative to the standard deviation   \n",
    "            if std_dev_i != 0:\n",
    "                percentage_mae_i = (mae_i / std_dev_i) * 100\n",
    "                percentage_mae_per_feature.append(percentage_mae_i)\n",
    "            else:\n",
    "                 percentage_mae_i = float('inf')\n",
    "            \n",
    "            mlflow.log_metric(f\"MAE_{feature_names[i]}\", mae_i)\n",
    "            mlflow.log_metric(f\"RMSE_{feature_names[i]}\",rmse_i)\n",
    "            mlflow.log_metric(f\"Percentage_MAE_{feature_names[i]}\", percentage_mae_i)\n",
    "\n",
    "        avg_mae = np.mean(mae_per_feature)\n",
    "        #avg_rmse=np.mean(rmse_per_feature)\n",
    "       \n",
    "        mlflow.log_metric(\"avg_mae\", np.mean(mae_list))\n",
    "        mlflow.log_metric(\"avg_rmse\", np.mean(rmse_list))\n",
    "\n",
    "        trial.set_user_attr(\"mlflow_run_id\", run.info.run_id)\n",
    "\n",
    "        return avg_mae\n",
    "\n",
    "    print(\"MAE per feature:\", mae_per_feature)\n",
    "    print(\"RMSE per feature\",rmse_per_feature)\n",
    "    print(\"Percentage MAE per feature:\", percentage_mae_per_feature)\n",
    "   \n",
    "\n",
    "# Run Optuna study\n",
    "mlflow.set_experiment(\"GP-VAE-2\")\n",
    "with mlflow.start_run(run_name=\"GPVAE_Optuna_Study(2)\") as parent_run:\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "\n",
    "    best_params = study.best_trial.params\n",
    "    best_value = study.best_trial.value\n",
    "    best_run_id = study.best_trial.user_attrs[\"mlflow_run_id\"]\n",
    "\n",
    "\n",
    "    \n",
    "    # Log best parameters\n",
    "    mlflow.log_params(best_params)\n",
    "\n",
    "    # Log best metric(s)\n",
    "    mlflow.log_metric(\"best_objective_value\", best_value)\n",
    "    mlflow.log_param(\"best_run_id\", best_run_id)\n",
    "\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Best Objective Value:\", best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f93e5d-3023-42d8-a6d0-6ebc8b47bc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column time has 0 NaN values\n",
      "Column time has 0.0 Missing_rate\n",
      "Column fr_eng has 0 NaN values\n",
      "Column fr_eng has 0.0 Missing_rate\n",
      "Column te_exh_cyl_out__0 has 0 NaN values\n",
      "Column te_exh_cyl_out__0 has 0.0 Missing_rate\n",
      "Column pd_air_ic__0 has 0 NaN values\n",
      "Column pd_air_ic__0 has 0.0 Missing_rate\n",
      "Column pr_exh_turb_out__0 has 316581 NaN values\n",
      "Column pr_exh_turb_out__0 has 1.0 Missing_rate\n",
      "Column te_air_ic_out__0 has 0 NaN values\n",
      "Column te_air_ic_out__0 has 0.0 Missing_rate\n",
      "Column te_seawater has 0 NaN values\n",
      "Column te_seawater has 0.0 Missing_rate\n",
      "Column te_air_comp_in_a__0 has 316581 NaN values\n",
      "Column te_air_comp_in_a__0 has 1.0 Missing_rate\n",
      "Column te_air_comp_in_b__0 has 316581 NaN values\n",
      "Column te_air_comp_in_b__0 has 1.0 Missing_rate\n",
      "Column fr_tc__0 has 316581 NaN values\n",
      "Column fr_tc__0 has 1.0 Missing_rate\n",
      "Column pr_baro has 0 NaN values\n",
      "Column pr_baro has 0.0 Missing_rate\n",
      "Column pd_air_ic__0_1 has 0 NaN values\n",
      "Column pd_air_ic__0_1 has 0.0 Missing_rate\n",
      "Column pr_exh_rec has 0 NaN values\n",
      "Column pr_exh_rec has 0.0 Missing_rate\n",
      "Column te_exh_turb_in__0 has 316581 NaN values\n",
      "Column te_exh_turb_in__0 has 1.0 Missing_rate\n",
      "Column te_exh_turb_out__0 has 316581 NaN values\n",
      "Column te_exh_turb_out__0 has 1.0 Missing_rate\n",
      "Column bo_aux_blower_running has 0 NaN values\n",
      "Column bo_aux_blower_running has 0.0 Missing_rate\n",
      "Column re_eng_load has 426 NaN values\n",
      "Column re_eng_load has 0.0013456271854596453 Missing_rate\n",
      "Column pr_air_scav_ecs has 0 NaN values\n",
      "Column pr_air_scav_ecs has 0.0 Missing_rate\n",
      "Column pr_air_scav has 0 NaN values\n",
      "Column pr_air_scav has 0.0 Missing_rate\n",
      "Column te_air_scav_rec has 0 NaN values\n",
      "Column te_air_scav_rec has 0.0 Missing_rate\n",
      "Column te_air_ic_out__0_1 has 0 NaN values\n",
      "Column te_air_ic_out__0_1 has 0.0 Missing_rate\n",
      "Column pr_cyl_comp__0 has 426 NaN values\n",
      "Column pr_cyl_comp__0 has 0.0013456271854596453 Missing_rate\n",
      "Column pr_cyl_max__0 has 426 NaN values\n",
      "Column pr_cyl_max__0 has 0.0013456271854596453 Missing_rate\n",
      "Column se_mip__0 has 426 NaN values\n",
      "Column se_mip__0 has 0.0013456271854596453 Missing_rate\n",
      "Column te_exh_cyl_out__0_1 has 0 NaN values\n",
      "Column te_exh_cyl_out__0_1 has 0.0 Missing_rate\n",
      "Column fr_eng_setpoint has 0 NaN values\n",
      "Column fr_eng_setpoint has 0.0 Missing_rate\n",
      "Column te_air_scav_rec_iso has 126055 NaN values\n",
      "Column te_air_scav_rec_iso has 0.3981761381763277 Missing_rate\n",
      "Column pr_cyl_max_mv_iso has 128557 NaN values\n",
      "Column pr_cyl_max_mv_iso has 0.40607932882895686 Missing_rate\n",
      "Column pr_cyl_comp_mv_iso has 128557 NaN values\n",
      "Column pr_cyl_comp_mv_iso has 0.40607932882895686 Missing_rate\n",
      "Column fr_eng_ecs has 0 NaN values\n",
      "Column fr_eng_ecs has 0.0 Missing_rate\n",
      "Column pr_air_scav_iso has 128557 NaN values\n",
      "Column pr_air_scav_iso has 0.40607932882895686 Missing_rate\n",
      "Column engine_type_G80ME-C9.5-GI-LPSCR has 0 NaN values\n",
      "Column engine_type_G80ME-C9.5-GI-LPSCR has 0.0 Missing_rate\n",
      "Original size:316581, Sampled size: 105527\n",
      "Reshaped data:(105527, 31)\n",
      "CUDA available: True\n",
      "Using CUDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "[I 2025-05-20 00:46:12,028] A new study created in memory with name: no-name-4d0fbc41-c27f-47d5-a555-65015c7c158d\n",
      "2025-05-20 00:46:12 [INFO]: Using the given device: cuda\n",
      "2025-05-20 00:46:12 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004612\n",
      "2025-05-20 00:46:12 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004612/tensorboard\n",
      "2025-05-20 00:46:12 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n",
      "2025-05-20 00:46:12 [INFO]: Epoch 001 - training loss (default): 1769.4689, validation loss: 1694.7700\n",
      "2025-05-20 00:46:13 [INFO]: Epoch 002 - training loss (default): 985.2781, validation loss: 1635.8644\n",
      "2025-05-20 00:46:13 [INFO]: Epoch 003 - training loss (default): 903.7158, validation loss: 1615.7388\n",
      "2025-05-20 00:46:14 [INFO]: Epoch 004 - training loss (default): 898.8576, validation loss: 1608.0842\n",
      "2025-05-20 00:46:15 [INFO]: Epoch 005 - training loss (default): 896.9738, validation loss: 1602.7625\n",
      "2025-05-20 00:46:15 [INFO]: Epoch 006 - training loss (default): 895.5933, validation loss: 1598.0637\n",
      "2025-05-20 00:46:16 [INFO]: Epoch 007 - training loss (default): 894.9450, validation loss: 1593.7680\n",
      "2025-05-20 00:46:17 [INFO]: Epoch 008 - training loss (default): 894.3920, validation loss: 1589.8751\n",
      "2025-05-20 00:46:17 [INFO]: Epoch 009 - training loss (default): 894.0252, validation loss: 1586.2939\n",
      "2025-05-20 00:46:18 [INFO]: Epoch 010 - training loss (default): 893.7055, validation loss: 1582.9624\n",
      "2025-05-20 00:46:19 [INFO]: Epoch 011 - training loss (default): 893.3984, validation loss: 1579.8869\n",
      "2025-05-20 00:46:19 [INFO]: Epoch 012 - training loss (default): 892.9622, validation loss: 1577.0419\n",
      "2025-05-20 00:46:20 [INFO]: Epoch 013 - training loss (default): 892.8523, validation loss: 1574.2243\n",
      "2025-05-20 00:46:21 [INFO]: Epoch 014 - training loss (default): 892.4699, validation loss: 1571.6885\n",
      "2025-05-20 00:46:21 [INFO]: Epoch 015 - training loss (default): 892.3121, validation loss: 1569.2081\n",
      "2025-05-20 00:46:22 [INFO]: Epoch 016 - training loss (default): 892.1193, validation loss: 1567.0264\n",
      "2025-05-20 00:46:23 [INFO]: Epoch 017 - training loss (default): 891.9588, validation loss: 1564.9038\n",
      "2025-05-20 00:46:23 [INFO]: Epoch 018 - training loss (default): 891.7639, validation loss: 1562.8533\n",
      "2025-05-20 00:46:24 [INFO]: Epoch 019 - training loss (default): 891.6738, validation loss: 1560.9370\n",
      "2025-05-20 00:46:25 [INFO]: Epoch 020 - training loss (default): 891.5710, validation loss: 1559.0569\n",
      "2025-05-20 00:46:25 [INFO]: Epoch 021 - training loss (default): 891.4383, validation loss: 1557.3491\n",
      "2025-05-20 00:46:26 [INFO]: Epoch 022 - training loss (default): 891.2986, validation loss: 1555.7144\n",
      "2025-05-20 00:46:27 [INFO]: Epoch 023 - training loss (default): 891.2534, validation loss: 1554.1209\n",
      "2025-05-20 00:46:27 [INFO]: Epoch 024 - training loss (default): 891.1140, validation loss: 1552.6946\n",
      "2025-05-20 00:46:28 [INFO]: Epoch 025 - training loss (default): 891.0463, validation loss: 1551.2666\n",
      "2025-05-20 00:46:29 [INFO]: Epoch 026 - training loss (default): 890.9641, validation loss: 1549.8434\n",
      "2025-05-20 00:46:29 [INFO]: Epoch 027 - training loss (default): 890.8742, validation loss: 1548.3436\n",
      "2025-05-20 00:46:30 [INFO]: Epoch 028 - training loss (default): 890.8489, validation loss: 1546.9022\n",
      "2025-05-20 00:46:31 [INFO]: Epoch 029 - training loss (default): 890.7116, validation loss: 1545.5475\n",
      "2025-05-20 00:46:31 [INFO]: Epoch 030 - training loss (default): 890.7611, validation loss: 1544.4453\n",
      "2025-05-20 00:46:31 [INFO]: Finished training. The best model is from epoch#30.\n",
      "2025-05-20 00:46:31 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004612/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-20 00:46:32,375] Trial 0 finished with value: 7681.086558596051 and parameters: {'lr': 0.00035745162270512613, 'epochs': 30, 'batch_size': 128, 'length_scale': 2.0077899412830975, 'beta': 0.7618304568241496}. Best is trial 0 with value: 7681.086558596051.\n",
      "2025-05-20 00:46:32 [INFO]: Using the given device: cuda\n",
      "2025-05-20 00:46:32 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004632\n",
      "2025-05-20 00:46:32 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004632/tensorboard\n",
      "2025-05-20 00:46:32 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/a0bb5ff8807f4059a8d3e3bff6fd3cb4\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 00:46:33 [INFO]: Epoch 001 - training loss (default): 729.5931, validation loss: 401.3451\n",
      "2025-05-20 00:46:33 [INFO]: Epoch 002 - training loss (default): 625.8241, validation loss: 395.7056\n",
      "2025-05-20 00:46:34 [INFO]: Epoch 003 - training loss (default): 617.7882, validation loss: 398.6986\n",
      "2025-05-20 00:46:34 [INFO]: Epoch 004 - training loss (default): 614.0817, validation loss: 400.5087\n",
      "2025-05-20 00:46:35 [INFO]: Epoch 005 - training loss (default): 611.5978, validation loss: 401.1664\n",
      "2025-05-20 00:46:35 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-05-20 00:46:35 [INFO]: Finished training. The best model is from epoch#2.\n",
      "2025-05-20 00:46:35 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004632/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-20 00:46:36,037] Trial 1 finished with value: 7896.3234836858055 and parameters: {'lr': 0.0009253207967568482, 'epochs': 15, 'batch_size': 128, 'length_scale': 1.391635180263604, 'beta': 0.8536322035686299}. Best is trial 0 with value: 7681.086558596051.\n",
      "2025-05-20 00:46:36 [INFO]: Using the given device: cuda\n",
      "2025-05-20 00:46:36 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004636\n",
      "2025-05-20 00:46:36 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004636/tensorboard\n",
      "2025-05-20 00:46:36 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/e276d6114fb54f20ab88bd491f4f0e84\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 00:46:36 [INFO]: Epoch 001 - training loss (default): 35626.6787, validation loss: 54946.1786\n",
      "2025-05-20 00:46:37 [INFO]: Epoch 002 - training loss (default): 18566.2784, validation loss: 53376.8956\n",
      "2025-05-20 00:46:38 [INFO]: Epoch 003 - training loss (default): 18473.0830, validation loss: 53134.3871\n",
      "2025-05-20 00:46:38 [INFO]: Epoch 004 - training loss (default): 18466.0904, validation loss: 52986.6506\n",
      "2025-05-20 00:46:39 [INFO]: Epoch 005 - training loss (default): 18462.0286, validation loss: 52842.9027\n",
      "2025-05-20 00:46:40 [INFO]: Epoch 006 - training loss (default): 18459.3339, validation loss: 52710.0497\n",
      "2025-05-20 00:46:40 [INFO]: Epoch 007 - training loss (default): 18457.4564, validation loss: 52575.4563\n",
      "2025-05-20 00:46:41 [INFO]: Epoch 008 - training loss (default): 18455.9654, validation loss: 52450.3782\n",
      "2025-05-20 00:46:42 [INFO]: Epoch 009 - training loss (default): 18454.7935, validation loss: 52335.2809\n",
      "2025-05-20 00:46:42 [INFO]: Epoch 010 - training loss (default): 18453.9118, validation loss: 52221.3036\n",
      "2025-05-20 00:46:43 [INFO]: Epoch 011 - training loss (default): 18453.1560, validation loss: 52118.8118\n",
      "2025-05-20 00:46:44 [INFO]: Epoch 012 - training loss (default): 18452.6114, validation loss: 52014.6143\n",
      "2025-05-20 00:46:44 [INFO]: Epoch 013 - training loss (default): 18451.9489, validation loss: 51920.1676\n",
      "2025-05-20 00:46:45 [INFO]: Epoch 014 - training loss (default): 18451.5023, validation loss: 51825.3047\n",
      "2025-05-20 00:46:46 [INFO]: Epoch 015 - training loss (default): 18451.0723, validation loss: 51736.3647\n",
      "2025-05-20 00:46:46 [INFO]: Epoch 016 - training loss (default): 18450.7766, validation loss: 51652.1197\n",
      "2025-05-20 00:46:47 [INFO]: Epoch 017 - training loss (default): 18450.4445, validation loss: 51570.7685\n",
      "2025-05-20 00:46:48 [INFO]: Epoch 018 - training loss (default): 18450.0629, validation loss: 51490.9425\n",
      "2025-05-20 00:46:49 [INFO]: Epoch 019 - training loss (default): 18449.7576, validation loss: 51415.3366\n",
      "2025-05-20 00:46:49 [INFO]: Epoch 020 - training loss (default): 18449.5118, validation loss: 51338.1747\n",
      "2025-05-20 00:46:50 [INFO]: Epoch 021 - training loss (default): 18449.3864, validation loss: 51269.3668\n",
      "2025-05-20 00:46:51 [INFO]: Epoch 022 - training loss (default): 18449.1190, validation loss: 51200.4993\n",
      "2025-05-20 00:46:51 [INFO]: Epoch 023 - training loss (default): 18448.9376, validation loss: 51134.8271\n",
      "2025-05-20 00:46:52 [INFO]: Epoch 024 - training loss (default): 18448.8626, validation loss: 51070.6506\n",
      "2025-05-20 00:46:52 [INFO]: Finished training. The best model is from epoch#24.\n",
      "2025-05-20 00:46:52 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004636/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-20 00:46:52,895] Trial 2 finished with value: 7862.8214861212355 and parameters: {'lr': 0.00049221691738657, 'epochs': 24, 'batch_size': 96, 'length_scale': 4.5554361322447745, 'beta': 0.7088218812233893}. Best is trial 0 with value: 7681.086558596051.\n",
      "2025-05-20 00:46:52 [INFO]: Using the given device: cuda\n",
      "2025-05-20 00:46:52 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004652\n",
      "2025-05-20 00:46:52 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004652/tensorboard\n",
      "2025-05-20 00:46:52 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/642a52e8aff94c989f2896bd9bd20501\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 00:46:53 [INFO]: Epoch 001 - training loss (default): 740.3638, validation loss: 281.3164\n",
      "2025-05-20 00:46:54 [INFO]: Epoch 002 - training loss (default): 593.7057, validation loss: 275.4472\n",
      "2025-05-20 00:46:55 [INFO]: Epoch 003 - training loss (default): 553.5827, validation loss: 272.9328\n",
      "2025-05-20 00:46:56 [INFO]: Epoch 004 - training loss (default): 547.3174, validation loss: 271.7761\n",
      "2025-05-20 00:46:56 [INFO]: Epoch 005 - training loss (default): 545.1988, validation loss: 271.1328\n",
      "2025-05-20 00:46:57 [INFO]: Epoch 006 - training loss (default): 543.6671, validation loss: 270.7908\n",
      "2025-05-20 00:46:58 [INFO]: Epoch 007 - training loss (default): 542.7140, validation loss: 270.5605\n",
      "2025-05-20 00:46:59 [INFO]: Epoch 008 - training loss (default): 542.0782, validation loss: 270.3504\n",
      "2025-05-20 00:47:00 [INFO]: Epoch 009 - training loss (default): 541.5363, validation loss: 270.1222\n",
      "2025-05-20 00:47:01 [INFO]: Epoch 010 - training loss (default): 541.1547, validation loss: 269.8547\n",
      "2025-05-20 00:47:01 [INFO]: Epoch 011 - training loss (default): 540.8569, validation loss: 269.5206\n",
      "2025-05-20 00:47:02 [INFO]: Epoch 012 - training loss (default): 540.5352, validation loss: 269.1908\n",
      "2025-05-20 00:47:03 [INFO]: Epoch 013 - training loss (default): 540.2632, validation loss: 268.9102\n",
      "2025-05-20 00:47:04 [INFO]: Epoch 014 - training loss (default): 540.0340, validation loss: 268.6340\n",
      "2025-05-20 00:47:05 [INFO]: Epoch 015 - training loss (default): 539.7085, validation loss: 268.3849\n",
      "2025-05-20 00:47:05 [INFO]: Epoch 016 - training loss (default): 539.6882, validation loss: 268.1614\n",
      "2025-05-20 00:47:06 [INFO]: Epoch 017 - training loss (default): 539.5049, validation loss: 267.9657\n",
      "2025-05-20 00:47:07 [INFO]: Epoch 018 - training loss (default): 539.4003, validation loss: 267.7984\n",
      "2025-05-20 00:47:08 [INFO]: Epoch 019 - training loss (default): 539.0707, validation loss: 267.6452\n",
      "2025-05-20 00:47:09 [INFO]: Epoch 020 - training loss (default): 539.0824, validation loss: 267.4985\n",
      "2025-05-20 00:47:10 [INFO]: Epoch 021 - training loss (default): 538.9329, validation loss: 267.3598\n",
      "2025-05-20 00:47:10 [INFO]: Finished training. The best model is from epoch#21.\n",
      "2025-05-20 00:47:10 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004652/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-20 00:47:10,664] Trial 3 finished with value: 7742.100039130347 and parameters: {'lr': 0.0001096864767395611, 'epochs': 21, 'batch_size': 64, 'length_scale': 1.4772926481933586, 'beta': 0.45541287831529687}. Best is trial 0 with value: 7681.086558596051.\n",
      "2025-05-20 00:47:10 [INFO]: Using the given device: cuda\n",
      "2025-05-20 00:47:10 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004710\n",
      "2025-05-20 00:47:10 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004710/tensorboard\n",
      "2025-05-20 00:47:10 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/4af395aea1af42af82f40a40d0f9a076\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 00:47:11 [INFO]: Epoch 001 - training loss (default): 568.2737, validation loss: 109.5377\n",
      "2025-05-20 00:47:12 [INFO]: Epoch 002 - training loss (default): 493.4547, validation loss: 107.7137\n",
      "2025-05-20 00:47:13 [INFO]: Epoch 003 - training loss (default): 481.3378, validation loss: 107.1298\n",
      "2025-05-20 00:47:14 [INFO]: Epoch 004 - training loss (default): 479.7130, validation loss: 106.7201\n",
      "2025-05-20 00:47:14 [INFO]: Epoch 005 - training loss (default): 478.8173, validation loss: 106.3582\n",
      "2025-05-20 00:47:15 [INFO]: Epoch 006 - training loss (default): 478.2687, validation loss: 106.0288\n",
      "2025-05-20 00:47:16 [INFO]: Epoch 007 - training loss (default): 477.7586, validation loss: 105.7073\n",
      "2025-05-20 00:47:17 [INFO]: Epoch 008 - training loss (default): 477.3618, validation loss: 105.3650\n",
      "2025-05-20 00:47:18 [INFO]: Epoch 009 - training loss (default): 476.9641, validation loss: 105.0312\n",
      "2025-05-20 00:47:18 [INFO]: Epoch 010 - training loss (default): 476.2958, validation loss: 104.6898\n",
      "2025-05-20 00:47:19 [INFO]: Epoch 011 - training loss (default): 475.6294, validation loss: 104.3402\n",
      "2025-05-20 00:47:20 [INFO]: Epoch 012 - training loss (default): 475.0702, validation loss: 103.9906\n",
      "2025-05-20 00:47:21 [INFO]: Epoch 013 - training loss (default): 474.9309, validation loss: 103.6415\n",
      "2025-05-20 00:47:22 [INFO]: Epoch 014 - training loss (default): 474.5970, validation loss: 103.3272\n",
      "2025-05-20 00:47:22 [INFO]: Epoch 015 - training loss (default): 474.4785, validation loss: 103.0266\n",
      "2025-05-20 00:47:23 [INFO]: Epoch 016 - training loss (default): 474.2706, validation loss: 102.7534\n",
      "2025-05-20 00:47:24 [INFO]: Epoch 017 - training loss (default): 474.1351, validation loss: 102.5204\n",
      "2025-05-20 00:47:25 [INFO]: Epoch 018 - training loss (default): 474.0938, validation loss: 102.2767\n",
      "2025-05-20 00:47:26 [INFO]: Epoch 019 - training loss (default): 473.9575, validation loss: 102.0615\n",
      "2025-05-20 00:47:26 [INFO]: Epoch 020 - training loss (default): 473.9943, validation loss: 101.8505\n",
      "2025-05-20 00:47:27 [INFO]: Epoch 021 - training loss (default): 473.9658, validation loss: 101.6479\n",
      "2025-05-20 00:47:28 [INFO]: Epoch 022 - training loss (default): 473.7506, validation loss: 101.4496\n",
      "2025-05-20 00:47:29 [INFO]: Epoch 023 - training loss (default): 473.8091, validation loss: 101.2538\n",
      "2025-05-20 00:47:30 [INFO]: Epoch 024 - training loss (default): 473.5933, validation loss: 101.0689\n",
      "2025-05-20 00:47:30 [WARNING]: ‼️ Training got interrupted by the user. Exist now ...\n",
      "2025-05-20 00:47:30 [INFO]: Finished training. The best model is from epoch#24.\n",
      "2025-05-20 00:47:30 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004710/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-20 00:47:31,342] Trial 4 finished with value: 7755.536186999624 and parameters: {'lr': 0.00017410500622537273, 'epochs': 25, 'batch_size': 64, 'length_scale': 0.73933055149302, 'beta': 0.6678557067755405}. Best is trial 0 with value: 7681.086558596051.\n",
      "2025-05-20 00:47:31 [INFO]: Using the given device: cuda\n",
      "2025-05-20 00:47:31 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004731\n",
      "2025-05-20 00:47:31 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004731/tensorboard\n",
      "2025-05-20 00:47:31 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/827b357822f6422192b115a4fca5ead4\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 00:47:32 [INFO]: Epoch 001 - training loss (default): 581.5353, validation loss: 165.5222\n",
      "2025-05-20 00:47:32 [INFO]: Epoch 002 - training loss (default): 518.2326, validation loss: 163.1140\n",
      "2025-05-20 00:47:33 [INFO]: Epoch 003 - training loss (default): 514.8347, validation loss: 162.1802\n",
      "2025-05-20 00:47:34 [INFO]: Epoch 004 - training loss (default): 513.6145, validation loss: 161.4055\n",
      "2025-05-20 00:47:34 [INFO]: Epoch 005 - training loss (default): 513.1113, validation loss: 160.7555\n",
      "2025-05-20 00:47:35 [INFO]: Epoch 006 - training loss (default): 512.5496, validation loss: 160.0362\n",
      "2025-05-20 00:47:36 [INFO]: Epoch 007 - training loss (default): 511.9527, validation loss: 159.2276\n",
      "2025-05-20 00:47:36 [INFO]: Epoch 008 - training loss (default): 511.3680, validation loss: 158.3934\n",
      "2025-05-20 00:47:37 [INFO]: Epoch 009 - training loss (default): 511.0814, validation loss: 157.7014\n",
      "2025-05-20 00:47:38 [INFO]: Epoch 010 - training loss (default): 510.5827, validation loss: 157.0253\n",
      "2025-05-20 00:47:38 [INFO]: Epoch 011 - training loss (default): 510.2035, validation loss: 156.4274\n",
      "2025-05-20 00:47:39 [INFO]: Epoch 012 - training loss (default): 509.9781, validation loss: 155.8822\n",
      "2025-05-20 00:47:40 [INFO]: Epoch 013 - training loss (default): 509.5861, validation loss: 155.3525\n",
      "2025-05-20 00:47:40 [INFO]: Epoch 014 - training loss (default): 509.3518, validation loss: 154.8664\n",
      "2025-05-20 00:47:41 [INFO]: Epoch 015 - training loss (default): 509.1359, validation loss: 154.4499\n",
      "2025-05-20 00:47:42 [INFO]: Epoch 016 - training loss (default): 509.0654, validation loss: 154.0802\n",
      "2025-05-20 00:47:43 [INFO]: Epoch 017 - training loss (default): 508.8592, validation loss: 153.6503\n",
      "2025-05-20 00:47:43 [INFO]: Epoch 018 - training loss (default): 508.8283, validation loss: 153.2881\n",
      "2025-05-20 00:47:44 [INFO]: Epoch 019 - training loss (default): 508.7795, validation loss: 152.9185\n",
      "2025-05-20 00:47:45 [INFO]: Epoch 020 - training loss (default): 508.6792, validation loss: 152.5734\n",
      "2025-05-20 00:47:45 [INFO]: Epoch 021 - training loss (default): 508.7604, validation loss: 152.2458\n",
      "2025-05-20 00:47:46 [INFO]: Epoch 022 - training loss (default): 508.7734, validation loss: 151.9640\n",
      "2025-05-20 00:47:47 [INFO]: Epoch 023 - training loss (default): 508.7271, validation loss: 151.6713\n",
      "2025-05-20 00:47:47 [INFO]: Epoch 024 - training loss (default): 508.6292, validation loss: 151.3520\n",
      "2025-05-20 00:47:48 [INFO]: Epoch 025 - training loss (default): 508.7120, validation loss: 151.0178\n",
      "2025-05-20 00:47:48 [INFO]: Finished training. The best model is from epoch#25.\n",
      "2025-05-20 00:47:48 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004731/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-20 00:47:48,877] Trial 5 finished with value: 7661.802608801183 and parameters: {'lr': 0.0007334929012456116, 'epochs': 25, 'batch_size': 96, 'length_scale': 0.9672766126928345, 'beta': 0.77408893354034}. Best is trial 5 with value: 7661.802608801183.\n",
      "2025-05-20 00:47:48 [INFO]: Using the given device: cuda\n",
      "2025-05-20 00:47:48 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004748\n",
      "2025-05-20 00:47:48 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004748/tensorboard\n",
      "2025-05-20 00:47:48 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/c9fcc321d3b64eb79486182c9914367b\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 00:47:50 [INFO]: Epoch 001 - training loss (default): 534.9839, validation loss: 132.9820\n",
      "2025-05-20 00:47:51 [INFO]: Epoch 002 - training loss (default): 496.3154, validation loss: 131.8449\n",
      "2025-05-20 00:47:52 [INFO]: Epoch 003 - training loss (default): 494.8530, validation loss: 131.0219\n",
      "2025-05-20 00:47:53 [INFO]: Epoch 004 - training loss (default): 494.0383, validation loss: 130.3757\n",
      "2025-05-20 00:47:54 [INFO]: Epoch 005 - training loss (default): 493.4082, validation loss: 129.9068\n",
      "2025-05-20 00:47:56 [INFO]: Epoch 006 - training loss (default): 492.8038, validation loss: 129.4393\n",
      "2025-05-20 00:47:57 [INFO]: Epoch 007 - training loss (default): 492.4928, validation loss: 129.0161\n",
      "2025-05-20 00:47:58 [INFO]: Epoch 008 - training loss (default): 492.1489, validation loss: 128.7140\n",
      "2025-05-20 00:47:59 [INFO]: Epoch 009 - training loss (default): 491.7445, validation loss: 128.3860\n",
      "2025-05-20 00:48:00 [INFO]: Epoch 010 - training loss (default): 491.0345, validation loss: 128.0431\n",
      "2025-05-20 00:48:02 [INFO]: Epoch 011 - training loss (default): 490.5502, validation loss: 127.7345\n",
      "2025-05-20 00:48:03 [INFO]: Epoch 012 - training loss (default): 490.4380, validation loss: 127.5246\n",
      "2025-05-20 00:48:04 [INFO]: Epoch 013 - training loss (default): 490.1686, validation loss: 127.2426\n",
      "2025-05-20 00:48:05 [INFO]: Epoch 014 - training loss (default): 490.1603, validation loss: 126.9384\n",
      "2025-05-20 00:48:06 [INFO]: Epoch 015 - training loss (default): 490.1313, validation loss: 126.6960\n",
      "2025-05-20 00:48:08 [INFO]: Epoch 016 - training loss (default): 490.0692, validation loss: 126.3918\n",
      "2025-05-20 00:48:09 [INFO]: Epoch 017 - training loss (default): 489.9352, validation loss: 126.1167\n",
      "2025-05-20 00:48:10 [INFO]: Epoch 018 - training loss (default): 489.9653, validation loss: 125.8815\n",
      "2025-05-20 00:48:11 [INFO]: Epoch 019 - training loss (default): 489.8731, validation loss: 125.5695\n",
      "2025-05-20 00:48:13 [INFO]: Epoch 020 - training loss (default): 489.9218, validation loss: 125.3070\n",
      "2025-05-20 00:48:14 [INFO]: Epoch 021 - training loss (default): 489.8280, validation loss: 125.0377\n",
      "2025-05-20 00:48:15 [INFO]: Epoch 022 - training loss (default): 489.8343, validation loss: 124.7750\n",
      "2025-05-20 00:48:15 [INFO]: Finished training. The best model is from epoch#22.\n",
      "2025-05-20 00:48:15 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004748/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-20 00:48:16,161] Trial 6 finished with value: 7674.316824349503 and parameters: {'lr': 0.00032159696898646084, 'epochs': 22, 'batch_size': 32, 'length_scale': 0.866207336845062, 'beta': 0.7170308384584969}. Best is trial 5 with value: 7661.802608801183.\n",
      "2025-05-20 00:48:16 [INFO]: Using the given device: cuda\n",
      "2025-05-20 00:48:16 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004816\n",
      "2025-05-20 00:48:16 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004816/tensorboard\n",
      "2025-05-20 00:48:16 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/c03faa9cfad34cd1a1651f347b97eb1e\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 00:48:16 [INFO]: Epoch 001 - training loss (default): 564.2214, validation loss: 82.5144\n",
      "2025-05-20 00:48:17 [INFO]: Epoch 002 - training loss (default): 515.3027, validation loss: 81.7079\n",
      "2025-05-20 00:48:18 [INFO]: Epoch 003 - training loss (default): 487.2850, validation loss: 80.8402\n",
      "2025-05-20 00:48:18 [INFO]: Epoch 004 - training loss (default): 479.8074, validation loss: 80.3979\n",
      "2025-05-20 00:48:19 [INFO]: Epoch 005 - training loss (default): 477.9726, validation loss: 80.1681\n",
      "2025-05-20 00:48:20 [INFO]: Epoch 006 - training loss (default): 476.9682, validation loss: 79.9906\n",
      "2025-05-20 00:48:20 [INFO]: Epoch 007 - training loss (default): 476.1901, validation loss: 79.8403\n",
      "2025-05-20 00:48:21 [INFO]: Epoch 008 - training loss (default): 475.7935, validation loss: 79.7118\n",
      "2025-05-20 00:48:22 [INFO]: Epoch 009 - training loss (default): 475.5791, validation loss: 79.5959\n",
      "2025-05-20 00:48:22 [INFO]: Epoch 010 - training loss (default): 475.3203, validation loss: 79.4917\n",
      "2025-05-20 00:48:23 [INFO]: Epoch 011 - training loss (default): 475.1986, validation loss: 79.4026\n",
      "2025-05-20 00:48:24 [INFO]: Epoch 012 - training loss (default): 474.8825, validation loss: 79.3147\n",
      "2025-05-20 00:48:24 [INFO]: Epoch 013 - training loss (default): 474.7793, validation loss: 79.2422\n",
      "2025-05-20 00:48:25 [INFO]: Epoch 014 - training loss (default): 474.5251, validation loss: 79.1692\n",
      "2025-05-20 00:48:26 [INFO]: Epoch 015 - training loss (default): 474.3965, validation loss: 79.1009\n",
      "2025-05-20 00:48:26 [INFO]: Epoch 016 - training loss (default): 474.2203, validation loss: 79.0416\n",
      "2025-05-20 00:48:27 [INFO]: Epoch 017 - training loss (default): 474.0962, validation loss: 78.9861\n",
      "2025-05-20 00:48:27 [INFO]: Epoch 018 - training loss (default): 473.8889, validation loss: 78.9351\n",
      "2025-05-20 00:48:28 [INFO]: Epoch 019 - training loss (default): 473.6441, validation loss: 78.8911\n",
      "2025-05-20 00:48:29 [INFO]: Epoch 020 - training loss (default): 473.5917, validation loss: 78.8498\n",
      "2025-05-20 00:48:29 [INFO]: Finished training. The best model is from epoch#20.\n",
      "2025-05-20 00:48:29 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004816/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-20 00:48:29,680] Trial 7 finished with value: 7759.442389013492 and parameters: {'lr': 0.00022249164455507314, 'epochs': 20, 'batch_size': 128, 'length_scale': 0.9592040339838777, 'beta': 0.3767376894957277}. Best is trial 5 with value: 7661.802608801183.\n",
      "2025-05-20 00:48:29 [INFO]: Using the given device: cuda\n",
      "2025-05-20 00:48:29 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004829\n",
      "2025-05-20 00:48:29 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004829/tensorboard\n",
      "2025-05-20 00:48:29 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/7f358a3698e740f2b579256bd6bb56be\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 00:48:30 [INFO]: Epoch 001 - training loss (default): 635.7738, validation loss: 206.5086\n",
      "2025-05-20 00:48:31 [INFO]: Epoch 002 - training loss (default): 522.5095, validation loss: 200.7939\n",
      "2025-05-20 00:48:32 [INFO]: Epoch 003 - training loss (default): 507.4184, validation loss: 198.7440\n",
      "2025-05-20 00:48:32 [INFO]: Epoch 004 - training loss (default): 504.4792, validation loss: 197.5117\n",
      "2025-05-20 00:48:33 [INFO]: Epoch 005 - training loss (default): 503.5044, validation loss: 196.5619\n",
      "2025-05-20 00:48:34 [INFO]: Epoch 006 - training loss (default): 502.5220, validation loss: 195.8170\n",
      "2025-05-20 00:48:35 [INFO]: Epoch 007 - training loss (default): 501.8651, validation loss: 195.2048\n",
      "2025-05-20 00:48:36 [INFO]: Epoch 008 - training loss (default): 501.5599, validation loss: 194.6842\n",
      "2025-05-20 00:48:36 [INFO]: Epoch 009 - training loss (default): 501.2877, validation loss: 194.2250\n",
      "2025-05-20 00:48:37 [INFO]: Epoch 010 - training loss (default): 500.9291, validation loss: 193.8119\n",
      "2025-05-20 00:48:38 [INFO]: Epoch 011 - training loss (default): 500.5019, validation loss: 193.4304\n",
      "2025-05-20 00:48:39 [INFO]: Epoch 012 - training loss (default): 500.3842, validation loss: 193.0830\n",
      "2025-05-20 00:48:40 [INFO]: Epoch 013 - training loss (default): 500.1866, validation loss: 192.7744\n",
      "2025-05-20 00:48:40 [INFO]: Epoch 014 - training loss (default): 499.9418, validation loss: 192.4923\n",
      "2025-05-20 00:48:41 [INFO]: Epoch 015 - training loss (default): 499.7890, validation loss: 192.2356\n",
      "2025-05-20 00:48:42 [INFO]: Epoch 016 - training loss (default): 499.6253, validation loss: 191.9583\n",
      "2025-05-20 00:48:43 [INFO]: Epoch 017 - training loss (default): 499.4368, validation loss: 191.7083\n",
      "2025-05-20 00:48:44 [INFO]: Epoch 018 - training loss (default): 499.4092, validation loss: 191.4867\n",
      "2025-05-20 00:48:44 [INFO]: Epoch 019 - training loss (default): 499.2048, validation loss: 191.2542\n",
      "2025-05-20 00:48:45 [INFO]: Epoch 020 - training loss (default): 499.0367, validation loss: 191.0313\n",
      "2025-05-20 00:48:46 [INFO]: Epoch 021 - training loss (default): 499.1365, validation loss: 190.8233\n",
      "2025-05-20 00:48:47 [INFO]: Epoch 022 - training loss (default): 499.0277, validation loss: 190.6155\n",
      "2025-05-20 00:48:48 [INFO]: Epoch 023 - training loss (default): 498.8016, validation loss: 190.4400\n",
      "2025-05-20 00:48:48 [INFO]: Epoch 024 - training loss (default): 499.0414, validation loss: 190.2458\n",
      "2025-05-20 00:48:49 [INFO]: Epoch 025 - training loss (default): 498.9298, validation loss: 190.0698\n",
      "2025-05-20 00:48:50 [INFO]: Epoch 026 - training loss (default): 498.7863, validation loss: 189.8951\n",
      "2025-05-20 00:48:51 [INFO]: Epoch 027 - training loss (default): 498.7246, validation loss: 189.7130\n",
      "2025-05-20 00:48:52 [INFO]: Epoch 028 - training loss (default): 498.7673, validation loss: 189.5687\n",
      "2025-05-20 00:48:52 [INFO]: Epoch 029 - training loss (default): 498.8213, validation loss: 189.4356\n",
      "2025-05-20 00:48:53 [INFO]: Epoch 030 - training loss (default): 498.7682, validation loss: 189.2852\n",
      "2025-05-20 00:48:54 [INFO]: Epoch 031 - training loss (default): 498.6523, validation loss: 189.1432\n",
      "2025-05-20 00:48:55 [INFO]: Epoch 032 - training loss (default): 498.7299, validation loss: 188.9927\n",
      "2025-05-20 00:48:56 [INFO]: Epoch 033 - training loss (default): 498.7239, validation loss: 188.8781\n",
      "2025-05-20 00:48:56 [INFO]: Epoch 034 - training loss (default): 498.6262, validation loss: 188.7331\n",
      "2025-05-20 00:48:57 [INFO]: Epoch 035 - training loss (default): 498.4735, validation loss: 188.6133\n",
      "2025-05-20 00:48:58 [INFO]: Epoch 036 - training loss (default): 498.6036, validation loss: 188.5069\n",
      "2025-05-20 00:48:58 [INFO]: Finished training. The best model is from epoch#36.\n",
      "2025-05-20 00:48:58 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004829/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-20 00:48:59,020] Trial 8 finished with value: 7720.197863605609 and parameters: {'lr': 0.0001669510261491101, 'epochs': 36, 'batch_size': 64, 'length_scale': 1.6510148682787178, 'beta': 0.22503744769961148}. Best is trial 5 with value: 7661.802608801183.\n",
      "2025-05-20 00:48:59 [INFO]: Using the given device: cuda\n",
      "2025-05-20 00:48:59 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004859\n",
      "2025-05-20 00:48:59 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004859/tensorboard\n",
      "2025-05-20 00:48:59 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/873d07d8ff154f7d935a18218765ad47\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 00:48:59 [INFO]: Epoch 001 - training loss (default): 1083.4903, validation loss: 649.3427\n",
      "2025-05-20 00:49:00 [INFO]: Epoch 002 - training loss (default): 850.0285, validation loss: 638.8513\n",
      "2025-05-20 00:49:00 [INFO]: Epoch 003 - training loss (default): 695.8480, validation loss: 629.3583\n",
      "2025-05-20 00:49:01 [INFO]: Epoch 004 - training loss (default): 642.3141, validation loss: 623.4281\n",
      "2025-05-20 00:49:02 [INFO]: Epoch 005 - training loss (default): 627.9804, validation loss: 619.8538\n",
      "2025-05-20 00:49:02 [INFO]: Epoch 006 - training loss (default): 623.6435, validation loss: 617.3318\n",
      "2025-05-20 00:49:03 [INFO]: Epoch 007 - training loss (default): 621.6120, validation loss: 615.3878\n",
      "2025-05-20 00:49:04 [INFO]: Epoch 008 - training loss (default): 620.0744, validation loss: 613.7218\n",
      "2025-05-20 00:49:04 [INFO]: Epoch 009 - training loss (default): 618.9038, validation loss: 611.8712\n",
      "2025-05-20 00:49:05 [INFO]: Epoch 010 - training loss (default): 618.0272, validation loss: 610.1489\n",
      "2025-05-20 00:49:06 [INFO]: Epoch 011 - training loss (default): 617.4595, validation loss: 608.5599\n",
      "2025-05-20 00:49:06 [INFO]: Epoch 012 - training loss (default): 617.0716, validation loss: 607.0614\n",
      "2025-05-20 00:49:07 [INFO]: Epoch 013 - training loss (default): 616.6771, validation loss: 605.6595\n",
      "2025-05-20 00:49:08 [INFO]: Epoch 014 - training loss (default): 616.5558, validation loss: 604.3673\n",
      "2025-05-20 00:49:08 [INFO]: Epoch 015 - training loss (default): 616.2675, validation loss: 603.1720\n",
      "2025-05-20 00:49:09 [INFO]: Epoch 016 - training loss (default): 616.0419, validation loss: 602.0364\n",
      "2025-05-20 00:49:10 [INFO]: Epoch 017 - training loss (default): 615.8780, validation loss: 600.9551\n",
      "2025-05-20 00:49:10 [INFO]: Epoch 018 - training loss (default): 615.7176, validation loss: 599.9469\n",
      "2025-05-20 00:49:11 [INFO]: Epoch 019 - training loss (default): 615.4946, validation loss: 598.9847\n",
      "2025-05-20 00:49:12 [INFO]: Epoch 020 - training loss (default): 615.3041, validation loss: 598.0733\n",
      "2025-05-20 00:49:12 [INFO]: Epoch 021 - training loss (default): 615.0986, validation loss: 597.2311\n",
      "2025-05-20 00:49:13 [INFO]: Epoch 022 - training loss (default): 614.9944, validation loss: 596.3889\n",
      "2025-05-20 00:49:14 [INFO]: Epoch 023 - training loss (default): 615.0205, validation loss: 595.5991\n",
      "2025-05-20 00:49:14 [INFO]: Epoch 024 - training loss (default): 614.8244, validation loss: 594.8401\n",
      "2025-05-20 00:49:15 [INFO]: Epoch 025 - training loss (default): 614.5739, validation loss: 594.1128\n",
      "2025-05-20 00:49:16 [INFO]: Epoch 026 - training loss (default): 614.5856, validation loss: 593.4434\n",
      "2025-05-20 00:49:16 [INFO]: Epoch 027 - training loss (default): 614.4256, validation loss: 592.7646\n",
      "2025-05-20 00:49:17 [INFO]: Epoch 028 - training loss (default): 614.4186, validation loss: 592.1376\n",
      "2025-05-20 00:49:17 [INFO]: Finished training. The best model is from epoch#28.\n",
      "2025-05-20 00:49:17 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004859/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-20 00:49:17,764] Trial 9 finished with value: 7701.9150700948885 and parameters: {'lr': 0.00015876258804683774, 'epochs': 28, 'batch_size': 128, 'length_scale': 1.846541739251264, 'beta': 0.4270146895828756}. Best is trial 5 with value: 7661.802608801183.\n",
      "2025-05-20 00:49:17 [INFO]: Using the given device: cuda\n",
      "2025-05-20 00:49:17 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004917\n",
      "2025-05-20 00:49:17 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004917/tensorboard\n",
      "2025-05-20 00:49:17 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/3686f94e1386462d8938b93c7dae2d8b\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 00:49:18 [INFO]: Epoch 001 - training loss (default): 13863.1958, validation loss: 26058.5918\n",
      "2025-05-20 00:49:19 [INFO]: Epoch 002 - training loss (default): 8647.5544, validation loss: 25423.8794\n",
      "2025-05-20 00:49:19 [INFO]: Epoch 003 - training loss (default): 8643.3694, validation loss: 25383.1621\n",
      "2025-05-20 00:49:20 [INFO]: Epoch 004 - training loss (default): 8642.2802, validation loss: 25369.1401\n",
      "2025-05-20 00:49:21 [INFO]: Epoch 005 - training loss (default): 8641.5062, validation loss: 25359.5989\n",
      "2025-05-20 00:49:21 [INFO]: Epoch 006 - training loss (default): 8640.8433, validation loss: 25349.9904\n",
      "2025-05-20 00:49:22 [INFO]: Epoch 007 - training loss (default): 8640.2585, validation loss: 25339.1475\n",
      "2025-05-20 00:49:23 [INFO]: Epoch 008 - training loss (default): 8639.8814, validation loss: 25329.8809\n",
      "2025-05-20 00:49:23 [INFO]: Epoch 009 - training loss (default): 8639.5593, validation loss: 25317.7351\n",
      "2025-05-20 00:49:24 [INFO]: Epoch 010 - training loss (default): 8639.4270, validation loss: 25307.5083\n",
      "2025-05-20 00:49:25 [INFO]: Epoch 011 - training loss (default): 8639.3735, validation loss: 25298.5835\n",
      "2025-05-20 00:49:25 [INFO]: Epoch 012 - training loss (default): 8638.8928, validation loss: 25283.0387\n",
      "2025-05-20 00:49:26 [INFO]: Epoch 013 - training loss (default): 8639.0616, validation loss: 25276.3794\n",
      "2025-05-20 00:49:27 [INFO]: Epoch 014 - training loss (default): 8638.6444, validation loss: 25262.8764\n",
      "2025-05-20 00:49:28 [INFO]: Epoch 015 - training loss (default): 8638.8135, validation loss: 25251.5609\n",
      "2025-05-20 00:49:28 [INFO]: Epoch 016 - training loss (default): 8638.5647, validation loss: 25237.5861\n",
      "2025-05-20 00:49:29 [INFO]: Epoch 017 - training loss (default): 8638.5060, validation loss: 25227.1518\n",
      "2025-05-20 00:49:30 [INFO]: Epoch 018 - training loss (default): 8638.3433, validation loss: 25214.7844\n",
      "2025-05-20 00:49:30 [INFO]: Epoch 019 - training loss (default): 8638.3225, validation loss: 25201.4545\n",
      "2025-05-20 00:49:31 [INFO]: Epoch 020 - training loss (default): 8638.3290, validation loss: 25194.3130\n",
      "2025-05-20 00:49:32 [INFO]: Epoch 021 - training loss (default): 8638.0729, validation loss: 25177.9924\n",
      "2025-05-20 00:49:32 [INFO]: Epoch 022 - training loss (default): 8638.2558, validation loss: 25169.0508\n",
      "2025-05-20 00:49:33 [INFO]: Epoch 023 - training loss (default): 8638.2476, validation loss: 25162.1515\n",
      "2025-05-20 00:49:34 [INFO]: Epoch 024 - training loss (default): 8637.9259, validation loss: 25148.1035\n",
      "2025-05-20 00:49:34 [INFO]: Epoch 025 - training loss (default): 8637.8307, validation loss: 25137.4524\n",
      "2025-05-20 00:49:35 [INFO]: Epoch 026 - training loss (default): 8637.8147, validation loss: 25125.0332\n",
      "2025-05-20 00:49:36 [INFO]: Epoch 027 - training loss (default): 8637.8766, validation loss: 25115.9634\n",
      "2025-05-20 00:49:36 [INFO]: Epoch 028 - training loss (default): 8637.8727, validation loss: 25107.0112\n",
      "2025-05-20 00:49:37 [INFO]: Epoch 029 - training loss (default): 8638.0871, validation loss: 25098.3075\n",
      "2025-05-20 00:49:38 [INFO]: Epoch 030 - training loss (default): 8637.7263, validation loss: 25089.6447\n",
      "2025-05-20 00:49:38 [INFO]: Epoch 031 - training loss (default): 8637.6482, validation loss: 25077.7003\n",
      "2025-05-20 00:49:39 [INFO]: Epoch 032 - training loss (default): 8637.5889, validation loss: 25067.3700\n",
      "2025-05-20 00:49:40 [INFO]: Epoch 033 - training loss (default): 8637.7669, validation loss: 25059.3596\n",
      "2025-05-20 00:49:40 [INFO]: Epoch 034 - training loss (default): 8637.8948, validation loss: 25056.7520\n",
      "2025-05-20 00:49:41 [INFO]: Epoch 035 - training loss (default): 8637.5781, validation loss: 25044.1806\n",
      "2025-05-20 00:49:42 [INFO]: Epoch 036 - training loss (default): 8637.5958, validation loss: 25033.6028\n",
      "2025-05-20 00:49:43 [INFO]: Epoch 037 - training loss (default): 8637.4862, validation loss: 25024.4354\n",
      "2025-05-20 00:49:43 [INFO]: Epoch 038 - training loss (default): 8637.4297, validation loss: 25017.0508\n",
      "2025-05-20 00:49:44 [INFO]: Epoch 039 - training loss (default): 8637.6441, validation loss: 25009.7001\n",
      "2025-05-20 00:49:45 [INFO]: Epoch 040 - training loss (default): 8637.6960, validation loss: 25002.7623\n",
      "2025-05-20 00:49:45 [INFO]: Epoch 041 - training loss (default): 8637.5064, validation loss: 24995.3056\n",
      "2025-05-20 00:49:46 [INFO]: Epoch 042 - training loss (default): 8637.3809, validation loss: 24982.9080\n",
      "2025-05-20 00:49:47 [INFO]: Epoch 043 - training loss (default): 8637.4475, validation loss: 24976.4865\n",
      "2025-05-20 00:49:47 [INFO]: Epoch 044 - training loss (default): 8637.4553, validation loss: 24969.7651\n",
      "2025-05-20 00:49:48 [INFO]: Epoch 045 - training loss (default): 8637.3833, validation loss: 24960.3929\n",
      "2025-05-20 00:49:49 [INFO]: Epoch 046 - training loss (default): 8637.2581, validation loss: 24949.6133\n",
      "2025-05-20 00:49:49 [INFO]: Epoch 047 - training loss (default): 8637.4533, validation loss: 24944.9885\n",
      "2025-05-20 00:49:49 [INFO]: Finished training. The best model is from epoch#47.\n",
      "2025-05-20 00:49:49 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004917/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-20 00:49:50,305] Trial 10 finished with value: 7702.747248157424 and parameters: {'lr': 0.0009881062146305788, 'epochs': 47, 'batch_size': 96, 'length_scale': 3.2403571323096902, 'beta': 0.9917468362947446}. Best is trial 5 with value: 7661.802608801183.\n",
      "2025-05-20 00:49:50 [INFO]: Using the given device: cuda\n",
      "2025-05-20 00:49:50 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004950\n",
      "2025-05-20 00:49:50 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004950/tensorboard\n",
      "2025-05-20 00:49:50 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/eb2c14bc505b41a2a558fff2232645fe\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 00:49:51 [INFO]: Epoch 001 - training loss (default): 4635.4631, validation loss: 10423.6187\n",
      "2025-05-20 00:49:52 [INFO]: Epoch 002 - training loss (default): 3570.1056, validation loss: 10368.1017\n",
      "2025-05-20 00:49:54 [INFO]: Epoch 003 - training loss (default): 3568.0122, validation loss: 10318.1197\n",
      "2025-05-20 00:49:55 [INFO]: Epoch 004 - training loss (default): 3566.6923, validation loss: 10272.8298\n",
      "2025-05-20 00:49:56 [INFO]: Epoch 005 - training loss (default): 3565.8259, validation loss: 10232.6030\n",
      "2025-05-20 00:49:58 [INFO]: Epoch 006 - training loss (default): 3565.2266, validation loss: 10195.4661\n",
      "2025-05-20 00:49:59 [INFO]: Epoch 007 - training loss (default): 3564.8014, validation loss: 10160.0702\n",
      "2025-05-20 00:50:00 [INFO]: Epoch 008 - training loss (default): 3564.5367, validation loss: 10128.7408\n",
      "2025-05-20 00:50:01 [INFO]: Epoch 009 - training loss (default): 3564.3861, validation loss: 10101.0913\n",
      "2025-05-20 00:50:02 [INFO]: Epoch 010 - training loss (default): 3564.1740, validation loss: 10071.8531\n",
      "2025-05-20 00:50:02 [INFO]: Finished training. The best model is from epoch#10.\n",
      "2025-05-20 00:50:03 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T004950/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-20 00:50:03,476] Trial 11 finished with value: 7689.0795888830135 and parameters: {'lr': 0.0005748641475135172, 'epochs': 10, 'batch_size': 32, 'length_scale': 2.96713408813375, 'beta': 0.6041600669712559}. Best is trial 5 with value: 7661.802608801183.\n",
      "2025-05-20 00:50:03 [INFO]: Using the given device: cuda\n",
      "2025-05-20 00:50:03 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T005003\n",
      "2025-05-20 00:50:03 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T005003/tensorboard\n",
      "2025-05-20 00:50:03 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/894629cbddc54ec785154a8a0662012a\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 00:50:04 [INFO]: Epoch 001 - training loss (default): 502.2495, validation loss: 123.2171\n",
      "2025-05-20 00:50:06 [INFO]: Epoch 002 - training loss (default): 473.7430, validation loss: 121.9633\n",
      "2025-05-20 00:50:07 [INFO]: Epoch 003 - training loss (default): 472.7724, validation loss: 120.4752\n",
      "2025-05-20 00:50:08 [INFO]: Epoch 004 - training loss (default): 471.6438, validation loss: 118.5636\n",
      "2025-05-20 00:50:10 [INFO]: Epoch 005 - training loss (default): 470.1152, validation loss: 116.0891\n",
      "2025-05-20 00:50:11 [INFO]: Epoch 006 - training loss (default): 469.1584, validation loss: 113.6763\n",
      "2025-05-20 00:50:12 [INFO]: Epoch 007 - training loss (default): 468.8160, validation loss: 111.7024\n",
      "2025-05-20 00:50:13 [INFO]: Epoch 008 - training loss (default): 468.5682, validation loss: 109.9479\n",
      "2025-05-20 00:50:15 [INFO]: Epoch 009 - training loss (default): 468.4283, validation loss: 108.2748\n",
      "2025-05-20 00:50:16 [INFO]: Epoch 010 - training loss (default): 468.3826, validation loss: 106.8770\n",
      "2025-05-20 00:50:17 [INFO]: Epoch 011 - training loss (default): 468.3695, validation loss: 105.6129\n",
      "2025-05-20 00:50:18 [INFO]: Epoch 012 - training loss (default): 468.2017, validation loss: 104.1481\n",
      "2025-05-20 00:50:20 [INFO]: Epoch 013 - training loss (default): 468.1528, validation loss: 102.7553\n",
      "2025-05-20 00:50:21 [INFO]: Epoch 014 - training loss (default): 468.1090, validation loss: 101.5204\n",
      "2025-05-20 00:50:22 [INFO]: Epoch 015 - training loss (default): 468.1449, validation loss: 100.2036\n",
      "2025-05-20 00:50:23 [INFO]: Epoch 016 - training loss (default): 468.1108, validation loss: 99.0054\n",
      "2025-05-20 00:50:24 [INFO]: Epoch 017 - training loss (default): 468.1957, validation loss: 97.7899\n",
      "2025-05-20 00:50:26 [INFO]: Epoch 018 - training loss (default): 468.1198, validation loss: 96.6154\n",
      "2025-05-20 00:50:27 [INFO]: Epoch 019 - training loss (default): 468.0753, validation loss: 95.4835\n",
      "2025-05-20 00:50:28 [INFO]: Epoch 020 - training loss (default): 468.0397, validation loss: 94.3490\n",
      "2025-05-20 00:50:30 [INFO]: Epoch 021 - training loss (default): 468.1011, validation loss: 93.1082\n",
      "2025-05-20 00:50:31 [INFO]: Epoch 022 - training loss (default): 468.0162, validation loss: 91.8230\n",
      "2025-05-20 00:50:32 [INFO]: Epoch 023 - training loss (default): 468.0689, validation loss: 90.4557\n",
      "2025-05-20 00:50:34 [INFO]: Epoch 024 - training loss (default): 468.1055, validation loss: 89.4090\n",
      "2025-05-20 00:50:35 [INFO]: Epoch 025 - training loss (default): 468.0271, validation loss: 88.3616\n",
      "2025-05-20 00:50:36 [INFO]: Epoch 026 - training loss (default): 467.9337, validation loss: 87.3997\n",
      "2025-05-20 00:50:37 [INFO]: Epoch 027 - training loss (default): 467.9398, validation loss: 86.5124\n",
      "2025-05-20 00:50:38 [INFO]: Epoch 028 - training loss (default): 467.9161, validation loss: 85.4915\n",
      "2025-05-20 00:50:40 [INFO]: Epoch 029 - training loss (default): 467.9805, validation loss: 84.4941\n",
      "2025-05-20 00:50:41 [INFO]: Epoch 030 - training loss (default): 467.9130, validation loss: 83.5849\n",
      "2025-05-20 00:50:42 [INFO]: Epoch 031 - training loss (default): 467.8964, validation loss: 82.9547\n",
      "2025-05-20 00:50:43 [INFO]: Epoch 032 - training loss (default): 467.8744, validation loss: 82.2640\n",
      "2025-05-20 00:50:44 [INFO]: Epoch 033 - training loss (default): 467.8958, validation loss: 81.3973\n",
      "2025-05-20 00:50:46 [INFO]: Epoch 034 - training loss (default): 467.8972, validation loss: 80.7756\n",
      "2025-05-20 00:50:47 [INFO]: Epoch 035 - training loss (default): 467.8540, validation loss: 80.2027\n",
      "2025-05-20 00:50:48 [INFO]: Epoch 036 - training loss (default): 467.9221, validation loss: 79.5760\n",
      "2025-05-20 00:50:49 [INFO]: Epoch 037 - training loss (default): 467.9078, validation loss: 78.8415\n",
      "2025-05-20 00:50:49 [INFO]: Finished training. The best model is from epoch#37.\n",
      "2025-05-20 00:50:49 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T005003/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-05-20 00:52:53,386] Trial 12 failed with parameters: {'lr': 0.0005489003043475935, 'epochs': 37, 'batch_size': 32, 'length_scale': 0.5911738963397587, 'beta': 0.8838948069479122} because of the following error: MlflowException(\"API request to http://localhost:5000/api/2.0/mlflow/runs/get failed with exception HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /api/2.0/mlflow/runs/get?run_uuid=622e0a78641f42fdbfa5a25f0cd22ed4&run_id=622e0a78641f42fdbfa5a25f0cd22ed4 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f1a967cfd60>: Failed to establish a new connection: [Errno 111] Connection refused'))\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connection.py\", line 199, in _new_conn\n",
      "    sock = connection.create_connection(\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/util/connection.py\", line 85, in create_connection\n",
      "    raise err\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/util/connection.py\", line 73, in create_connection\n",
      "    sock.connect(sa)\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n",
      "    response = self._make_request(\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 495, in _make_request\n",
      "    conn.request(\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connection.py\", line 441, in request\n",
      "    self.endheaders()\n",
      "  File \"/home/ec2-user/SageMaker/gdmake/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/http/client.py\", line 1278, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"/home/ec2-user/SageMaker/gdmake/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/http/client.py\", line 1038, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"/home/ec2-user/SageMaker/gdmake/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/http/client.py\", line 976, in send\n",
      "    self.connect()\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connection.py\", line 279, in connect\n",
      "    self.sock = self._new_conn()\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connection.py\", line 214, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f1a967cd480>: Failed to establish a new connection: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/requests/adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 873, in urlopen\n",
      "    return self.urlopen(\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 873, in urlopen\n",
      "    return self.urlopen(\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 873, in urlopen\n",
      "    return self.urlopen(\n",
      "  [Previous line repeated 2 more times]\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/util/retry.py\", line 519, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /api/2.0/mlflow/runs/log-metric (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f1a967cd480>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/utils/rest_utils.py\", line 189, in http_request\n",
      "    return _get_http_response_with_retries(\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/utils/request_utils.py\", line 237, in _get_http_response_with_retries\n",
      "    return session.request(method, url, allow_redirects=allow_redirects, **kwargs)\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/requests/adapters.py\", line 700, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /api/2.0/mlflow/runs/log-metric (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f1a967cd480>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-18-c131910abc1a>\", line 337, in objective\n",
      "    mlflow.log_metric(f\"MAE_{feature_names[i]}\", mae_i)\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/tracking/fluent.py\", line 923, in log_metric\n",
      "    return MlflowClient().log_metric(\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/tracking/client.py\", line 1526, in log_metric\n",
      "    return self._tracking_client.log_metric(\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py\", line 583, in log_metric\n",
      "    self.store.log_metric(run_id, metric)\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/store/tracking/rest_store.py\", line 407, in log_metric\n",
      "    self._call_endpoint(LogMetric, req_body)\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/store/tracking/rest_store.py\", line 82, in _call_endpoint\n",
      "    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/utils/rest_utils.py\", line 368, in call_endpoint\n",
      "    response = http_request(**call_kwargs)\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/utils/rest_utils.py\", line 212, in http_request\n",
      "    raise MlflowException(f\"API request to {url} failed with exception {e}\")\n",
      "mlflow.exceptions.MlflowException: API request to http://localhost:5000/api/2.0/mlflow/runs/log-metric failed with exception HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /api/2.0/mlflow/runs/log-metric (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f1a967cd480>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connection.py\", line 199, in _new_conn\n",
      "    sock = connection.create_connection(\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/util/connection.py\", line 85, in create_connection\n",
      "    raise err\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/util/connection.py\", line 73, in create_connection\n",
      "    sock.connect(sa)\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n",
      "    response = self._make_request(\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 495, in _make_request\n",
      "    conn.request(\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connection.py\", line 441, in request\n",
      "    self.endheaders()\n",
      "  File \"/home/ec2-user/SageMaker/gdmake/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/http/client.py\", line 1278, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"/home/ec2-user/SageMaker/gdmake/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/http/client.py\", line 1038, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"/home/ec2-user/SageMaker/gdmake/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/http/client.py\", line 976, in send\n",
      "    self.connect()\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connection.py\", line 279, in connect\n",
      "    self.sock = self._new_conn()\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connection.py\", line 214, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f1a967cfd60>: Failed to establish a new connection: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/requests/adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 873, in urlopen\n",
      "    return self.urlopen(\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 873, in urlopen\n",
      "    return self.urlopen(\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 873, in urlopen\n",
      "    return self.urlopen(\n",
      "  [Previous line repeated 2 more times]\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/util/retry.py\", line 519, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /api/2.0/mlflow/runs/get?run_uuid=622e0a78641f42fdbfa5a25f0cd22ed4&run_id=622e0a78641f42fdbfa5a25f0cd22ed4 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f1a967cfd60>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/utils/rest_utils.py\", line 189, in http_request\n",
      "    return _get_http_response_with_retries(\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/utils/request_utils.py\", line 237, in _get_http_response_with_retries\n",
      "    return session.request(method, url, allow_redirects=allow_redirects, **kwargs)\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/requests/adapters.py\", line 700, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /api/2.0/mlflow/runs/get?run_uuid=622e0a78641f42fdbfa5a25f0cd22ed4&run_id=622e0a78641f42fdbfa5a25f0cd22ed4 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f1a967cfd60>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"<ipython-input-18-c131910abc1a>\", line 255, in objective\n",
      "    with mlflow.start_run(run_name=\"GP-VAE-Trial\", nested=True) as run:\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/tracking/fluent.py\", line 229, in __exit__\n",
      "    end_run(RunStatus.to_string(status))\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/tracking/fluent.py\", line 519, in end_run\n",
      "    MlflowClient().set_terminated(last_active_run_id, status)\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/tracking/client.py\", line 2905, in set_terminated\n",
      "    self._tracking_client.set_terminated(run_id, status, end_time)\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py\", line 942, in set_terminated\n",
      "    self._log_url(run_id)\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py\", line 916, in _log_url\n",
      "    run_info = self.store.get_run(run_id).info\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/store/tracking/rest_store.py\", line 169, in get_run\n",
      "    response_proto = self._call_endpoint(GetRun, req_body)\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/store/tracking/rest_store.py\", line 82, in _call_endpoint\n",
      "    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/utils/rest_utils.py\", line 365, in call_endpoint\n",
      "    response = http_request(**call_kwargs)\n",
      "  File \"/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/utils/rest_utils.py\", line 212, in http_request\n",
      "    raise MlflowException(f\"API request to {url} failed with exception {e}\")\n",
      "mlflow.exceptions.MlflowException: API request to http://localhost:5000/api/2.0/mlflow/runs/get failed with exception HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /api/2.0/mlflow/runs/get?run_uuid=622e0a78641f42fdbfa5a25f0cd22ed4&run_id=622e0a78641f42fdbfa5a25f0cd22ed4 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f1a967cfd60>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "[W 2025-05-20 00:52:53,390] Trial 12 failed with value None.\n"
     ]
    },
    {
     "ename": "MlflowException",
     "evalue": "API request to http://localhost:5000/api/2.0/mlflow/runs/get failed with exception HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /api/2.0/mlflow/runs/get?run_uuid=886d8065455e497ab608c33e67318f7c&run_id=886d8065455e497ab608c33e67318f7c (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f1a966c33d0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connection.py:199\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 199\u001b[0m     sock \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[1;32m    200\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport),\n\u001b[1;32m    201\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout,\n\u001b[1;32m    202\u001b[0m         source_address\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msource_address,\n\u001b[1;32m    203\u001b[0m         socket_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msocket_options,\n\u001b[1;32m    204\u001b[0m     )\n\u001b[1;32m    205\u001b[0m \u001b[39mexcept\u001b[39;00m socket\u001b[39m.\u001b[39mgaierror \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     \u001b[39mraise\u001b[39;00m err\n\u001b[1;32m     86\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     \u001b[39m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m     sock\u001b[39m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m sock\u001b[39m.\u001b[39;49mconnect(sa)\n\u001b[1;32m     74\u001b[0m \u001b[39m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    790\u001b[0m     conn,\n\u001b[1;32m    791\u001b[0m     method,\n\u001b[1;32m    792\u001b[0m     url,\n\u001b[1;32m    793\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    794\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    795\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    796\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    797\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[1;32m    798\u001b[0m     response_conn\u001b[39m=\u001b[39;49mresponse_conn,\n\u001b[1;32m    799\u001b[0m     preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    800\u001b[0m     decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    801\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    802\u001b[0m )\n\u001b[1;32m    804\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:495\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 495\u001b[0m     conn\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    496\u001b[0m         method,\n\u001b[1;32m    497\u001b[0m         url,\n\u001b[1;32m    498\u001b[0m         body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    499\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    500\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    501\u001b[0m         preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    502\u001b[0m         decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    503\u001b[0m         enforce_content_length\u001b[39m=\u001b[39;49menforce_content_length,\n\u001b[1;32m    504\u001b[0m     )\n\u001b[1;32m    506\u001b[0m \u001b[39m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \u001b[39m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connection.py:441\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mputheader(header, value)\n\u001b[0;32m--> 441\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendheaders()\n\u001b[1;32m    443\u001b[0m \u001b[39m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/gdmake/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/http/client.py:1278\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[39mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1278\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_output(message_body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[0;32m~/SageMaker/gdmake/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/http/client.py:1038\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1038\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(msg)\n\u001b[1;32m   1040\u001b[0m \u001b[39mif\u001b[39;00m message_body \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1041\u001b[0m \n\u001b[1;32m   1042\u001b[0m     \u001b[39m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/gdmake/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/http/client.py:976\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_open:\n\u001b[0;32m--> 976\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m    977\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connection.py:279\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tunnel_host:\n\u001b[1;32m    281\u001b[0m         \u001b[39m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connection.py:214\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 214\u001b[0m     \u001b[39mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    215\u001b[0m         \u001b[39mself\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to establish a new connection: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    216\u001b[0m     ) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39me\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[39m# Audit hooks are only available in Python 3.8+\u001b[39;00m\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x7f1a967cd480>: Failed to establish a new connection: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    668\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    669\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    670\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    671\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    672\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    673\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    674\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    675\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    676\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    677\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    678\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    679\u001b[0m     )\n\u001b[1;32m    681\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:873\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    870\u001b[0m     log\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    871\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRetrying (\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m) after connection broken by \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, retries, err, url\n\u001b[1;32m    872\u001b[0m     )\n\u001b[0;32m--> 873\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    874\u001b[0m         method,\n\u001b[1;32m    875\u001b[0m         url,\n\u001b[1;32m    876\u001b[0m         body,\n\u001b[1;32m    877\u001b[0m         headers,\n\u001b[1;32m    878\u001b[0m         retries,\n\u001b[1;32m    879\u001b[0m         redirect,\n\u001b[1;32m    880\u001b[0m         assert_same_host,\n\u001b[1;32m    881\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    882\u001b[0m         pool_timeout\u001b[39m=\u001b[39;49mpool_timeout,\n\u001b[1;32m    883\u001b[0m         release_conn\u001b[39m=\u001b[39;49mrelease_conn,\n\u001b[1;32m    884\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    885\u001b[0m         body_pos\u001b[39m=\u001b[39;49mbody_pos,\n\u001b[1;32m    886\u001b[0m         preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    887\u001b[0m         decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    888\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    889\u001b[0m     )\n\u001b[1;32m    891\u001b[0m \u001b[39m# Handle redirect?\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:873\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    870\u001b[0m     log\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    871\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRetrying (\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m) after connection broken by \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, retries, err, url\n\u001b[1;32m    872\u001b[0m     )\n\u001b[0;32m--> 873\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    874\u001b[0m         method,\n\u001b[1;32m    875\u001b[0m         url,\n\u001b[1;32m    876\u001b[0m         body,\n\u001b[1;32m    877\u001b[0m         headers,\n\u001b[1;32m    878\u001b[0m         retries,\n\u001b[1;32m    879\u001b[0m         redirect,\n\u001b[1;32m    880\u001b[0m         assert_same_host,\n\u001b[1;32m    881\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    882\u001b[0m         pool_timeout\u001b[39m=\u001b[39;49mpool_timeout,\n\u001b[1;32m    883\u001b[0m         release_conn\u001b[39m=\u001b[39;49mrelease_conn,\n\u001b[1;32m    884\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    885\u001b[0m         body_pos\u001b[39m=\u001b[39;49mbody_pos,\n\u001b[1;32m    886\u001b[0m         preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    887\u001b[0m         decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    888\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    889\u001b[0m     )\n\u001b[1;32m    891\u001b[0m \u001b[39m# Handle redirect?\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: HTTPConnectionPool.urlopen at line 873 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:873\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    870\u001b[0m     log\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    871\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRetrying (\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m) after connection broken by \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, retries, err, url\n\u001b[1;32m    872\u001b[0m     )\n\u001b[0;32m--> 873\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    874\u001b[0m         method,\n\u001b[1;32m    875\u001b[0m         url,\n\u001b[1;32m    876\u001b[0m         body,\n\u001b[1;32m    877\u001b[0m         headers,\n\u001b[1;32m    878\u001b[0m         retries,\n\u001b[1;32m    879\u001b[0m         redirect,\n\u001b[1;32m    880\u001b[0m         assert_same_host,\n\u001b[1;32m    881\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    882\u001b[0m         pool_timeout\u001b[39m=\u001b[39;49mpool_timeout,\n\u001b[1;32m    883\u001b[0m         release_conn\u001b[39m=\u001b[39;49mrelease_conn,\n\u001b[1;32m    884\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    885\u001b[0m         body_pos\u001b[39m=\u001b[39;49mbody_pos,\n\u001b[1;32m    886\u001b[0m         preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    887\u001b[0m         decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    888\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    889\u001b[0m     )\n\u001b[1;32m    891\u001b[0m \u001b[39m# Handle redirect?\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:843\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    841\u001b[0m     new_e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, new_e)\n\u001b[0;32m--> 843\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[1;32m    844\u001b[0m     method, url, error\u001b[39m=\u001b[39;49mnew_e, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[1;32m    845\u001b[0m )\n\u001b[1;32m    846\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/util/retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    518\u001b[0m     reason \u001b[39m=\u001b[39m error \u001b[39mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 519\u001b[0m     \u001b[39mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mreason\u001b[39;00m  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    521\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mIncremented Retry for (url=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m): \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /api/2.0/mlflow/runs/log-metric (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f1a967cd480>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/utils/rest_utils.py:189\u001b[0m, in \u001b[0;36mhttp_request\u001b[0;34m(host_creds, endpoint, method, max_retries, backoff_factor, backoff_jitter, extra_headers, retry_codes, timeout, raise_on_status, respect_retry_after_header, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 189\u001b[0m     \u001b[39mreturn\u001b[39;00m _get_http_response_with_retries(\n\u001b[1;32m    190\u001b[0m         method,\n\u001b[1;32m    191\u001b[0m         url,\n\u001b[1;32m    192\u001b[0m         max_retries,\n\u001b[1;32m    193\u001b[0m         backoff_factor,\n\u001b[1;32m    194\u001b[0m         backoff_jitter,\n\u001b[1;32m    195\u001b[0m         retry_codes,\n\u001b[1;32m    196\u001b[0m         raise_on_status,\n\u001b[1;32m    197\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    198\u001b[0m         verify\u001b[39m=\u001b[39;49mhost_creds\u001b[39m.\u001b[39;49mverify,\n\u001b[1;32m    199\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    200\u001b[0m         respect_retry_after_header\u001b[39m=\u001b[39;49mrespect_retry_after_header,\n\u001b[1;32m    201\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    202\u001b[0m     )\n\u001b[1;32m    203\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout \u001b[39mas\u001b[39;00m to:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/utils/request_utils.py:237\u001b[0m, in \u001b[0;36m_get_http_response_with_retries\u001b[0;34m(method, url, max_retries, backoff_factor, backoff_jitter, retry_codes, raise_on_status, allow_redirects, respect_retry_after_header, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m allow_redirects \u001b[39m=\u001b[39m env_value \u001b[39mif\u001b[39;00m allow_redirects \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m allow_redirects\n\u001b[0;32m--> 237\u001b[0m \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method, url, allow_redirects\u001b[39m=\u001b[39;49mallow_redirects, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/requests/adapters.py:700\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[39mraise\u001b[39;00m SSLError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[0;32m--> 700\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    702\u001b[0m \u001b[39mexcept\u001b[39;00m ClosedPoolError \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /api/2.0/mlflow/runs/log-metric (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f1a967cd480>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMlflowException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 337\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    335\u001b[0m      percentage_mae_i \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39m'\u001b[39m\u001b[39minf\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 337\u001b[0m mlflow\u001b[39m.\u001b[39;49mlog_metric(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mMAE_\u001b[39;49m\u001b[39m{\u001b[39;49;00mfeature_names[i]\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m, mae_i)\n\u001b[1;32m    338\u001b[0m mlflow\u001b[39m.\u001b[39mlog_metric(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRMSE_\u001b[39m\u001b[39m{\u001b[39;00mfeature_names[i]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,rmse_i)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/tracking/fluent.py:923\u001b[0m, in \u001b[0;36mlog_metric\u001b[0;34m(key, value, step, synchronous, timestamp, run_id)\u001b[0m\n\u001b[1;32m    922\u001b[0m synchronous \u001b[39m=\u001b[39m synchronous \u001b[39mif\u001b[39;00m synchronous \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mnot\u001b[39;00m MLFLOW_ENABLE_ASYNC_LOGGING\u001b[39m.\u001b[39mget()\n\u001b[0;32m--> 923\u001b[0m \u001b[39mreturn\u001b[39;00m MlflowClient()\u001b[39m.\u001b[39;49mlog_metric(\n\u001b[1;32m    924\u001b[0m     run_id,\n\u001b[1;32m    925\u001b[0m     key,\n\u001b[1;32m    926\u001b[0m     value,\n\u001b[1;32m    927\u001b[0m     timestamp \u001b[39mor\u001b[39;49;00m get_current_time_millis(),\n\u001b[1;32m    928\u001b[0m     step \u001b[39mor\u001b[39;49;00m \u001b[39m0\u001b[39;49m,\n\u001b[1;32m    929\u001b[0m     synchronous\u001b[39m=\u001b[39;49msynchronous,\n\u001b[1;32m    930\u001b[0m )\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/tracking/client.py:1526\u001b[0m, in \u001b[0;36mMlflowClient.log_metric\u001b[0;34m(self, run_id, key, value, timestamp, step, synchronous)\u001b[0m\n\u001b[1;32m   1523\u001b[0m synchronous \u001b[39m=\u001b[39m (\n\u001b[1;32m   1524\u001b[0m     synchronous \u001b[39mif\u001b[39;00m synchronous \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mnot\u001b[39;00m MLFLOW_ENABLE_ASYNC_LOGGING\u001b[39m.\u001b[39mget()\n\u001b[1;32m   1525\u001b[0m )\n\u001b[0;32m-> 1526\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tracking_client\u001b[39m.\u001b[39;49mlog_metric(\n\u001b[1;32m   1527\u001b[0m     run_id, key, value, timestamp, step, synchronous\u001b[39m=\u001b[39;49msynchronous\n\u001b[1;32m   1528\u001b[0m )\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py:583\u001b[0m, in \u001b[0;36mTrackingServiceClient.log_metric\u001b[0;34m(self, run_id, key, value, timestamp, step, synchronous)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[39mif\u001b[39;00m synchronous:\n\u001b[0;32m--> 583\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstore\u001b[39m.\u001b[39;49mlog_metric(run_id, metric)\n\u001b[1;32m    584\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/store/tracking/rest_store.py:407\u001b[0m, in \u001b[0;36mRestStore.log_metric\u001b[0;34m(self, run_id, metric)\u001b[0m\n\u001b[1;32m    397\u001b[0m req_body \u001b[39m=\u001b[39m message_to_json(\n\u001b[1;32m    398\u001b[0m     LogMetric(\n\u001b[1;32m    399\u001b[0m         run_uuid\u001b[39m=\u001b[39mrun_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    405\u001b[0m     )\n\u001b[1;32m    406\u001b[0m )\n\u001b[0;32m--> 407\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_endpoint(LogMetric, req_body)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/store/tracking/rest_store.py:82\u001b[0m, in \u001b[0;36mRestStore._call_endpoint\u001b[0;34m(self, api, json_body, endpoint)\u001b[0m\n\u001b[1;32m     81\u001b[0m response_proto \u001b[39m=\u001b[39m api\u001b[39m.\u001b[39mResponse()\n\u001b[0;32m---> 82\u001b[0m \u001b[39mreturn\u001b[39;00m call_endpoint(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_host_creds(), endpoint, method, json_body, response_proto)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/utils/rest_utils.py:368\u001b[0m, in \u001b[0;36mcall_endpoint\u001b[0;34m(host_creds, endpoint, method, json_body, response_proto, extra_headers)\u001b[0m\n\u001b[1;32m    367\u001b[0m     call_kwargs[\u001b[39m\"\u001b[39m\u001b[39mjson\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m json_body\n\u001b[0;32m--> 368\u001b[0m     response \u001b[39m=\u001b[39m http_request(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcall_kwargs)\n\u001b[1;32m    370\u001b[0m response \u001b[39m=\u001b[39m verify_rest_response(response, endpoint)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/utils/rest_utils.py:212\u001b[0m, in \u001b[0;36mhttp_request\u001b[0;34m(host_creds, endpoint, method, max_retries, backoff_factor, backoff_jitter, extra_headers, retry_codes, timeout, raise_on_status, respect_retry_after_header, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 212\u001b[0m     \u001b[39mraise\u001b[39;00m MlflowException(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAPI request to \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m failed with exception \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mMlflowException\u001b[0m: API request to http://localhost:5000/api/2.0/mlflow/runs/log-metric failed with exception HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /api/2.0/mlflow/runs/log-metric (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f1a967cd480>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connection.py:199\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 199\u001b[0m     sock \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[1;32m    200\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport),\n\u001b[1;32m    201\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout,\n\u001b[1;32m    202\u001b[0m         source_address\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msource_address,\n\u001b[1;32m    203\u001b[0m         socket_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msocket_options,\n\u001b[1;32m    204\u001b[0m     )\n\u001b[1;32m    205\u001b[0m \u001b[39mexcept\u001b[39;00m socket\u001b[39m.\u001b[39mgaierror \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     \u001b[39mraise\u001b[39;00m err\n\u001b[1;32m     86\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     \u001b[39m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m     sock\u001b[39m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m sock\u001b[39m.\u001b[39;49mconnect(sa)\n\u001b[1;32m     74\u001b[0m \u001b[39m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    790\u001b[0m     conn,\n\u001b[1;32m    791\u001b[0m     method,\n\u001b[1;32m    792\u001b[0m     url,\n\u001b[1;32m    793\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    794\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    795\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    796\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    797\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[1;32m    798\u001b[0m     response_conn\u001b[39m=\u001b[39;49mresponse_conn,\n\u001b[1;32m    799\u001b[0m     preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    800\u001b[0m     decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    801\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    802\u001b[0m )\n\u001b[1;32m    804\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:495\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 495\u001b[0m     conn\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    496\u001b[0m         method,\n\u001b[1;32m    497\u001b[0m         url,\n\u001b[1;32m    498\u001b[0m         body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    499\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    500\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    501\u001b[0m         preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    502\u001b[0m         decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    503\u001b[0m         enforce_content_length\u001b[39m=\u001b[39;49menforce_content_length,\n\u001b[1;32m    504\u001b[0m     )\n\u001b[1;32m    506\u001b[0m \u001b[39m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \u001b[39m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connection.py:441\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mputheader(header, value)\n\u001b[0;32m--> 441\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendheaders()\n\u001b[1;32m    443\u001b[0m \u001b[39m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/gdmake/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/http/client.py:1278\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[39mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1278\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_output(message_body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[0;32m~/SageMaker/gdmake/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/http/client.py:1038\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1038\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(msg)\n\u001b[1;32m   1040\u001b[0m \u001b[39mif\u001b[39;00m message_body \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1041\u001b[0m \n\u001b[1;32m   1042\u001b[0m     \u001b[39m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/gdmake/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/http/client.py:976\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_open:\n\u001b[0;32m--> 976\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m    977\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connection.py:279\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tunnel_host:\n\u001b[1;32m    281\u001b[0m         \u001b[39m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connection.py:214\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 214\u001b[0m     \u001b[39mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    215\u001b[0m         \u001b[39mself\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to establish a new connection: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    216\u001b[0m     ) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39me\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[39m# Audit hooks are only available in Python 3.8+\u001b[39;00m\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x7f1a967cfd60>: Failed to establish a new connection: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    668\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    669\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    670\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    671\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    672\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    673\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    674\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    675\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    676\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    677\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    678\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    679\u001b[0m     )\n\u001b[1;32m    681\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:873\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    870\u001b[0m     log\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    871\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRetrying (\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m) after connection broken by \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, retries, err, url\n\u001b[1;32m    872\u001b[0m     )\n\u001b[0;32m--> 873\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    874\u001b[0m         method,\n\u001b[1;32m    875\u001b[0m         url,\n\u001b[1;32m    876\u001b[0m         body,\n\u001b[1;32m    877\u001b[0m         headers,\n\u001b[1;32m    878\u001b[0m         retries,\n\u001b[1;32m    879\u001b[0m         redirect,\n\u001b[1;32m    880\u001b[0m         assert_same_host,\n\u001b[1;32m    881\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    882\u001b[0m         pool_timeout\u001b[39m=\u001b[39;49mpool_timeout,\n\u001b[1;32m    883\u001b[0m         release_conn\u001b[39m=\u001b[39;49mrelease_conn,\n\u001b[1;32m    884\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    885\u001b[0m         body_pos\u001b[39m=\u001b[39;49mbody_pos,\n\u001b[1;32m    886\u001b[0m         preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    887\u001b[0m         decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    888\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    889\u001b[0m     )\n\u001b[1;32m    891\u001b[0m \u001b[39m# Handle redirect?\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:873\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    870\u001b[0m     log\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    871\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRetrying (\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m) after connection broken by \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, retries, err, url\n\u001b[1;32m    872\u001b[0m     )\n\u001b[0;32m--> 873\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    874\u001b[0m         method,\n\u001b[1;32m    875\u001b[0m         url,\n\u001b[1;32m    876\u001b[0m         body,\n\u001b[1;32m    877\u001b[0m         headers,\n\u001b[1;32m    878\u001b[0m         retries,\n\u001b[1;32m    879\u001b[0m         redirect,\n\u001b[1;32m    880\u001b[0m         assert_same_host,\n\u001b[1;32m    881\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    882\u001b[0m         pool_timeout\u001b[39m=\u001b[39;49mpool_timeout,\n\u001b[1;32m    883\u001b[0m         release_conn\u001b[39m=\u001b[39;49mrelease_conn,\n\u001b[1;32m    884\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    885\u001b[0m         body_pos\u001b[39m=\u001b[39;49mbody_pos,\n\u001b[1;32m    886\u001b[0m         preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    887\u001b[0m         decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    888\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    889\u001b[0m     )\n\u001b[1;32m    891\u001b[0m \u001b[39m# Handle redirect?\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: HTTPConnectionPool.urlopen at line 873 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:873\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    870\u001b[0m     log\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    871\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRetrying (\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m) after connection broken by \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, retries, err, url\n\u001b[1;32m    872\u001b[0m     )\n\u001b[0;32m--> 873\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    874\u001b[0m         method,\n\u001b[1;32m    875\u001b[0m         url,\n\u001b[1;32m    876\u001b[0m         body,\n\u001b[1;32m    877\u001b[0m         headers,\n\u001b[1;32m    878\u001b[0m         retries,\n\u001b[1;32m    879\u001b[0m         redirect,\n\u001b[1;32m    880\u001b[0m         assert_same_host,\n\u001b[1;32m    881\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    882\u001b[0m         pool_timeout\u001b[39m=\u001b[39;49mpool_timeout,\n\u001b[1;32m    883\u001b[0m         release_conn\u001b[39m=\u001b[39;49mrelease_conn,\n\u001b[1;32m    884\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    885\u001b[0m         body_pos\u001b[39m=\u001b[39;49mbody_pos,\n\u001b[1;32m    886\u001b[0m         preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    887\u001b[0m         decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    888\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    889\u001b[0m     )\n\u001b[1;32m    891\u001b[0m \u001b[39m# Handle redirect?\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:843\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    841\u001b[0m     new_e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, new_e)\n\u001b[0;32m--> 843\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[1;32m    844\u001b[0m     method, url, error\u001b[39m=\u001b[39;49mnew_e, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[1;32m    845\u001b[0m )\n\u001b[1;32m    846\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/util/retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    518\u001b[0m     reason \u001b[39m=\u001b[39m error \u001b[39mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 519\u001b[0m     \u001b[39mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mreason\u001b[39;00m  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    521\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mIncremented Retry for (url=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m): \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /api/2.0/mlflow/runs/get?run_uuid=622e0a78641f42fdbfa5a25f0cd22ed4&run_id=622e0a78641f42fdbfa5a25f0cd22ed4 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f1a967cfd60>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/utils/rest_utils.py:189\u001b[0m, in \u001b[0;36mhttp_request\u001b[0;34m(host_creds, endpoint, method, max_retries, backoff_factor, backoff_jitter, extra_headers, retry_codes, timeout, raise_on_status, respect_retry_after_header, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 189\u001b[0m     \u001b[39mreturn\u001b[39;00m _get_http_response_with_retries(\n\u001b[1;32m    190\u001b[0m         method,\n\u001b[1;32m    191\u001b[0m         url,\n\u001b[1;32m    192\u001b[0m         max_retries,\n\u001b[1;32m    193\u001b[0m         backoff_factor,\n\u001b[1;32m    194\u001b[0m         backoff_jitter,\n\u001b[1;32m    195\u001b[0m         retry_codes,\n\u001b[1;32m    196\u001b[0m         raise_on_status,\n\u001b[1;32m    197\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    198\u001b[0m         verify\u001b[39m=\u001b[39;49mhost_creds\u001b[39m.\u001b[39;49mverify,\n\u001b[1;32m    199\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    200\u001b[0m         respect_retry_after_header\u001b[39m=\u001b[39;49mrespect_retry_after_header,\n\u001b[1;32m    201\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    202\u001b[0m     )\n\u001b[1;32m    203\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout \u001b[39mas\u001b[39;00m to:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/utils/request_utils.py:237\u001b[0m, in \u001b[0;36m_get_http_response_with_retries\u001b[0;34m(method, url, max_retries, backoff_factor, backoff_jitter, retry_codes, raise_on_status, allow_redirects, respect_retry_after_header, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m allow_redirects \u001b[39m=\u001b[39m env_value \u001b[39mif\u001b[39;00m allow_redirects \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m allow_redirects\n\u001b[0;32m--> 237\u001b[0m \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method, url, allow_redirects\u001b[39m=\u001b[39;49mallow_redirects, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/requests/adapters.py:700\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[39mraise\u001b[39;00m SSLError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[0;32m--> 700\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    702\u001b[0m \u001b[39mexcept\u001b[39;00m ClosedPoolError \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /api/2.0/mlflow/runs/get?run_uuid=622e0a78641f42fdbfa5a25f0cd22ed4&run_id=622e0a78641f42fdbfa5a25f0cd22ed4 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f1a967cfd60>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMlflowException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 360\u001b[0m\n\u001b[1;32m    359\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 360\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\n\u001b[1;32m    362\u001b[0m best_params \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_trial\u001b[39m.\u001b[39mparams\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[39mOptimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[39m        If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m _optimize(\n\u001b[1;32m    476\u001b[0m     study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    477\u001b[0m     func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    478\u001b[0m     n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    479\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    480\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    481\u001b[0m     catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    482\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    483\u001b[0m     gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    484\u001b[0m     show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    485\u001b[0m )\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m     _optimize_sequential(\n\u001b[1;32m     64\u001b[0m         study,\n\u001b[1;32m     65\u001b[0m         func,\n\u001b[1;32m     66\u001b[0m         n_trials,\n\u001b[1;32m     67\u001b[0m         timeout,\n\u001b[1;32m     68\u001b[0m         catch,\n\u001b[1;32m     69\u001b[0m         callbacks,\n\u001b[1;32m     70\u001b[0m         gc_after_trial,\n\u001b[1;32m     71\u001b[0m         reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     72\u001b[0m         time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     73\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     74\u001b[0m     )\n\u001b[1;32m     75\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    161\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m     value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    198\u001b[0m \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m     \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[18], line 255\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    247\u001b[0m    params \u001b[39m=\u001b[39m {\n\u001b[1;32m    248\u001b[0m        \u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m: trial\u001b[39m.\u001b[39msuggest_float(\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m1e-4\u001b[39m, \u001b[39m1e-3\u001b[39m, log\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[1;32m    249\u001b[0m        \u001b[39m\"\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m\"\u001b[39m: trial\u001b[39m.\u001b[39msuggest_int(\u001b[39m\"\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m10\u001b[39m, \u001b[39m50\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    252\u001b[0m        \u001b[39m\"\u001b[39m\u001b[39mbeta\u001b[39m\u001b[39m\"\u001b[39m: trial\u001b[39m.\u001b[39msuggest_float(\u001b[39m\"\u001b[39m\u001b[39mbeta\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m0.1\u001b[39m,\u001b[39m1.0\u001b[39m)\n\u001b[1;32m    253\u001b[0m }\n\u001b[0;32m--> 255\u001b[0m    \u001b[39mwith\u001b[39;00m mlflow\u001b[39m.\u001b[39mstart_run(run_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGP-VAE-Trial\u001b[39m\u001b[39m\"\u001b[39m, nested\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mas\u001b[39;00m run:\n\u001b[1;32m    256\u001b[0m        mlflow\u001b[39m.\u001b[39mlog_params(params)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/tracking/fluent.py:229\u001b[0m, in \u001b[0;36mActiveRun.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    228\u001b[0m     status \u001b[39m=\u001b[39m RunStatus\u001b[39m.\u001b[39mFINISHED \u001b[39mif\u001b[39;00m exc_type \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m RunStatus\u001b[39m.\u001b[39mFAILED\n\u001b[0;32m--> 229\u001b[0m     end_run(RunStatus\u001b[39m.\u001b[39;49mto_string(status))\n\u001b[1;32m    231\u001b[0m \u001b[39mreturn\u001b[39;00m exc_type \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/tracking/fluent.py:519\u001b[0m, in \u001b[0;36mend_run\u001b[0;34m(status)\u001b[0m\n\u001b[1;32m    518\u001b[0m _last_active_run_id\u001b[39m.\u001b[39mset(last_active_run_id)\n\u001b[0;32m--> 519\u001b[0m MlflowClient()\u001b[39m.\u001b[39;49mset_terminated(last_active_run_id, status)\n\u001b[1;32m    520\u001b[0m \u001b[39mif\u001b[39;00m last_active_run_id \u001b[39min\u001b[39;00m run_id_to_system_metrics_monitor:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/tracking/client.py:2905\u001b[0m, in \u001b[0;36mMlflowClient.set_terminated\u001b[0;34m(self, run_id, status, end_time)\u001b[0m\n\u001b[1;32m   2863\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Set a run's status to terminated.\u001b[39;00m\n\u001b[1;32m   2864\u001b[0m \n\u001b[1;32m   2865\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2903\u001b[0m \n\u001b[1;32m   2904\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2905\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tracking_client\u001b[39m.\u001b[39;49mset_terminated(run_id, status, end_time)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py:942\u001b[0m, in \u001b[0;36mTrackingServiceClient.set_terminated\u001b[0;34m(self, run_id, status, end_time)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstore\u001b[39m.\u001b[39mshut_down_async_logging()\n\u001b[0;32m--> 942\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_url(run_id)\n\u001b[1;32m    943\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstore\u001b[39m.\u001b[39mupdate_run_info(\n\u001b[1;32m    944\u001b[0m     run_id,\n\u001b[1;32m    945\u001b[0m     run_status\u001b[39m=\u001b[39mRunStatus\u001b[39m.\u001b[39mfrom_string(status),\n\u001b[1;32m    946\u001b[0m     end_time\u001b[39m=\u001b[39mend_time,\n\u001b[1;32m    947\u001b[0m     run_name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    948\u001b[0m )\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py:916\u001b[0m, in \u001b[0;36mTrackingServiceClient._log_url\u001b[0;34m(self, run_id)\u001b[0m\n\u001b[1;32m    915\u001b[0m     host_url \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstore\u001b[39m.\u001b[39mget_host_creds()\u001b[39m.\u001b[39mhost\u001b[39m.\u001b[39mrstrip(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 916\u001b[0m run_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstore\u001b[39m.\u001b[39;49mget_run(run_id)\u001b[39m.\u001b[39minfo\n\u001b[1;32m    917\u001b[0m experiment_id \u001b[39m=\u001b[39m run_info\u001b[39m.\u001b[39mexperiment_id\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/store/tracking/rest_store.py:169\u001b[0m, in \u001b[0;36mRestStore.get_run\u001b[0;34m(self, run_id)\u001b[0m\n\u001b[1;32m    168\u001b[0m req_body \u001b[39m=\u001b[39m message_to_json(GetRun(run_uuid\u001b[39m=\u001b[39mrun_id, run_id\u001b[39m=\u001b[39mrun_id))\n\u001b[0;32m--> 169\u001b[0m response_proto \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_endpoint(GetRun, req_body)\n\u001b[1;32m    170\u001b[0m \u001b[39mreturn\u001b[39;00m Run\u001b[39m.\u001b[39mfrom_proto(response_proto\u001b[39m.\u001b[39mrun)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/store/tracking/rest_store.py:82\u001b[0m, in \u001b[0;36mRestStore._call_endpoint\u001b[0;34m(self, api, json_body, endpoint)\u001b[0m\n\u001b[1;32m     81\u001b[0m response_proto \u001b[39m=\u001b[39m api\u001b[39m.\u001b[39mResponse()\n\u001b[0;32m---> 82\u001b[0m \u001b[39mreturn\u001b[39;00m call_endpoint(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_host_creds(), endpoint, method, json_body, response_proto)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/utils/rest_utils.py:365\u001b[0m, in \u001b[0;36mcall_endpoint\u001b[0;34m(host_creds, endpoint, method, json_body, response_proto, extra_headers)\u001b[0m\n\u001b[1;32m    364\u001b[0m     call_kwargs[\u001b[39m\"\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m json_body\n\u001b[0;32m--> 365\u001b[0m     response \u001b[39m=\u001b[39m http_request(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcall_kwargs)\n\u001b[1;32m    366\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/utils/rest_utils.py:212\u001b[0m, in \u001b[0;36mhttp_request\u001b[0;34m(host_creds, endpoint, method, max_retries, backoff_factor, backoff_jitter, extra_headers, retry_codes, timeout, raise_on_status, respect_retry_after_header, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 212\u001b[0m     \u001b[39mraise\u001b[39;00m MlflowException(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAPI request to \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m failed with exception \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mMlflowException\u001b[0m: API request to http://localhost:5000/api/2.0/mlflow/runs/get failed with exception HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /api/2.0/mlflow/runs/get?run_uuid=622e0a78641f42fdbfa5a25f0cd22ed4&run_id=622e0a78641f42fdbfa5a25f0cd22ed4 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f1a967cfd60>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connection.py:199\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 199\u001b[0m     sock \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[1;32m    200\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport),\n\u001b[1;32m    201\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout,\n\u001b[1;32m    202\u001b[0m         source_address\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msource_address,\n\u001b[1;32m    203\u001b[0m         socket_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msocket_options,\n\u001b[1;32m    204\u001b[0m     )\n\u001b[1;32m    205\u001b[0m \u001b[39mexcept\u001b[39;00m socket\u001b[39m.\u001b[39mgaierror \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     \u001b[39mraise\u001b[39;00m err\n\u001b[1;32m     86\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     \u001b[39m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m     sock\u001b[39m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m sock\u001b[39m.\u001b[39;49mconnect(sa)\n\u001b[1;32m     74\u001b[0m \u001b[39m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    790\u001b[0m     conn,\n\u001b[1;32m    791\u001b[0m     method,\n\u001b[1;32m    792\u001b[0m     url,\n\u001b[1;32m    793\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    794\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    795\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    796\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    797\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[1;32m    798\u001b[0m     response_conn\u001b[39m=\u001b[39;49mresponse_conn,\n\u001b[1;32m    799\u001b[0m     preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    800\u001b[0m     decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    801\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    802\u001b[0m )\n\u001b[1;32m    804\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:495\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 495\u001b[0m     conn\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    496\u001b[0m         method,\n\u001b[1;32m    497\u001b[0m         url,\n\u001b[1;32m    498\u001b[0m         body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    499\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    500\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    501\u001b[0m         preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    502\u001b[0m         decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    503\u001b[0m         enforce_content_length\u001b[39m=\u001b[39;49menforce_content_length,\n\u001b[1;32m    504\u001b[0m     )\n\u001b[1;32m    506\u001b[0m \u001b[39m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \u001b[39m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connection.py:441\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mputheader(header, value)\n\u001b[0;32m--> 441\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendheaders()\n\u001b[1;32m    443\u001b[0m \u001b[39m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/gdmake/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/http/client.py:1278\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[39mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1278\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_output(message_body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[0;32m~/SageMaker/gdmake/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/http/client.py:1038\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1038\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(msg)\n\u001b[1;32m   1040\u001b[0m \u001b[39mif\u001b[39;00m message_body \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1041\u001b[0m \n\u001b[1;32m   1042\u001b[0m     \u001b[39m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/gdmake/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/http/client.py:976\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_open:\n\u001b[0;32m--> 976\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m    977\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connection.py:279\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tunnel_host:\n\u001b[1;32m    281\u001b[0m         \u001b[39m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connection.py:214\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 214\u001b[0m     \u001b[39mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    215\u001b[0m         \u001b[39mself\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to establish a new connection: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    216\u001b[0m     ) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39me\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[39m# Audit hooks are only available in Python 3.8+\u001b[39;00m\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x7f1a966c33d0>: Failed to establish a new connection: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    668\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    669\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    670\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    671\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    672\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    673\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    674\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    675\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    676\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    677\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    678\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    679\u001b[0m     )\n\u001b[1;32m    681\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:873\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    870\u001b[0m     log\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    871\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRetrying (\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m) after connection broken by \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, retries, err, url\n\u001b[1;32m    872\u001b[0m     )\n\u001b[0;32m--> 873\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    874\u001b[0m         method,\n\u001b[1;32m    875\u001b[0m         url,\n\u001b[1;32m    876\u001b[0m         body,\n\u001b[1;32m    877\u001b[0m         headers,\n\u001b[1;32m    878\u001b[0m         retries,\n\u001b[1;32m    879\u001b[0m         redirect,\n\u001b[1;32m    880\u001b[0m         assert_same_host,\n\u001b[1;32m    881\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    882\u001b[0m         pool_timeout\u001b[39m=\u001b[39;49mpool_timeout,\n\u001b[1;32m    883\u001b[0m         release_conn\u001b[39m=\u001b[39;49mrelease_conn,\n\u001b[1;32m    884\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    885\u001b[0m         body_pos\u001b[39m=\u001b[39;49mbody_pos,\n\u001b[1;32m    886\u001b[0m         preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    887\u001b[0m         decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    888\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    889\u001b[0m     )\n\u001b[1;32m    891\u001b[0m \u001b[39m# Handle redirect?\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:873\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    870\u001b[0m     log\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    871\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRetrying (\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m) after connection broken by \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, retries, err, url\n\u001b[1;32m    872\u001b[0m     )\n\u001b[0;32m--> 873\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    874\u001b[0m         method,\n\u001b[1;32m    875\u001b[0m         url,\n\u001b[1;32m    876\u001b[0m         body,\n\u001b[1;32m    877\u001b[0m         headers,\n\u001b[1;32m    878\u001b[0m         retries,\n\u001b[1;32m    879\u001b[0m         redirect,\n\u001b[1;32m    880\u001b[0m         assert_same_host,\n\u001b[1;32m    881\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    882\u001b[0m         pool_timeout\u001b[39m=\u001b[39;49mpool_timeout,\n\u001b[1;32m    883\u001b[0m         release_conn\u001b[39m=\u001b[39;49mrelease_conn,\n\u001b[1;32m    884\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    885\u001b[0m         body_pos\u001b[39m=\u001b[39;49mbody_pos,\n\u001b[1;32m    886\u001b[0m         preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    887\u001b[0m         decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    888\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    889\u001b[0m     )\n\u001b[1;32m    891\u001b[0m \u001b[39m# Handle redirect?\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: HTTPConnectionPool.urlopen at line 873 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:873\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    870\u001b[0m     log\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    871\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRetrying (\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m) after connection broken by \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, retries, err, url\n\u001b[1;32m    872\u001b[0m     )\n\u001b[0;32m--> 873\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    874\u001b[0m         method,\n\u001b[1;32m    875\u001b[0m         url,\n\u001b[1;32m    876\u001b[0m         body,\n\u001b[1;32m    877\u001b[0m         headers,\n\u001b[1;32m    878\u001b[0m         retries,\n\u001b[1;32m    879\u001b[0m         redirect,\n\u001b[1;32m    880\u001b[0m         assert_same_host,\n\u001b[1;32m    881\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    882\u001b[0m         pool_timeout\u001b[39m=\u001b[39;49mpool_timeout,\n\u001b[1;32m    883\u001b[0m         release_conn\u001b[39m=\u001b[39;49mrelease_conn,\n\u001b[1;32m    884\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    885\u001b[0m         body_pos\u001b[39m=\u001b[39;49mbody_pos,\n\u001b[1;32m    886\u001b[0m         preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    887\u001b[0m         decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    888\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    889\u001b[0m     )\n\u001b[1;32m    891\u001b[0m \u001b[39m# Handle redirect?\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:843\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    841\u001b[0m     new_e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, new_e)\n\u001b[0;32m--> 843\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[1;32m    844\u001b[0m     method, url, error\u001b[39m=\u001b[39;49mnew_e, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[1;32m    845\u001b[0m )\n\u001b[1;32m    846\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/urllib3/util/retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    518\u001b[0m     reason \u001b[39m=\u001b[39m error \u001b[39mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 519\u001b[0m     \u001b[39mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mreason\u001b[39;00m  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    521\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mIncremented Retry for (url=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m): \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /api/2.0/mlflow/runs/get?run_uuid=886d8065455e497ab608c33e67318f7c&run_id=886d8065455e497ab608c33e67318f7c (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f1a966c33d0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/utils/rest_utils.py:189\u001b[0m, in \u001b[0;36mhttp_request\u001b[0;34m(host_creds, endpoint, method, max_retries, backoff_factor, backoff_jitter, extra_headers, retry_codes, timeout, raise_on_status, respect_retry_after_header, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 189\u001b[0m     \u001b[39mreturn\u001b[39;00m _get_http_response_with_retries(\n\u001b[1;32m    190\u001b[0m         method,\n\u001b[1;32m    191\u001b[0m         url,\n\u001b[1;32m    192\u001b[0m         max_retries,\n\u001b[1;32m    193\u001b[0m         backoff_factor,\n\u001b[1;32m    194\u001b[0m         backoff_jitter,\n\u001b[1;32m    195\u001b[0m         retry_codes,\n\u001b[1;32m    196\u001b[0m         raise_on_status,\n\u001b[1;32m    197\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    198\u001b[0m         verify\u001b[39m=\u001b[39;49mhost_creds\u001b[39m.\u001b[39;49mverify,\n\u001b[1;32m    199\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    200\u001b[0m         respect_retry_after_header\u001b[39m=\u001b[39;49mrespect_retry_after_header,\n\u001b[1;32m    201\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    202\u001b[0m     )\n\u001b[1;32m    203\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout \u001b[39mas\u001b[39;00m to:\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/utils/request_utils.py:237\u001b[0m, in \u001b[0;36m_get_http_response_with_retries\u001b[0;34m(method, url, max_retries, backoff_factor, backoff_jitter, retry_codes, raise_on_status, allow_redirects, respect_retry_after_header, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m allow_redirects \u001b[39m=\u001b[39m env_value \u001b[39mif\u001b[39;00m allow_redirects \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m allow_redirects\n\u001b[0;32m--> 237\u001b[0m \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method, url, allow_redirects\u001b[39m=\u001b[39;49mallow_redirects, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/requests/adapters.py:700\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[39mraise\u001b[39;00m SSLError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[0;32m--> 700\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    702\u001b[0m \u001b[39mexcept\u001b[39;00m ClosedPoolError \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /api/2.0/mlflow/runs/get?run_uuid=886d8065455e497ab608c33e67318f7c&run_id=886d8065455e497ab608c33e67318f7c (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f1a966c33d0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMlflowException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 358\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[39m# Run Optuna study\u001b[39;00m\n\u001b[1;32m    357\u001b[0m mlflow\u001b[39m.\u001b[39mset_experiment(\u001b[39m\"\u001b[39m\u001b[39mGP-VAE-2\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 358\u001b[0m \u001b[39mwith\u001b[39;00m mlflow\u001b[39m.\u001b[39mstart_run(run_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGPVAE_Optuna_Study(2)\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m parent_run:\n\u001b[1;32m    359\u001b[0m     study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    360\u001b[0m     study\u001b[39m.\u001b[39moptimize(objective, n_trials\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/tracking/fluent.py:229\u001b[0m, in \u001b[0;36mActiveRun.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(r\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mrun_id \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mrun_id \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m active_run_stack):\n\u001b[1;32m    228\u001b[0m     status \u001b[39m=\u001b[39m RunStatus\u001b[39m.\u001b[39mFINISHED \u001b[39mif\u001b[39;00m exc_type \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m RunStatus\u001b[39m.\u001b[39mFAILED\n\u001b[0;32m--> 229\u001b[0m     end_run(RunStatus\u001b[39m.\u001b[39;49mto_string(status))\n\u001b[1;32m    231\u001b[0m \u001b[39mreturn\u001b[39;00m exc_type \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/tracking/fluent.py:519\u001b[0m, in \u001b[0;36mend_run\u001b[0;34m(status)\u001b[0m\n\u001b[1;32m    517\u001b[0m last_active_run_id \u001b[39m=\u001b[39m run\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mrun_id\n\u001b[1;32m    518\u001b[0m _last_active_run_id\u001b[39m.\u001b[39mset(last_active_run_id)\n\u001b[0;32m--> 519\u001b[0m MlflowClient()\u001b[39m.\u001b[39;49mset_terminated(last_active_run_id, status)\n\u001b[1;32m    520\u001b[0m \u001b[39mif\u001b[39;00m last_active_run_id \u001b[39min\u001b[39;00m run_id_to_system_metrics_monitor:\n\u001b[1;32m    521\u001b[0m     system_metrics_monitor \u001b[39m=\u001b[39m run_id_to_system_metrics_monitor\u001b[39m.\u001b[39mpop(last_active_run_id)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/tracking/client.py:2905\u001b[0m, in \u001b[0;36mMlflowClient.set_terminated\u001b[0;34m(self, run_id, status, end_time)\u001b[0m\n\u001b[1;32m   2860\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mset_terminated\u001b[39m(\n\u001b[1;32m   2861\u001b[0m     \u001b[39mself\u001b[39m, run_id: \u001b[39mstr\u001b[39m, status: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, end_time: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2862\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2863\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Set a run's status to terminated.\u001b[39;00m\n\u001b[1;32m   2864\u001b[0m \n\u001b[1;32m   2865\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2903\u001b[0m \n\u001b[1;32m   2904\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2905\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tracking_client\u001b[39m.\u001b[39;49mset_terminated(run_id, status, end_time)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py:942\u001b[0m, in \u001b[0;36mTrackingServiceClient.set_terminated\u001b[0;34m(self, run_id, status, end_time)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[39m# Tell the store to stop async logging: stop accepting new data and log already enqueued\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[39m# data in the background. This call is making sure every async logging data has been\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[39m# submitted for logging, but not necessarily finished logging.\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstore\u001b[39m.\u001b[39mshut_down_async_logging()\n\u001b[0;32m--> 942\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_url(run_id)\n\u001b[1;32m    943\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstore\u001b[39m.\u001b[39mupdate_run_info(\n\u001b[1;32m    944\u001b[0m     run_id,\n\u001b[1;32m    945\u001b[0m     run_status\u001b[39m=\u001b[39mRunStatus\u001b[39m.\u001b[39mfrom_string(status),\n\u001b[1;32m    946\u001b[0m     end_time\u001b[39m=\u001b[39mend_time,\n\u001b[1;32m    947\u001b[0m     run_name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    948\u001b[0m )\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py:916\u001b[0m, in \u001b[0;36mTrackingServiceClient._log_url\u001b[0;34m(self, run_id)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[39mif\u001b[39;00m host_url \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    915\u001b[0m     host_url \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstore\u001b[39m.\u001b[39mget_host_creds()\u001b[39m.\u001b[39mhost\u001b[39m.\u001b[39mrstrip(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 916\u001b[0m run_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstore\u001b[39m.\u001b[39;49mget_run(run_id)\u001b[39m.\u001b[39minfo\n\u001b[1;32m    917\u001b[0m experiment_id \u001b[39m=\u001b[39m run_info\u001b[39m.\u001b[39mexperiment_id\n\u001b[1;32m    918\u001b[0m run_name \u001b[39m=\u001b[39m run_info\u001b[39m.\u001b[39mrun_name\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/store/tracking/rest_store.py:169\u001b[0m, in \u001b[0;36mRestStore.get_run\u001b[0;34m(self, run_id)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[39mFetch the run from backend store\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[39m    A single Run object if it exists, otherwise raises an Exception\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    168\u001b[0m req_body \u001b[39m=\u001b[39m message_to_json(GetRun(run_uuid\u001b[39m=\u001b[39mrun_id, run_id\u001b[39m=\u001b[39mrun_id))\n\u001b[0;32m--> 169\u001b[0m response_proto \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_endpoint(GetRun, req_body)\n\u001b[1;32m    170\u001b[0m \u001b[39mreturn\u001b[39;00m Run\u001b[39m.\u001b[39mfrom_proto(response_proto\u001b[39m.\u001b[39mrun)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/store/tracking/rest_store.py:82\u001b[0m, in \u001b[0;36mRestStore._call_endpoint\u001b[0;34m(self, api, json_body, endpoint)\u001b[0m\n\u001b[1;32m     80\u001b[0m     endpoint, method \u001b[39m=\u001b[39m _METHOD_TO_INFO[api]\n\u001b[1;32m     81\u001b[0m response_proto \u001b[39m=\u001b[39m api\u001b[39m.\u001b[39mResponse()\n\u001b[0;32m---> 82\u001b[0m \u001b[39mreturn\u001b[39;00m call_endpoint(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_host_creds(), endpoint, method, json_body, response_proto)\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/utils/rest_utils.py:365\u001b[0m, in \u001b[0;36mcall_endpoint\u001b[0;34m(host_creds, endpoint, method, json_body, response_proto, extra_headers)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[39mif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mGET\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    364\u001b[0m     call_kwargs[\u001b[39m\"\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m json_body\n\u001b[0;32m--> 365\u001b[0m     response \u001b[39m=\u001b[39m http_request(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcall_kwargs)\n\u001b[1;32m    366\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    367\u001b[0m     call_kwargs[\u001b[39m\"\u001b[39m\u001b[39mjson\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m json_body\n",
      "File \u001b[0;32m~/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/mlflow/utils/rest_utils.py:212\u001b[0m, in \u001b[0;36mhttp_request\u001b[0;34m(host_creds, endpoint, method, max_retries, backoff_factor, backoff_jitter, extra_headers, retry_codes, timeout, raise_on_status, respect_retry_after_header, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[39mraise\u001b[39;00m InvalidUrlException(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid url: \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39miu\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 212\u001b[0m     \u001b[39mraise\u001b[39;00m MlflowException(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAPI request to \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m failed with exception \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mMlflowException\u001b[0m: API request to http://localhost:5000/api/2.0/mlflow/runs/get failed with exception HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /api/2.0/mlflow/runs/get?run_uuid=886d8065455e497ab608c33e67318f7c&run_id=886d8065455e497ab608c33e67318f7c (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f1a966c33d0>: Failed to establish a new connection: [Errno 111] Connection refused'))"
     ]
    }
   ],
   "source": [
    "#Import Pypots Library\n",
    "from pypots.optim import Adam\n",
    "from pypots.imputation import GPVAE\n",
    "#from pypots.utils.metrics import calc_mae\n",
    "from pypots.nn.functional import calc_mae\n",
    "\n",
    "\n",
    "import argparse\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import data_insight\n",
    "from data_insight import setup_duckdb\n",
    "from duckdb import DuckDBPyConnection as DuckDB\n",
    "from duckdb import DuckDBPyRelation as Relation\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "from duckdb import DuckDBPyConnection as DuckDB\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna \n",
    "from optuna.visualization import plot_optimization_history\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, Dataset\n",
    "from pygrinder.missing_completely_at_random import mcar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import sensor_imputation_thesis.shared.load_data as load\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "#PatchTST might be an ideal choise if SAITS is too slow \n",
    "\n",
    "##Drop columns with different indexes while loading data.. Or the mean values \n",
    "\n",
    "df=pd.read_parquet(\"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/ny_df_for_pypots.parquet\")\n",
    "\n",
    "len(df)\n",
    "\n",
    "#current length of the dataframe is 119439\n",
    "\n",
    "# Check nan values in each column\n",
    "for col in df.columns:\n",
    "    print(f\"Column {col} has {df[col].isna().sum()} NaN values\")\n",
    "    missing_rate=df[col].isna().sum()/len(df[col])\n",
    "    print(f\"Column {col} has {missing_rate} Missing_rate\")\n",
    "\n",
    "\n",
    "#Try with smaller dataset, size 4000\n",
    "##SAMPLE the percengtage of the dataset, df.sample (averagely pick samples)\n",
    "#not df.sample cuz it will randomly select \n",
    "original_size=len(df)\n",
    "desired_fraction=0.3 #Select data every 3 minutes \n",
    "step=int(1/desired_fraction) #step_size=10 (sample every 10th (3/10) minute)\n",
    "\n",
    "#Systematic sampling: Start at a random offset to avoid bias \n",
    "start=np.random.randint(0,step) #Random start between 0-9\n",
    "df1=df.iloc[start::step].reset_index(drop=True)\n",
    "\n",
    "print(f\"Original size:{len(df)}, Sampled size: {len(df1)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Custom Dataset class\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Data processing code\n",
    "sensor_cols = [col for col in df1.columns if col != \"time\"]\n",
    "data = df1[sensor_cols].values\n",
    "\n",
    "#¤get feature names for printing mae later \n",
    "feature_names=df1[sensor_cols].columns.tolist()\n",
    "\n",
    "## Convert data to 3D arrays of shape n_samples, n_timesteps, n_features, X_ori refers to the original data without missing values \n",
    "## Reconstruct all columns simultaneously  #num_features: 119\n",
    "n_features = data.shape[1]  # exclude the time column\n",
    "n_steps = 20 #60 (was 60 previously) #(TRY TO CHANGE HERE)  # # window length, 1440 steps = 24 hours of 1-minute data, but here is revised to 60 again\n",
    "#total_elements = data.shape[0] * data.shape[1]\n",
    "n_samples = data.shape[0] // n_steps \n",
    "\n",
    "\n",
    "\n",
    "# Reshape to (n_samples // n_steps, n_steps, n_features)\n",
    "#data_reshaped = data.reshape((n_samples, n_steps, n_features))\n",
    "data_reshaped=data[:n_samples*n_steps].reshape(n_samples,n_steps,n_features)\n",
    "print(f\"Reshaped data:{data.shape}\")\n",
    "\n",
    "#Split into train, test, val, fit scaler only on the train set (prevent data leakage)\n",
    "\n",
    "#train_size = int(0.6 * len(data))\n",
    "#val_size = int(0.2 * len(data))\n",
    "#test_size = len(data) - train_size - val_size\n",
    "\n",
    "#train_data = data_reshaped[:train_size]\n",
    "#val_data = data_reshaped[train_size:train_size + val_size]\n",
    "#test_data= data_reshaped[train_size + val_size:]\n",
    "\n",
    "\n",
    "#Apply time series split \n",
    "#Split into train(60%), val(20%), and test (20%)\n",
    "train_data, temp_data=train_test_split(data_reshaped,test_size=0.4,shuffle=True)\n",
    "val_data, test_data=train_test_split(temp_data, test_size=0.5, shuffle=False)\n",
    "\n",
    "##Normalization is important because of the nature of mse calculation of saits, columns with large \n",
    "#values dominate the loss, making metrics meaningless. SAITS computes MSE/MAE column-wise and averages \n",
    "#them across all columns \n",
    "#  Apply minmax scaler here \n",
    "#normalize each feature independently\n",
    "scalers={}\n",
    "\n",
    "\n",
    "#train_scaled = np.zeros_like(data_reshaped[train_size])  # Initialize the normalized data array\n",
    "#val_scaled=np.zeros_like(data_reshaped[train_size:train_size + val_size])\n",
    "#test_scaled=np.zeros_like(data_reshaped[train_size + val_size:])\n",
    "\n",
    "train_scaled = np.zeros_like(train_data)\n",
    "val_scaled = np.zeros_like(val_data)\n",
    "test_scaled = np.zeros_like(test_data)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(data_reshaped.shape[2]):\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1)) #changed to -1,1\n",
    "    # Flatten timesteps and samples for scaling\n",
    "    train_scaled[:, :, i] = scaler.fit_transform(train_data[:, :, i].reshape(-1, 1)).reshape(train_data.shape[0], train_data.shape[1])\n",
    "    val_scaled[:, :, i] = scaler.transform(val_data[:, :, i].reshape(-1, 1)).reshape(val_data.shape[0], val_data.shape[1])\n",
    "    test_scaled[:, :, i] = scaler.transform(test_data[:, :, i].reshape(-1, 1)).reshape(test_data.shape[0], test_data.shape[1])\n",
    "    scalers[i] = scaler  # Save scalers to inverse-transform later\n",
    "\n",
    "#Inverse Scale\n",
    "def inverse_scale(imputation, scalers):\n",
    "    n_samples, n_timesteps, n_features = imputation.shape\n",
    "    imputation_denorm = np.empty_like(imputation)\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        reshaped = imputation[:, :, i].reshape(-1, 1)\n",
    "        inversed = scalers[i].inverse_transform(reshaped)\n",
    "        imputation_denorm[:, :, i] = inversed.reshape(n_samples, n_timesteps)\n",
    "    \n",
    "    return imputation_denorm\n",
    "\n",
    "\n",
    "\n",
    "#Optional: Artificially mask. Mask 20% of the data (MIT part). Try masking 30% here \n",
    "def mcar_f(X, mask_ratio=0.3):\n",
    "    \"\"\"Apply MCAR only to observed values.\"\"\"\n",
    "    observed_mask=~np.isnan(X) #find observed positions\n",
    "    artificial_mask=mcar(X,mask_ratio).astype(bool) #generate MCAR mask, cast to boolean\n",
    "    #combine masks \n",
    "    combined_mask=observed_mask & artificial_mask\n",
    "\n",
    "    #Apply masking\n",
    "    X_masked=X.copy()\n",
    "    X_masked[combined_mask]=np.nan\n",
    "    return X_masked,combined_mask\n",
    "\n",
    "\n",
    "#Use mcar on validation data \n",
    "val_X_masked, val_mask =mcar_f(val_scaled)\n",
    "val_X_ori=val_scaled.copy() \n",
    "\n",
    "test_X_masked, test_mask =mcar_f(test_scaled)\n",
    "test_X_ori=test_scaled.copy() \n",
    "\n",
    "\n",
    "#?? Problem: Can't have the best input for testing\n",
    "#1.Create synthetic test_data cuz if I drop nan values for test set, there's basically nothing left\n",
    "#synthetic_data=np.random.randn(n_samples,n_steps,n_features)\n",
    "#test_X_masked,test_mask=mcar_f(synthetic_data)\n",
    "#test_X_ori=synthetic_data.copy() #Ground truth\n",
    "\n",
    "# 2, Ensure no NaN values in synthetic data\n",
    "#test_X_masked = np.nan_to_num(test_X_masked, nan=np.nanmean(test_X_masked))\n",
    "#test_X_ori = np.nan_to_num(test_X_ori, nan=np.nanmean(test_X_ori))\n",
    "\n",
    "\n",
    "\n",
    "class Config:\n",
    "    no_cuda = False\n",
    "    no_mps = False\n",
    "    seed = 1\n",
    "\n",
    "args=Config()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "use_mps = not args.no_mps and torch.backends.mps.is_available()\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "\n",
    "if args.cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA\")\n",
    "elif use_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "train_scaled = torch.tensor(train_scaled, dtype=torch.float32)\n",
    "val_X_masked = torch.tensor(val_X_masked, dtype=torch.float32)\n",
    "val_X_ori = torch.tensor(val_X_ori, dtype=torch.float32)\n",
    "\n",
    "train_scaled = train_scaled.to(device)\n",
    "val_X_masked = val_X_masked.to(device)\n",
    "val_X_ori = val_X_ori.to(device)\n",
    "\n",
    "\n",
    "#MLflow set up\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "mlflow.set_experiment(\"GP_VAE_2\")\n",
    "\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"lr\": trial.suggest_float(\"lr\", 1e-4, 1e-3, log=True),\n",
    "        \"epochs\": trial.suggest_int(\"epochs\", 10, 50),\n",
    "        \"batch_size\": trial.suggest_int(\"batch_size\", 32, 128, step=32),\n",
    "        \"length_scale\": trial.suggest_float(\"length_scale\",0.5,5.0),\n",
    "        \"beta\": trial.suggest_float(\"beta\",0.1,1.0)\n",
    " }\n",
    "\n",
    "    with mlflow.start_run(run_name=\"GP-VAE-Trial\", nested=True) as run:\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        gp_vae = GPVAE(\n",
    "            n_steps=data_reshaped.shape[1],\n",
    "            n_features=data_reshaped.shape[2],\n",
    "            latent_size=37, #should be the latent dimensions \n",
    "            encoder_sizes=(128,128), #should I change it here too?\n",
    "            decoder_sizes=(256,256), #should I change the model size?\n",
    "            kernel=\"cauchy\",\n",
    "            beta=params[\"beta\"], #The weight of KL divergence in ELBO\n",
    "            M=1,  #The number of Monte Carlo samples for ELBO estimation during training.\n",
    "            K=1,  #The number of importance weights for IWAE model training loss.\n",
    "            sigma=1.005, # The scale parameter for a kernel function\n",
    "            length_scale=params[\"length_scale\"], #The length scale parameter for a kernel function\n",
    "            kernel_scales=1, #The number of different length scales over latent space dimensions\n",
    "            window_size=24,  # Window size for the inference CNN.\n",
    "            batch_size=params[\"batch_size\"],\n",
    "            # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "            epochs=params[\"epochs\"],\n",
    "            # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "            # You can leave it to defualt as None to disable early stopping.\n",
    "            patience=3,\n",
    "            # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "            # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "            optimizer=Adam(lr=params[\"lr\"]),\n",
    "            # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "            # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "            # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "            num_workers=0,\n",
    "            # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "            # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "            device=device,\n",
    "            # set the path for saving tensorboard and trained model files \n",
    "            saving_path=\"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model\",\n",
    "            # only save the best model after training finished.\n",
    "            # You can also set it as \"better\" to save models performing better ever during training.\n",
    "            model_saving_strategy=\"best\",\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
    "        gp_vae.fit(train_set={\"X\": train_scaled}, val_set={\"X\": val_X_masked, \"X_ori\": val_X_ori})\n",
    "        gp_vae_results = gp_vae.predict({\"X\": test_X_masked}, n_sampling_times=2)\n",
    "        gp_vae_imputation = gp_vae_results[\"imputation\"]\n",
    "\n",
    "        print(f\"The shape of gp_vae_imputation is {gp_vae_imputation.shape}\")\n",
    "\n",
    "        # for error calculation, we need to take the mean value of the multiple samplings for each data sample\n",
    "        mean_gp_vae_imputation = gp_vae_imputation.mean(axis=1)\n",
    "\n",
    "        test_imputation_denorm = inverse_scale(mean_gp_vae_imputation, scalers)\n",
    "        test_ori_denorm = inverse_scale(test_X_ori, scalers)\n",
    "\n",
    "\n",
    "         # Calculate metrics\n",
    "        mae_per_feature = []\n",
    "        rmse_per_feature=[]\n",
    "        percentage_mae_per_feature = []\n",
    "\n",
    "        for i in range(n_features):\n",
    "            imputation_i = test_imputation_denorm[:, :, i]\n",
    "            ground_truth_i = test_ori_denorm[:, :, i]\n",
    "            mask_i = test_mask[:, :, i]\n",
    "            if np.isnan(imputation_i).any() or np.isnan(ground_truth_i).any():\n",
    "                continue\n",
    "            mae_i = calc_mae(imputation_i, ground_truth_i, mask_i)\n",
    "            mae_per_feature.append(mae_i)\n",
    "            rmse_i = np.sqrt(mean_squared_error(imputation_i, ground_truth_i))\n",
    "            rmse_per_feature.append(rmse_i)\n",
    "\n",
    "            #Calculate the original standard deviation for the feature\n",
    "            std_dev_i = np.std(ground_truth_i[mask_i == 1])\n",
    "             # Calculate the percentage of MAE relative to the standard deviation   \n",
    "            if std_dev_i != 0:\n",
    "                percentage_mae_i = (mae_i / std_dev_i) * 100\n",
    "                percentage_mae_per_feature.append(percentage_mae_i)\n",
    "            else:\n",
    "                 percentage_mae_i = float('inf')\n",
    "            \n",
    "            mlflow.log_metric(f\"MAE_{feature_names[i]}\", mae_i)\n",
    "            mlflow.log_metric(f\"RMSE_{feature_names[i]}\",rmse_i)\n",
    "            mlflow.log_metric(f\"Percentage_MAE_{feature_names[i]}\", percentage_mae_i)\n",
    "\n",
    "        avg_mae = np.mean(mae_per_feature)\n",
    "        avg_rmse=np.mean(rmse_per_feature)\n",
    "       \n",
    "        mlflow.log_metric(\"avg_mae\", avg_mae)\n",
    "        mlflow.log_metric(\"avg_rmse\", avg_rmse)\n",
    "\n",
    "        trial.set_user_attr(\"mlflow_run_id\", run.info.run_id)\n",
    "\n",
    "        return avg_mae\n",
    "\n",
    "    print(\"MAE per feature:\", mae_per_feature)\n",
    "    print(\"RMSE per feature\",rmse_per_feature)\n",
    "    print(\"Percentage MAE per feature:\", percentage_mae_per_feature)\n",
    "   \n",
    "\n",
    "# Run Optuna study\n",
    "mlflow.set_experiment(\"GP-VAE-2\")\n",
    "with mlflow.start_run(run_name=\"GPVAE_Optuna_Study(2)\") as parent_run:\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "\n",
    "    best_params = study.best_trial.params\n",
    "    best_value = study.best_trial.value\n",
    "    best_run_id = study.best_trial.user_attrs[\"mlflow_run_id\"]\n",
    "\n",
    "\n",
    "    \n",
    "    # Log best parameters\n",
    "    mlflow.log_params(best_params)\n",
    "\n",
    "    # Log best metric(s)\n",
    "    mlflow.log_metric(\"best_objective_value\", best_value)\n",
    "    mlflow.log_param(\"best_run_id\", best_run_id)\n",
    "\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Best Objective Value:\", best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bb587f-77bf-41a7-9ed4-2013f732b9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column time has 0 NaN values\n",
      "Column time has 0.0 Missing_rate\n",
      "Column fr_eng has 0 NaN values\n",
      "Column fr_eng has 0.0 Missing_rate\n",
      "Column te_exh_cyl_out__0 has 0 NaN values\n",
      "Column te_exh_cyl_out__0 has 0.0 Missing_rate\n",
      "Column pd_air_ic__0 has 0 NaN values\n",
      "Column pd_air_ic__0 has 0.0 Missing_rate\n",
      "Column pr_exh_turb_out__0 has 316581 NaN values\n",
      "Column pr_exh_turb_out__0 has 1.0 Missing_rate\n",
      "Column te_air_ic_out__0 has 0 NaN values\n",
      "Column te_air_ic_out__0 has 0.0 Missing_rate\n",
      "Column te_seawater has 0 NaN values\n",
      "Column te_seawater has 0.0 Missing_rate\n",
      "Column te_air_comp_in_a__0 has 316581 NaN values\n",
      "Column te_air_comp_in_a__0 has 1.0 Missing_rate\n",
      "Column te_air_comp_in_b__0 has 316581 NaN values\n",
      "Column te_air_comp_in_b__0 has 1.0 Missing_rate\n",
      "Column fr_tc__0 has 316581 NaN values\n",
      "Column fr_tc__0 has 1.0 Missing_rate\n",
      "Column pr_baro has 0 NaN values\n",
      "Column pr_baro has 0.0 Missing_rate\n",
      "Column pd_air_ic__0_1 has 0 NaN values\n",
      "Column pd_air_ic__0_1 has 0.0 Missing_rate\n",
      "Column pr_exh_rec has 0 NaN values\n",
      "Column pr_exh_rec has 0.0 Missing_rate\n",
      "Column te_exh_turb_in__0 has 316581 NaN values\n",
      "Column te_exh_turb_in__0 has 1.0 Missing_rate\n",
      "Column te_exh_turb_out__0 has 316581 NaN values\n",
      "Column te_exh_turb_out__0 has 1.0 Missing_rate\n",
      "Column bo_aux_blower_running has 0 NaN values\n",
      "Column bo_aux_blower_running has 0.0 Missing_rate\n",
      "Column re_eng_load has 426 NaN values\n",
      "Column re_eng_load has 0.0013456271854596453 Missing_rate\n",
      "Column pr_air_scav_ecs has 0 NaN values\n",
      "Column pr_air_scav_ecs has 0.0 Missing_rate\n",
      "Column pr_air_scav has 0 NaN values\n",
      "Column pr_air_scav has 0.0 Missing_rate\n",
      "Column te_air_scav_rec has 0 NaN values\n",
      "Column te_air_scav_rec has 0.0 Missing_rate\n",
      "Column te_air_ic_out__0_1 has 0 NaN values\n",
      "Column te_air_ic_out__0_1 has 0.0 Missing_rate\n",
      "Column pr_cyl_comp__0 has 426 NaN values\n",
      "Column pr_cyl_comp__0 has 0.0013456271854596453 Missing_rate\n",
      "Column pr_cyl_max__0 has 426 NaN values\n",
      "Column pr_cyl_max__0 has 0.0013456271854596453 Missing_rate\n",
      "Column se_mip__0 has 426 NaN values\n",
      "Column se_mip__0 has 0.0013456271854596453 Missing_rate\n",
      "Column te_exh_cyl_out__0_1 has 0 NaN values\n",
      "Column te_exh_cyl_out__0_1 has 0.0 Missing_rate\n",
      "Column fr_eng_setpoint has 0 NaN values\n",
      "Column fr_eng_setpoint has 0.0 Missing_rate\n",
      "Column te_air_scav_rec_iso has 126055 NaN values\n",
      "Column te_air_scav_rec_iso has 0.3981761381763277 Missing_rate\n",
      "Column pr_cyl_max_mv_iso has 128557 NaN values\n",
      "Column pr_cyl_max_mv_iso has 0.40607932882895686 Missing_rate\n",
      "Column pr_cyl_comp_mv_iso has 128557 NaN values\n",
      "Column pr_cyl_comp_mv_iso has 0.40607932882895686 Missing_rate\n",
      "Column fr_eng_ecs has 0 NaN values\n",
      "Column fr_eng_ecs has 0.0 Missing_rate\n",
      "Column pr_air_scav_iso has 128557 NaN values\n",
      "Column pr_air_scav_iso has 0.40607932882895686 Missing_rate\n",
      "Column engine_type_G80ME-C9.5-GI-LPSCR has 0 NaN values\n",
      "Column engine_type_G80ME-C9.5-GI-LPSCR has 0.0 Missing_rate\n",
      "Original size:316581, Sampled size: 105527\n",
      "Reshaped data:(105527, 31)\n",
      "CUDA available: True\n",
      "Using CUDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/ec2-user/SageMaker/sensor-imputation-thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "[I 2025-05-20 00:57:43,021] A new study created in memory with name: no-name-3d6a6bf1-75a8-48eb-98d1-69ce229069ff\n",
      "2025-05-20 00:57:43 [INFO]: Using the given device: cuda\n",
      "2025-05-20 00:57:43 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T005743\n",
      "2025-05-20 00:57:43 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T005743/tensorboard\n",
      "2025-05-20 00:57:43 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n",
      "2025-05-20 00:57:43 [INFO]: Epoch 001 - training loss (default): 8269.4697, validation loss: 10171.5896\n",
      "2025-05-20 00:57:44 [INFO]: Epoch 002 - training loss (default): 3754.5485, validation loss: 9868.3738\n",
      "2025-05-20 00:57:45 [INFO]: Epoch 003 - training loss (default): 3330.7387, validation loss: 9768.7977\n",
      "2025-05-20 00:57:45 [INFO]: Epoch 004 - training loss (default): 3307.4108, validation loss: 9716.8488\n",
      "2025-05-20 00:57:46 [INFO]: Epoch 005 - training loss (default): 3299.7014, validation loss: 9678.8203\n",
      "2025-05-20 00:57:47 [INFO]: Epoch 006 - training loss (default): 3295.3856, validation loss: 9642.8580\n",
      "2025-05-20 00:57:47 [INFO]: Epoch 007 - training loss (default): 3292.6842, validation loss: 9611.7855\n",
      "2025-05-20 00:57:48 [INFO]: Epoch 008 - training loss (default): 3290.8583, validation loss: 9584.4788\n",
      "2025-05-20 00:57:49 [INFO]: Epoch 009 - training loss (default): 3289.6684, validation loss: 9550.5531\n",
      "2025-05-20 00:57:49 [INFO]: Epoch 010 - training loss (default): 3288.8941, validation loss: 9519.3331\n",
      "2025-05-20 00:57:50 [INFO]: Epoch 011 - training loss (default): 3288.3107, validation loss: 9489.8539\n",
      "2025-05-20 00:57:51 [INFO]: Epoch 012 - training loss (default): 3287.8137, validation loss: 9463.5636\n",
      "2025-05-20 00:57:51 [INFO]: Epoch 013 - training loss (default): 3287.4699, validation loss: 9438.7710\n",
      "2025-05-20 00:57:52 [INFO]: Epoch 014 - training loss (default): 3287.1393, validation loss: 9415.9283\n",
      "2025-05-20 00:57:53 [INFO]: Epoch 015 - training loss (default): 3286.8107, validation loss: 9394.4055\n",
      "2025-05-20 00:57:53 [INFO]: Epoch 016 - training loss (default): 3286.4961, validation loss: 9375.1547\n",
      "2025-05-20 00:57:54 [INFO]: Epoch 017 - training loss (default): 3286.2827, validation loss: 9356.0858\n",
      "2025-05-20 00:57:55 [INFO]: Epoch 018 - training loss (default): 3286.0371, validation loss: 9338.5532\n",
      "2025-05-20 00:57:55 [INFO]: Epoch 019 - training loss (default): 3285.8413, validation loss: 9322.1393\n",
      "2025-05-20 00:57:56 [INFO]: Epoch 020 - training loss (default): 3285.6231, validation loss: 9307.2035\n",
      "2025-05-20 00:57:57 [INFO]: Epoch 021 - training loss (default): 3285.4461, validation loss: 9292.1146\n",
      "2025-05-20 00:57:57 [INFO]: Epoch 022 - training loss (default): 3285.3399, validation loss: 9278.2499\n",
      "2025-05-20 00:57:58 [INFO]: Epoch 023 - training loss (default): 3285.2933, validation loss: 9265.2018\n",
      "2025-05-20 00:57:59 [INFO]: Epoch 024 - training loss (default): 3285.0777, validation loss: 9253.1980\n",
      "2025-05-20 00:58:00 [INFO]: Epoch 025 - training loss (default): 3284.9824, validation loss: 9241.3018\n",
      "2025-05-20 00:58:00 [INFO]: Epoch 026 - training loss (default): 3284.8896, validation loss: 9229.9922\n",
      "2025-05-20 00:58:01 [INFO]: Epoch 027 - training loss (default): 3284.7833, validation loss: 9219.1762\n",
      "2025-05-20 00:58:02 [INFO]: Epoch 028 - training loss (default): 3284.7050, validation loss: 9208.8514\n",
      "2025-05-20 00:58:02 [INFO]: Epoch 029 - training loss (default): 3284.6844, validation loss: 9199.0845\n",
      "2025-05-20 00:58:03 [INFO]: Epoch 030 - training loss (default): 3284.5953, validation loss: 9189.9235\n",
      "2025-05-20 00:58:04 [INFO]: Epoch 031 - training loss (default): 3284.5376, validation loss: 9180.7912\n",
      "2025-05-20 00:58:04 [INFO]: Epoch 032 - training loss (default): 3284.4701, validation loss: 9172.4328\n",
      "2025-05-20 00:58:05 [INFO]: Epoch 033 - training loss (default): 3284.4088, validation loss: 9163.7884\n",
      "2025-05-20 00:58:06 [INFO]: Epoch 034 - training loss (default): 3284.3881, validation loss: 9156.3239\n",
      "2025-05-20 00:58:06 [INFO]: Epoch 035 - training loss (default): 3284.2649, validation loss: 9148.3474\n",
      "2025-05-20 00:58:07 [INFO]: Epoch 036 - training loss (default): 3284.1879, validation loss: 9141.1235\n",
      "2025-05-20 00:58:08 [INFO]: Epoch 037 - training loss (default): 3284.1720, validation loss: 9134.0649\n",
      "2025-05-20 00:58:08 [INFO]: Epoch 038 - training loss (default): 3284.1224, validation loss: 9127.6538\n",
      "2025-05-20 00:58:09 [INFO]: Epoch 039 - training loss (default): 3284.0956, validation loss: 9120.8594\n",
      "2025-05-20 00:58:10 [INFO]: Epoch 040 - training loss (default): 3284.0019, validation loss: 9114.6868\n",
      "2025-05-20 00:58:10 [INFO]: Epoch 041 - training loss (default): 3284.1440, validation loss: 9108.8689\n",
      "2025-05-20 00:58:11 [INFO]: Epoch 042 - training loss (default): 3284.0381, validation loss: 9103.4746\n",
      "2025-05-20 00:58:12 [INFO]: Epoch 043 - training loss (default): 3283.9405, validation loss: 9097.7662\n",
      "2025-05-20 00:58:12 [INFO]: Epoch 044 - training loss (default): 3283.8636, validation loss: 9091.5748\n",
      "2025-05-20 00:58:13 [INFO]: Epoch 045 - training loss (default): 3283.8162, validation loss: 9086.0703\n",
      "2025-05-20 00:58:14 [INFO]: Epoch 046 - training loss (default): 3283.7957, validation loss: 9081.2977\n",
      "2025-05-20 00:58:14 [INFO]: Epoch 047 - training loss (default): 3283.8464, validation loss: 9076.3445\n",
      "2025-05-20 00:58:14 [INFO]: Finished training. The best model is from epoch#47.\n",
      "2025-05-20 00:58:14 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T005743/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-20 00:58:15,405] Trial 0 finished with value: 7669.867760159568 and parameters: {'lr': 0.00025886707779089533, 'epochs': 47, 'batch_size': 96, 'length_scale': 2.9032511643432954, 'beta': 0.6220489199186152}. Best is trial 0 with value: 7669.867760159568.\n",
      "2025-05-20 00:58:15 [INFO]: Using the given device: cuda\n",
      "2025-05-20 00:58:15 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T005815\n",
      "2025-05-20 00:58:15 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T005815/tensorboard\n",
      "2025-05-20 00:58:15 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/27e36dc1f0814678894742cae6a5e15c\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 00:58:16 [INFO]: Epoch 001 - training loss (default): 5549.1198, validation loss: 9613.2180\n",
      "2025-05-20 00:58:16 [INFO]: Epoch 002 - training loss (default): 3382.6455, validation loss: 9385.7774\n",
      "2025-05-20 00:58:17 [INFO]: Epoch 003 - training loss (default): 3378.0290, validation loss: 9369.0943\n",
      "2025-05-20 00:58:18 [INFO]: Epoch 004 - training loss (default): 3376.5573, validation loss: 9360.9138\n",
      "2025-05-20 00:58:18 [INFO]: Epoch 005 - training loss (default): 3375.8012, validation loss: 9353.0008\n",
      "2025-05-20 00:58:19 [INFO]: Epoch 006 - training loss (default): 3375.1069, validation loss: 9345.0131\n",
      "2025-05-20 00:58:20 [INFO]: Epoch 007 - training loss (default): 3374.4601, validation loss: 9335.8580\n",
      "2025-05-20 00:58:20 [INFO]: Epoch 008 - training loss (default): 3374.0187, validation loss: 9327.1689\n",
      "2025-05-20 00:58:21 [INFO]: Epoch 009 - training loss (default): 3373.6748, validation loss: 9317.7734\n",
      "2025-05-20 00:58:22 [INFO]: Epoch 010 - training loss (default): 3373.5798, validation loss: 9310.5653\n",
      "2025-05-20 00:58:22 [INFO]: Epoch 011 - training loss (default): 3373.1475, validation loss: 9302.4911\n",
      "2025-05-20 00:58:23 [INFO]: Epoch 012 - training loss (default): 3372.9399, validation loss: 9294.1187\n",
      "2025-05-20 00:58:24 [INFO]: Epoch 013 - training loss (default): 3372.7554, validation loss: 9285.8680\n",
      "2025-05-20 00:58:24 [INFO]: Epoch 014 - training loss (default): 3372.6510, validation loss: 9278.1124\n",
      "2025-05-20 00:58:25 [INFO]: Epoch 015 - training loss (default): 3372.5788, validation loss: 9270.1807\n",
      "2025-05-20 00:58:26 [INFO]: Epoch 016 - training loss (default): 3372.4090, validation loss: 9262.9180\n",
      "2025-05-20 00:58:26 [INFO]: Epoch 017 - training loss (default): 3372.3376, validation loss: 9255.1495\n",
      "2025-05-20 00:58:27 [INFO]: Epoch 018 - training loss (default): 3372.4131, validation loss: 9249.3415\n",
      "2025-05-20 00:58:28 [INFO]: Epoch 019 - training loss (default): 3372.1613, validation loss: 9241.8819\n",
      "2025-05-20 00:58:28 [INFO]: Epoch 020 - training loss (default): 3372.1784, validation loss: 9235.2813\n",
      "2025-05-20 00:58:29 [INFO]: Epoch 021 - training loss (default): 3372.0278, validation loss: 9227.8371\n",
      "2025-05-20 00:58:30 [INFO]: Epoch 022 - training loss (default): 3371.9802, validation loss: 9221.3839\n",
      "2025-05-20 00:58:31 [INFO]: Epoch 023 - training loss (default): 3371.9371, validation loss: 9214.3756\n",
      "2025-05-20 00:58:31 [INFO]: Epoch 024 - training loss (default): 3371.8131, validation loss: 9207.0877\n",
      "2025-05-20 00:58:32 [INFO]: Epoch 025 - training loss (default): 3371.8116, validation loss: 9200.1976\n",
      "2025-05-20 00:58:33 [INFO]: Epoch 026 - training loss (default): 3371.6907, validation loss: 9194.4520\n",
      "2025-05-20 00:58:33 [INFO]: Finished training. The best model is from epoch#26.\n",
      "2025-05-20 00:58:33 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T005815/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-20 00:58:33,507] Trial 1 finished with value: 7673.5243591065655 and parameters: {'lr': 0.0007837202581459632, 'epochs': 26, 'batch_size': 96, 'length_scale': 3.010983823446949, 'beta': 0.5227149508146103}. Best is trial 0 with value: 7669.867760159568.\n",
      "2025-05-20 00:58:33 [INFO]: Using the given device: cuda\n",
      "2025-05-20 00:58:33 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T005833\n",
      "2025-05-20 00:58:33 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T005833/tensorboard\n",
      "2025-05-20 00:58:33 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/7bcc647ef9c5494196337f197fa83866\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 00:58:34 [INFO]: Epoch 001 - training loss (default): 544.4703, validation loss: 128.1299\n",
      "2025-05-20 00:58:35 [INFO]: Epoch 002 - training loss (default): 475.2225, validation loss: 127.5210\n",
      "2025-05-20 00:58:35 [INFO]: Epoch 003 - training loss (default): 471.4964, validation loss: 127.5934\n",
      "2025-05-20 00:58:36 [INFO]: Epoch 004 - training loss (default): 470.5382, validation loss: 127.2535\n",
      "2025-05-20 00:58:37 [INFO]: Epoch 005 - training loss (default): 470.1760, validation loss: 126.8746\n",
      "2025-05-20 00:58:38 [INFO]: Epoch 006 - training loss (default): 469.8465, validation loss: 126.4669\n",
      "2025-05-20 00:58:39 [INFO]: Epoch 007 - training loss (default): 469.3890, validation loss: 126.0887\n",
      "2025-05-20 00:58:39 [INFO]: Epoch 008 - training loss (default): 468.6625, validation loss: 125.7014\n",
      "2025-05-20 00:58:40 [INFO]: Epoch 009 - training loss (default): 467.7936, validation loss: 125.3369\n",
      "2025-05-20 00:58:41 [INFO]: Epoch 010 - training loss (default): 466.8862, validation loss: 125.0142\n",
      "2025-05-20 00:58:42 [INFO]: Epoch 011 - training loss (default): 466.7178, validation loss: 124.7314\n",
      "2025-05-20 00:58:43 [INFO]: Epoch 012 - training loss (default): 466.5118, validation loss: 124.4270\n",
      "2025-05-20 00:58:44 [INFO]: Epoch 013 - training loss (default): 466.2501, validation loss: 124.0795\n",
      "2025-05-20 00:58:44 [INFO]: Epoch 014 - training loss (default): 466.0429, validation loss: 123.7759\n",
      "2025-05-20 00:58:45 [INFO]: Epoch 015 - training loss (default): 466.1599, validation loss: 123.4872\n",
      "2025-05-20 00:58:46 [INFO]: Epoch 016 - training loss (default): 465.9008, validation loss: 123.1597\n",
      "2025-05-20 00:58:47 [INFO]: Epoch 017 - training loss (default): 465.9972, validation loss: 122.8136\n",
      "2025-05-20 00:58:48 [INFO]: Epoch 018 - training loss (default): 465.8315, validation loss: 122.4632\n",
      "2025-05-20 00:58:48 [INFO]: Epoch 019 - training loss (default): 465.5226, validation loss: 122.1099\n",
      "2025-05-20 00:58:49 [INFO]: Epoch 020 - training loss (default): 465.5624, validation loss: 121.7126\n",
      "2025-05-20 00:58:50 [INFO]: Epoch 021 - training loss (default): 465.7296, validation loss: 121.3334\n",
      "2025-05-20 00:58:51 [INFO]: Epoch 022 - training loss (default): 465.5204, validation loss: 120.9292\n",
      "2025-05-20 00:58:52 [INFO]: Epoch 023 - training loss (default): 465.5784, validation loss: 120.4817\n",
      "2025-05-20 00:58:52 [INFO]: Epoch 024 - training loss (default): 465.2753, validation loss: 120.0714\n",
      "2025-05-20 00:58:53 [INFO]: Epoch 025 - training loss (default): 465.2589, validation loss: 119.6794\n",
      "2025-05-20 00:58:54 [INFO]: Epoch 026 - training loss (default): 464.9892, validation loss: 119.2683\n",
      "2025-05-20 00:58:55 [INFO]: Epoch 027 - training loss (default): 465.2828, validation loss: 118.8628\n",
      "2025-05-20 00:58:56 [INFO]: Epoch 028 - training loss (default): 465.0901, validation loss: 118.4837\n",
      "2025-05-20 00:58:56 [INFO]: Epoch 029 - training loss (default): 465.1381, validation loss: 118.1180\n",
      "2025-05-20 00:58:57 [INFO]: Epoch 030 - training loss (default): 465.1975, validation loss: 117.7618\n",
      "2025-05-20 00:58:58 [INFO]: Epoch 031 - training loss (default): 465.1024, validation loss: 117.4496\n",
      "2025-05-20 00:58:59 [INFO]: Epoch 032 - training loss (default): 465.0186, validation loss: 117.0930\n",
      "2025-05-20 00:59:00 [INFO]: Epoch 033 - training loss (default): 464.8727, validation loss: 116.7811\n",
      "2025-05-20 00:59:00 [INFO]: Epoch 034 - training loss (default): 465.0479, validation loss: 116.4614\n",
      "2025-05-20 00:59:01 [INFO]: Epoch 035 - training loss (default): 465.0260, validation loss: 116.1548\n",
      "2025-05-20 00:59:02 [INFO]: Epoch 036 - training loss (default): 464.8756, validation loss: 115.8937\n",
      "2025-05-20 00:59:03 [INFO]: Epoch 037 - training loss (default): 465.0014, validation loss: 115.6048\n",
      "2025-05-20 00:59:04 [INFO]: Epoch 038 - training loss (default): 465.0504, validation loss: 115.3312\n",
      "2025-05-20 00:59:04 [INFO]: Epoch 039 - training loss (default): 464.9405, validation loss: 115.0725\n",
      "2025-05-20 00:59:05 [INFO]: Epoch 040 - training loss (default): 464.9875, validation loss: 114.7858\n",
      "2025-05-20 00:59:06 [INFO]: Epoch 041 - training loss (default): 464.8651, validation loss: 114.5595\n",
      "2025-05-20 00:59:07 [INFO]: Epoch 042 - training loss (default): 465.1141, validation loss: 114.3134\n",
      "2025-05-20 00:59:08 [INFO]: Epoch 043 - training loss (default): 465.0250, validation loss: 114.1112\n",
      "2025-05-20 00:59:09 [INFO]: Epoch 044 - training loss (default): 464.9022, validation loss: 113.8777\n",
      "2025-05-20 00:59:10 [INFO]: Epoch 045 - training loss (default): 464.8374, validation loss: 113.5808\n",
      "2025-05-20 00:59:10 [INFO]: Finished training. The best model is from epoch#45.\n",
      "2025-05-20 00:59:10 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T005833/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-20 00:59:10,574] Trial 2 finished with value: 7684.9697188667 and parameters: {'lr': 0.00036432831979090385, 'epochs': 45, 'batch_size': 64, 'length_scale': 0.5334382463286361, 'beta': 0.946294598906401}. Best is trial 0 with value: 7669.867760159568.\n",
      "2025-05-20 00:59:10 [INFO]: Using the given device: cuda\n",
      "2025-05-20 00:59:10 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T005910\n",
      "2025-05-20 00:59:10 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T005910/tensorboard\n",
      "2025-05-20 00:59:10 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/40fc11eeaa794325a190bb707f4b6790\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 00:59:11 [INFO]: Epoch 001 - training loss (default): 489.3693, validation loss: 36.9562\n",
      "2025-05-20 00:59:12 [INFO]: Epoch 002 - training loss (default): 453.9106, validation loss: 36.6587\n",
      "2025-05-20 00:59:13 [INFO]: Epoch 003 - training loss (default): 451.9676, validation loss: 36.4459\n",
      "2025-05-20 00:59:13 [INFO]: Epoch 004 - training loss (default): 451.1696, validation loss: 36.2601\n",
      "2025-05-20 00:59:14 [INFO]: Epoch 005 - training loss (default): 450.3631, validation loss: 36.0891\n",
      "2025-05-20 00:59:15 [INFO]: Epoch 006 - training loss (default): 449.3554, validation loss: 35.9154\n",
      "2025-05-20 00:59:16 [INFO]: Epoch 007 - training loss (default): 448.4563, validation loss: 35.7512\n",
      "2025-05-20 00:59:17 [INFO]: Epoch 008 - training loss (default): 447.9314, validation loss: 35.6131\n",
      "2025-05-20 00:59:17 [INFO]: Epoch 009 - training loss (default): 447.6712, validation loss: 35.4632\n",
      "2025-05-20 00:59:18 [INFO]: Epoch 010 - training loss (default): 447.3082, validation loss: 35.3252\n",
      "2025-05-20 00:59:19 [INFO]: Epoch 011 - training loss (default): 447.1417, validation loss: 35.1959\n",
      "2025-05-20 00:59:20 [INFO]: Epoch 012 - training loss (default): 446.9558, validation loss: 35.0645\n",
      "2025-05-20 00:59:21 [INFO]: Epoch 013 - training loss (default): 446.8880, validation loss: 34.9329\n",
      "2025-05-20 00:59:21 [INFO]: Epoch 014 - training loss (default): 446.7509, validation loss: 34.8088\n",
      "2025-05-20 00:59:22 [INFO]: Epoch 015 - training loss (default): 446.6912, validation loss: 34.6884\n",
      "2025-05-20 00:59:23 [INFO]: Epoch 016 - training loss (default): 446.4673, validation loss: 34.5556\n",
      "2025-05-20 00:59:24 [INFO]: Epoch 017 - training loss (default): 446.5409, validation loss: 34.4463\n",
      "2025-05-20 00:59:25 [INFO]: Epoch 018 - training loss (default): 446.4618, validation loss: 34.3257\n",
      "2025-05-20 00:59:25 [INFO]: Epoch 019 - training loss (default): 446.4388, validation loss: 34.2064\n",
      "2025-05-20 00:59:26 [INFO]: Epoch 020 - training loss (default): 446.4231, validation loss: 34.0972\n",
      "2025-05-20 00:59:27 [INFO]: Epoch 021 - training loss (default): 446.3561, validation loss: 33.9799\n",
      "2025-05-20 00:59:28 [INFO]: Epoch 022 - training loss (default): 446.2909, validation loss: 33.8776\n",
      "2025-05-20 00:59:29 [INFO]: Epoch 023 - training loss (default): 446.2184, validation loss: 33.7639\n",
      "2025-05-20 00:59:30 [INFO]: Epoch 024 - training loss (default): 446.1661, validation loss: 33.6613\n",
      "2025-05-20 00:59:30 [INFO]: Epoch 025 - training loss (default): 446.3227, validation loss: 33.5526\n",
      "2025-05-20 00:59:31 [INFO]: Epoch 026 - training loss (default): 446.2847, validation loss: 33.4709\n",
      "2025-05-20 00:59:32 [INFO]: Epoch 027 - training loss (default): 446.1444, validation loss: 33.3690\n",
      "2025-05-20 00:59:33 [INFO]: Epoch 028 - training loss (default): 446.2214, validation loss: 33.2709\n",
      "2025-05-20 00:59:34 [INFO]: Epoch 029 - training loss (default): 446.2631, validation loss: 33.1692\n",
      "2025-05-20 00:59:34 [INFO]: Epoch 030 - training loss (default): 446.1948, validation loss: 33.0820\n",
      "2025-05-20 00:59:35 [INFO]: Epoch 031 - training loss (default): 446.1716, validation loss: 32.9697\n",
      "2025-05-20 00:59:36 [INFO]: Epoch 032 - training loss (default): 446.2076, validation loss: 32.8836\n",
      "2025-05-20 00:59:37 [INFO]: Epoch 033 - training loss (default): 446.0874, validation loss: 32.7968\n",
      "2025-05-20 00:59:38 [INFO]: Epoch 034 - training loss (default): 445.9756, validation loss: 32.7145\n",
      "2025-05-20 00:59:38 [INFO]: Epoch 035 - training loss (default): 445.8508, validation loss: 32.6351\n",
      "2025-05-20 00:59:39 [INFO]: Epoch 036 - training loss (default): 446.1473, validation loss: 32.5282\n",
      "2025-05-20 00:59:40 [INFO]: Epoch 037 - training loss (default): 446.1036, validation loss: 32.4445\n",
      "2025-05-20 00:59:41 [INFO]: Epoch 038 - training loss (default): 446.1402, validation loss: 32.3506\n",
      "2025-05-20 00:59:42 [INFO]: Epoch 039 - training loss (default): 445.8907, validation loss: 32.2732\n",
      "2025-05-20 00:59:42 [INFO]: Epoch 040 - training loss (default): 446.0489, validation loss: 32.2181\n",
      "2025-05-20 00:59:43 [INFO]: Epoch 041 - training loss (default): 446.0565, validation loss: 32.1387\n",
      "2025-05-20 00:59:44 [INFO]: Epoch 042 - training loss (default): 446.1239, validation loss: 32.0540\n",
      "2025-05-20 00:59:45 [INFO]: Epoch 043 - training loss (default): 445.8819, validation loss: 31.9732\n",
      "2025-05-20 00:59:46 [INFO]: Epoch 044 - training loss (default): 445.9830, validation loss: 31.8976\n",
      "2025-05-20 00:59:47 [INFO]: Epoch 045 - training loss (default): 445.8609, validation loss: 31.8035\n",
      "2025-05-20 00:59:47 [INFO]: Finished training. The best model is from epoch#45.\n",
      "2025-05-20 00:59:47 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T005910/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-20 00:59:47,495] Trial 3 finished with value: 7709.205387372332 and parameters: {'lr': 0.0003859200724579461, 'epochs': 45, 'batch_size': 64, 'length_scale': 0.6141147439985831, 'beta': 0.25315684009494477}. Best is trial 0 with value: 7669.867760159568.\n",
      "2025-05-20 00:59:47 [INFO]: Using the given device: cuda\n",
      "2025-05-20 00:59:47 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T005947\n",
      "2025-05-20 00:59:47 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T005947/tensorboard\n",
      "2025-05-20 00:59:47 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/b6dc0dfd69394ffd995ebc87280bbb36\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 00:59:48 [INFO]: Epoch 001 - training loss (default): 2396.5461, validation loss: 2281.7020\n",
      "2025-05-20 00:59:49 [INFO]: Epoch 002 - training loss (default): 1358.6825, validation loss: 2226.6020\n",
      "2025-05-20 00:59:49 [INFO]: Epoch 003 - training loss (default): 1093.7280, validation loss: 2197.7651\n",
      "2025-05-20 00:59:50 [INFO]: Epoch 004 - training loss (default): 1061.5229, validation loss: 2180.8514\n",
      "2025-05-20 00:59:51 [INFO]: Epoch 005 - training loss (default): 1053.8107, validation loss: 2168.6415\n",
      "2025-05-20 00:59:52 [INFO]: Epoch 006 - training loss (default): 1050.4967, validation loss: 2158.8166\n",
      "2025-05-20 00:59:53 [INFO]: Epoch 007 - training loss (default): 1048.3797, validation loss: 2150.1668\n",
      "2025-05-20 00:59:53 [INFO]: Epoch 008 - training loss (default): 1047.1401, validation loss: 2142.8459\n",
      "2025-05-20 00:59:54 [INFO]: Epoch 009 - training loss (default): 1046.3984, validation loss: 2136.3121\n",
      "2025-05-20 00:59:55 [INFO]: Epoch 010 - training loss (default): 1045.9999, validation loss: 2130.5196\n",
      "2025-05-20 00:59:56 [INFO]: Epoch 011 - training loss (default): 1045.2585, validation loss: 2125.2686\n",
      "2025-05-20 00:59:57 [INFO]: Epoch 012 - training loss (default): 1044.9677, validation loss: 2120.4286\n",
      "2025-05-20 00:59:57 [INFO]: Epoch 013 - training loss (default): 1044.7409, validation loss: 2115.9093\n",
      "2025-05-20 00:59:58 [INFO]: Epoch 014 - training loss (default): 1044.2609, validation loss: 2111.9503\n",
      "2025-05-20 00:59:59 [INFO]: Epoch 015 - training loss (default): 1044.1110, validation loss: 2108.0204\n",
      "2025-05-20 01:00:00 [INFO]: Epoch 016 - training loss (default): 1043.9971, validation loss: 2104.5062\n",
      "2025-05-20 01:00:01 [INFO]: Epoch 017 - training loss (default): 1043.7329, validation loss: 2101.2486\n",
      "2025-05-20 01:00:02 [INFO]: Epoch 018 - training loss (default): 1043.4895, validation loss: 2098.1536\n",
      "2025-05-20 01:00:02 [INFO]: Epoch 019 - training loss (default): 1043.3674, validation loss: 2095.1481\n",
      "2025-05-20 01:00:03 [INFO]: Epoch 020 - training loss (default): 1043.4000, validation loss: 2092.3698\n",
      "2025-05-20 01:00:04 [INFO]: Epoch 021 - training loss (default): 1043.1776, validation loss: 2089.5695\n",
      "2025-05-20 01:00:05 [INFO]: Epoch 022 - training loss (default): 1042.8722, validation loss: 2087.0296\n",
      "2025-05-20 01:00:06 [INFO]: Epoch 023 - training loss (default): 1042.6742, validation loss: 2084.5600\n",
      "2025-05-20 01:00:06 [INFO]: Epoch 024 - training loss (default): 1042.6630, validation loss: 2082.1941\n",
      "2025-05-20 01:00:07 [INFO]: Epoch 025 - training loss (default): 1042.7134, validation loss: 2079.7718\n",
      "2025-05-20 01:00:08 [INFO]: Epoch 026 - training loss (default): 1042.3891, validation loss: 2077.5269\n",
      "2025-05-20 01:00:09 [INFO]: Epoch 027 - training loss (default): 1042.4506, validation loss: 2075.2369\n",
      "2025-05-20 01:00:10 [INFO]: Epoch 028 - training loss (default): 1042.2666, validation loss: 2072.9178\n",
      "2025-05-20 01:00:10 [INFO]: Epoch 029 - training loss (default): 1042.2848, validation loss: 2070.8681\n",
      "2025-05-20 01:00:11 [INFO]: Epoch 030 - training loss (default): 1042.2195, validation loss: 2068.9845\n",
      "2025-05-20 01:00:12 [INFO]: Epoch 031 - training loss (default): 1042.2051, validation loss: 2067.1575\n",
      "2025-05-20 01:00:13 [INFO]: Epoch 032 - training loss (default): 1042.1026, validation loss: 2065.5624\n",
      "2025-05-20 01:00:14 [INFO]: Epoch 033 - training loss (default): 1041.9698, validation loss: 2063.9393\n",
      "2025-05-20 01:00:14 [INFO]: Epoch 034 - training loss (default): 1041.9636, validation loss: 2062.4186\n",
      "2025-05-20 01:00:15 [INFO]: Epoch 035 - training loss (default): 1042.0164, validation loss: 2060.8469\n",
      "2025-05-20 01:00:16 [INFO]: Epoch 036 - training loss (default): 1041.8115, validation loss: 2059.3372\n",
      "2025-05-20 01:00:17 [INFO]: Epoch 037 - training loss (default): 1041.6596, validation loss: 2058.0787\n",
      "2025-05-20 01:00:18 [INFO]: Epoch 038 - training loss (default): 1041.6293, validation loss: 2056.6263\n",
      "2025-05-20 01:00:18 [INFO]: Epoch 039 - training loss (default): 1041.6700, validation loss: 2055.4228\n",
      "2025-05-20 01:00:19 [INFO]: Epoch 040 - training loss (default): 1041.4321, validation loss: 2053.8601\n",
      "2025-05-20 01:00:20 [INFO]: Epoch 041 - training loss (default): 1041.5564, validation loss: 2052.8050\n",
      "2025-05-20 01:00:21 [INFO]: Epoch 042 - training loss (default): 1041.4419, validation loss: 2051.4238\n",
      "2025-05-20 01:00:22 [INFO]: Epoch 043 - training loss (default): 1041.3644, validation loss: 2050.0800\n",
      "2025-05-20 01:00:22 [INFO]: Finished training. The best model is from epoch#43.\n",
      "2025-05-20 01:00:22 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T005947/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-20 01:00:22,610] Trial 4 finished with value: 7664.664779482548 and parameters: {'lr': 0.00012021352690757769, 'epochs': 43, 'batch_size': 64, 'length_scale': 2.3487754392417886, 'beta': 0.45088192430534046}. Best is trial 4 with value: 7664.664779482548.\n",
      "2025-05-20 01:00:22 [INFO]: Using the given device: cuda\n",
      "2025-05-20 01:00:22 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010022\n",
      "2025-05-20 01:00:22 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010022/tensorboard\n",
      "2025-05-20 01:00:22 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/259728ca815f4e7fa06ac124e62fa4f7\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 01:00:23 [INFO]: Epoch 001 - training loss (default): 496.0277, validation loss: 51.9901\n",
      "2025-05-20 01:00:25 [INFO]: Epoch 002 - training loss (default): 458.2804, validation loss: 51.6145\n",
      "2025-05-20 01:00:26 [INFO]: Epoch 003 - training loss (default): 456.7171, validation loss: 51.3118\n",
      "2025-05-20 01:00:27 [INFO]: Epoch 004 - training loss (default): 455.9602, validation loss: 51.0407\n",
      "2025-05-20 01:00:28 [INFO]: Epoch 005 - training loss (default): 455.4130, validation loss: 50.7801\n",
      "2025-05-20 01:00:29 [INFO]: Epoch 006 - training loss (default): 454.7341, validation loss: 50.5429\n",
      "2025-05-20 01:00:31 [INFO]: Epoch 007 - training loss (default): 453.7897, validation loss: 50.3125\n",
      "2025-05-20 01:00:32 [INFO]: Epoch 008 - training loss (default): 453.0438, validation loss: 50.0887\n",
      "2025-05-20 01:00:33 [INFO]: Epoch 009 - training loss (default): 452.5990, validation loss: 49.8736\n",
      "2025-05-20 01:00:34 [INFO]: Epoch 010 - training loss (default): 452.2415, validation loss: 49.6548\n",
      "2025-05-20 01:00:36 [INFO]: Epoch 011 - training loss (default): 452.0455, validation loss: 49.4348\n",
      "2025-05-20 01:00:37 [INFO]: Epoch 012 - training loss (default): 451.7687, validation loss: 49.2357\n",
      "2025-05-20 01:00:38 [INFO]: Epoch 013 - training loss (default): 451.6502, validation loss: 49.0302\n",
      "2025-05-20 01:00:39 [INFO]: Epoch 014 - training loss (default): 451.4789, validation loss: 48.8335\n",
      "2025-05-20 01:00:40 [INFO]: Epoch 015 - training loss (default): 451.3892, validation loss: 48.6345\n",
      "2025-05-20 01:00:42 [INFO]: Epoch 016 - training loss (default): 451.3344, validation loss: 48.4614\n",
      "2025-05-20 01:00:43 [INFO]: Epoch 017 - training loss (default): 451.2946, validation loss: 48.2912\n",
      "2025-05-20 01:00:44 [INFO]: Epoch 018 - training loss (default): 451.1614, validation loss: 48.1098\n",
      "2025-05-20 01:00:45 [INFO]: Epoch 019 - training loss (default): 451.1327, validation loss: 47.9609\n",
      "2025-05-20 01:00:46 [INFO]: Epoch 020 - training loss (default): 451.1118, validation loss: 47.8121\n",
      "2025-05-20 01:00:48 [INFO]: Epoch 021 - training loss (default): 451.1083, validation loss: 47.6751\n",
      "2025-05-20 01:00:49 [INFO]: Epoch 022 - training loss (default): 451.0249, validation loss: 47.5371\n",
      "2025-05-20 01:00:50 [INFO]: Epoch 023 - training loss (default): 451.0192, validation loss: 47.4107\n",
      "2025-05-20 01:00:51 [INFO]: Epoch 024 - training loss (default): 450.9296, validation loss: 47.2740\n",
      "2025-05-20 01:00:53 [INFO]: Epoch 025 - training loss (default): 450.9328, validation loss: 47.1487\n",
      "2025-05-20 01:00:54 [INFO]: Epoch 026 - training loss (default): 450.9273, validation loss: 47.0070\n",
      "2025-05-20 01:00:55 [INFO]: Epoch 027 - training loss (default): 450.9049, validation loss: 46.8804\n",
      "2025-05-20 01:00:56 [INFO]: Epoch 028 - training loss (default): 450.8799, validation loss: 46.7925\n",
      "2025-05-20 01:00:57 [INFO]: Epoch 029 - training loss (default): 450.8843, validation loss: 46.6725\n",
      "2025-05-20 01:00:59 [INFO]: Epoch 030 - training loss (default): 450.8445, validation loss: 46.5539\n",
      "2025-05-20 01:01:00 [INFO]: Epoch 031 - training loss (default): 450.7841, validation loss: 46.4418\n",
      "2025-05-20 01:01:01 [INFO]: Epoch 032 - training loss (default): 450.7845, validation loss: 46.3433\n",
      "2025-05-20 01:01:02 [INFO]: Epoch 033 - training loss (default): 450.7760, validation loss: 46.2376\n",
      "2025-05-20 01:01:03 [INFO]: Epoch 034 - training loss (default): 450.7999, validation loss: 46.1240\n",
      "2025-05-20 01:01:05 [INFO]: Epoch 035 - training loss (default): 450.7481, validation loss: 46.0514\n",
      "2025-05-20 01:01:06 [INFO]: Epoch 036 - training loss (default): 450.7627, validation loss: 45.9461\n",
      "2025-05-20 01:01:07 [INFO]: Epoch 037 - training loss (default): 450.7363, validation loss: 45.8408\n",
      "2025-05-20 01:01:08 [INFO]: Epoch 038 - training loss (default): 450.6942, validation loss: 45.7443\n",
      "2025-05-20 01:01:09 [INFO]: Epoch 039 - training loss (default): 450.7288, validation loss: 45.6370\n",
      "2025-05-20 01:01:11 [INFO]: Epoch 040 - training loss (default): 450.7026, validation loss: 45.5307\n",
      "2025-05-20 01:01:11 [INFO]: Finished training. The best model is from epoch#40.\n",
      "2025-05-20 01:01:11 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010022/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-20 01:01:11,744] Trial 5 finished with value: 7764.636355451376 and parameters: {'lr': 0.00017871967272931443, 'epochs': 40, 'batch_size': 32, 'length_scale': 0.6219436362577234, 'beta': 0.355810074145431}. Best is trial 4 with value: 7664.664779482548.\n",
      "2025-05-20 01:01:11 [INFO]: Using the given device: cuda\n",
      "2025-05-20 01:01:11 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010111\n",
      "2025-05-20 01:01:11 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010111/tensorboard\n",
      "2025-05-20 01:01:11 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/6e6cf13ce45f46eb9fad31a1dc83efaf\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 01:01:12 [INFO]: Epoch 001 - training loss (default): 989.4459, validation loss: 1266.3272\n",
      "2025-05-20 01:01:14 [INFO]: Epoch 002 - training loss (default): 811.4540, validation loss: 1255.2776\n",
      "2025-05-20 01:01:15 [INFO]: Epoch 003 - training loss (default): 809.6540, validation loss: 1247.7605\n",
      "2025-05-20 01:01:16 [INFO]: Epoch 004 - training loss (default): 808.8941, validation loss: 1242.5047\n",
      "2025-05-20 01:01:17 [INFO]: Epoch 005 - training loss (default): 808.1918, validation loss: 1237.4741\n",
      "2025-05-20 01:01:19 [INFO]: Epoch 006 - training loss (default): 807.8758, validation loss: 1233.7165\n",
      "2025-05-20 01:01:20 [INFO]: Epoch 007 - training loss (default): 807.4945, validation loss: 1230.7543\n",
      "2025-05-20 01:01:21 [INFO]: Epoch 008 - training loss (default): 807.2998, validation loss: 1228.3886\n",
      "2025-05-20 01:01:22 [INFO]: Epoch 009 - training loss (default): 806.9639, validation loss: 1225.5584\n",
      "2025-05-20 01:01:23 [INFO]: Epoch 010 - training loss (default): 807.0220, validation loss: 1224.3967\n",
      "2025-05-20 01:01:25 [INFO]: Epoch 011 - training loss (default): 806.8567, validation loss: 1222.6943\n",
      "2025-05-20 01:01:26 [INFO]: Epoch 012 - training loss (default): 806.7437, validation loss: 1220.6056\n",
      "2025-05-20 01:01:27 [INFO]: Epoch 013 - training loss (default): 806.6367, validation loss: 1218.4250\n",
      "2025-05-20 01:01:29 [INFO]: Epoch 014 - training loss (default): 806.5524, validation loss: 1216.4486\n",
      "2025-05-20 01:01:30 [INFO]: Epoch 015 - training loss (default): 806.5228, validation loss: 1214.3878\n",
      "2025-05-20 01:01:31 [INFO]: Epoch 016 - training loss (default): 806.4759, validation loss: 1212.1264\n",
      "2025-05-20 01:01:32 [INFO]: Epoch 017 - training loss (default): 806.4342, validation loss: 1210.3568\n",
      "2025-05-20 01:01:34 [INFO]: Epoch 018 - training loss (default): 806.4923, validation loss: 1209.9301\n",
      "2025-05-20 01:01:35 [INFO]: Epoch 019 - training loss (default): 806.4348, validation loss: 1209.1081\n",
      "2025-05-20 01:01:36 [INFO]: Epoch 020 - training loss (default): 806.3695, validation loss: 1207.8716\n",
      "2025-05-20 01:01:37 [INFO]: Epoch 021 - training loss (default): 806.3961, validation loss: 1207.7060\n",
      "2025-05-20 01:01:38 [INFO]: Epoch 022 - training loss (default): 806.3645, validation loss: 1206.2831\n",
      "2025-05-20 01:01:40 [INFO]: Epoch 023 - training loss (default): 806.2953, validation loss: 1205.0803\n",
      "2025-05-20 01:01:41 [INFO]: Epoch 024 - training loss (default): 806.2997, validation loss: 1202.4758\n",
      "2025-05-20 01:01:42 [INFO]: Epoch 025 - training loss (default): 806.2378, validation loss: 1200.3829\n",
      "2025-05-20 01:01:43 [INFO]: Epoch 026 - training loss (default): 806.2148, validation loss: 1198.1721\n",
      "2025-05-20 01:01:44 [INFO]: Epoch 027 - training loss (default): 806.3190, validation loss: 1198.9970\n",
      "2025-05-20 01:01:46 [INFO]: Epoch 028 - training loss (default): 806.1704, validation loss: 1195.9719\n",
      "2025-05-20 01:01:47 [INFO]: Epoch 029 - training loss (default): 806.1641, validation loss: 1193.7431\n",
      "2025-05-20 01:01:48 [INFO]: Epoch 030 - training loss (default): 806.2037, validation loss: 1192.0111\n",
      "2025-05-20 01:01:48 [INFO]: Finished training. The best model is from epoch#30.\n",
      "2025-05-20 01:01:48 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010111/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-20 01:01:49,153] Trial 6 finished with value: 7981.207828600384 and parameters: {'lr': 0.0004555324343993705, 'epochs': 30, 'batch_size': 32, 'length_scale': 1.958614821449487, 'beta': 0.6962865502148616}. Best is trial 4 with value: 7664.664779482548.\n",
      "2025-05-20 01:01:49 [INFO]: Using the given device: cuda\n",
      "2025-05-20 01:01:49 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010149\n",
      "2025-05-20 01:01:49 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010149/tensorboard\n",
      "2025-05-20 01:01:49 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/b2de9279e40c4c76b19bd81787f1ed51\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 01:01:49 [INFO]: Epoch 001 - training loss (default): 10581.4894, validation loss: 13670.1480\n",
      "2025-05-20 01:01:50 [INFO]: Epoch 002 - training loss (default): 5040.5104, validation loss: 13221.5676\n",
      "2025-05-20 01:01:51 [INFO]: Epoch 003 - training loss (default): 4649.4156, validation loss: 13099.0653\n",
      "2025-05-20 01:01:51 [INFO]: Epoch 004 - training loss (default): 4632.8065, validation loss: 13056.2556\n",
      "2025-05-20 01:01:52 [INFO]: Epoch 005 - training loss (default): 4627.8174, validation loss: 13026.5353\n",
      "2025-05-20 01:01:53 [INFO]: Epoch 006 - training loss (default): 4624.8992, validation loss: 12998.2837\n",
      "2025-05-20 01:01:53 [INFO]: Epoch 007 - training loss (default): 4622.7121, validation loss: 12972.6780\n",
      "2025-05-20 01:01:54 [INFO]: Epoch 008 - training loss (default): 4621.2216, validation loss: 12948.2343\n",
      "2025-05-20 01:01:54 [INFO]: Epoch 009 - training loss (default): 4620.1189, validation loss: 12925.8358\n",
      "2025-05-20 01:01:55 [INFO]: Epoch 010 - training loss (default): 4619.2743, validation loss: 12904.2411\n",
      "2025-05-20 01:01:56 [INFO]: Epoch 011 - training loss (default): 4618.7043, validation loss: 12883.3715\n",
      "2025-05-20 01:01:56 [INFO]: Epoch 012 - training loss (default): 4618.0639, validation loss: 12863.6501\n",
      "2025-05-20 01:01:57 [INFO]: Epoch 013 - training loss (default): 4617.6298, validation loss: 12844.9626\n",
      "2025-05-20 01:01:58 [INFO]: Epoch 014 - training loss (default): 4617.1890, validation loss: 12827.5174\n",
      "2025-05-20 01:01:58 [INFO]: Epoch 015 - training loss (default): 4616.7204, validation loss: 12811.3482\n",
      "2025-05-20 01:01:59 [INFO]: Epoch 016 - training loss (default): 4616.4813, validation loss: 12795.2504\n",
      "2025-05-20 01:02:00 [INFO]: Epoch 017 - training loss (default): 4616.1677, validation loss: 12780.0621\n",
      "2025-05-20 01:02:00 [INFO]: Epoch 018 - training loss (default): 4615.8405, validation loss: 12765.7811\n",
      "2025-05-20 01:02:01 [INFO]: Epoch 019 - training loss (default): 4615.6119, validation loss: 12751.7217\n",
      "2025-05-20 01:02:02 [INFO]: Epoch 020 - training loss (default): 4615.4039, validation loss: 12738.1853\n",
      "2025-05-20 01:02:02 [INFO]: Epoch 021 - training loss (default): 4615.2464, validation loss: 12725.3249\n",
      "2025-05-20 01:02:03 [INFO]: Epoch 022 - training loss (default): 4615.1229, validation loss: 12712.9510\n",
      "2025-05-20 01:02:04 [INFO]: Epoch 023 - training loss (default): 4614.9628, validation loss: 12701.3172\n",
      "2025-05-20 01:02:04 [INFO]: Epoch 024 - training loss (default): 4614.9209, validation loss: 12690.5268\n",
      "2025-05-20 01:02:05 [INFO]: Epoch 025 - training loss (default): 4614.8302, validation loss: 12679.8073\n",
      "2025-05-20 01:02:05 [INFO]: Epoch 026 - training loss (default): 4614.6177, validation loss: 12669.4937\n",
      "2025-05-20 01:02:06 [INFO]: Epoch 027 - training loss (default): 4614.6144, validation loss: 12659.1216\n",
      "2025-05-20 01:02:07 [INFO]: Epoch 028 - training loss (default): 4614.5327, validation loss: 12649.3210\n",
      "2025-05-20 01:02:07 [INFO]: Epoch 029 - training loss (default): 4614.3745, validation loss: 12640.0535\n",
      "2025-05-20 01:02:08 [INFO]: Epoch 030 - training loss (default): 4614.3678, validation loss: 12630.8644\n",
      "2025-05-20 01:02:09 [INFO]: Epoch 031 - training loss (default): 4614.1870, validation loss: 12621.9276\n",
      "2025-05-20 01:02:09 [INFO]: Finished training. The best model is from epoch#31.\n",
      "2025-05-20 01:02:09 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010149/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-20 01:02:09,799] Trial 7 finished with value: 7699.871230387868 and parameters: {'lr': 0.0003689571079285201, 'epochs': 31, 'batch_size': 128, 'length_scale': 3.8583632102331533, 'beta': 0.24865609459032972}. Best is trial 4 with value: 7664.664779482548.\n",
      "2025-05-20 01:02:09 [INFO]: Using the given device: cuda\n",
      "2025-05-20 01:02:09 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010209\n",
      "2025-05-20 01:02:09 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010209/tensorboard\n",
      "2025-05-20 01:02:09 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/b44afb216df244f4833e6b06dbbaabd7\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 01:02:11 [INFO]: Epoch 001 - training loss (default): 22367.2716, validation loss: 32717.9982\n",
      "2025-05-20 01:02:12 [INFO]: Epoch 002 - training loss (default): 11385.1933, validation loss: 32026.7265\n",
      "2025-05-20 01:02:13 [INFO]: Epoch 003 - training loss (default): 10947.8143, validation loss: 31680.5275\n",
      "2025-05-20 01:02:14 [INFO]: Epoch 004 - training loss (default): 10879.9530, validation loss: 31425.0185\n",
      "2025-05-20 01:02:16 [INFO]: Epoch 005 - training loss (default): 10858.5507, validation loss: 31225.6940\n",
      "2025-05-20 01:02:17 [INFO]: Epoch 006 - training loss (default): 10849.3779, validation loss: 31064.4976\n",
      "2025-05-20 01:02:18 [INFO]: Epoch 007 - training loss (default): 10844.5407, validation loss: 30932.6919\n",
      "2025-05-20 01:02:19 [INFO]: Epoch 008 - training loss (default): 10841.7055, validation loss: 30811.9387\n",
      "2025-05-20 01:02:21 [INFO]: Epoch 009 - training loss (default): 10839.8575, validation loss: 30707.6836\n",
      "2025-05-20 01:02:22 [INFO]: Epoch 010 - training loss (default): 10838.6150, validation loss: 30610.8186\n",
      "2025-05-20 01:02:23 [INFO]: Epoch 011 - training loss (default): 10837.5954, validation loss: 30525.4885\n",
      "2025-05-20 01:02:24 [INFO]: Epoch 012 - training loss (default): 10836.8265, validation loss: 30444.7218\n",
      "2025-05-20 01:02:25 [INFO]: Epoch 013 - training loss (default): 10836.2990, validation loss: 30371.2428\n",
      "2025-05-20 01:02:27 [INFO]: Epoch 014 - training loss (default): 10835.7388, validation loss: 30301.7190\n",
      "2025-05-20 01:02:28 [INFO]: Epoch 015 - training loss (default): 10835.2736, validation loss: 30237.7633\n",
      "2025-05-20 01:02:29 [INFO]: Epoch 016 - training loss (default): 10834.8926, validation loss: 30175.1107\n",
      "2025-05-20 01:02:30 [INFO]: Epoch 017 - training loss (default): 10834.5729, validation loss: 30116.8136\n",
      "2025-05-20 01:02:31 [INFO]: Epoch 018 - training loss (default): 10834.2487, validation loss: 30061.8959\n",
      "2025-05-20 01:02:33 [INFO]: Epoch 019 - training loss (default): 10833.9440, validation loss: 30008.4019\n",
      "2025-05-20 01:02:34 [INFO]: Epoch 020 - training loss (default): 10833.7610, validation loss: 29956.6764\n",
      "2025-05-20 01:02:35 [INFO]: Epoch 021 - training loss (default): 10833.5208, validation loss: 29908.6992\n",
      "2025-05-20 01:02:36 [INFO]: Epoch 022 - training loss (default): 10833.2724, validation loss: 29860.3908\n",
      "2025-05-20 01:02:38 [INFO]: Epoch 023 - training loss (default): 10833.0686, validation loss: 29814.7450\n",
      "2025-05-20 01:02:39 [INFO]: Epoch 024 - training loss (default): 10832.9025, validation loss: 29769.3363\n",
      "2025-05-20 01:02:40 [INFO]: Epoch 025 - training loss (default): 10832.7307, validation loss: 29727.5574\n",
      "2025-05-20 01:02:41 [INFO]: Epoch 026 - training loss (default): 10832.5884, validation loss: 29685.4587\n",
      "2025-05-20 01:02:42 [INFO]: Epoch 027 - training loss (default): 10832.4700, validation loss: 29644.3105\n",
      "2025-05-20 01:02:44 [INFO]: Epoch 028 - training loss (default): 10832.3691, validation loss: 29604.3951\n",
      "2025-05-20 01:02:45 [INFO]: Epoch 029 - training loss (default): 10832.2085, validation loss: 29565.3875\n",
      "2025-05-20 01:02:46 [INFO]: Epoch 030 - training loss (default): 10832.1147, validation loss: 29527.5590\n",
      "2025-05-20 01:02:47 [INFO]: Epoch 031 - training loss (default): 10832.0506, validation loss: 29491.6450\n",
      "2025-05-20 01:02:49 [INFO]: Epoch 032 - training loss (default): 10831.9761, validation loss: 29455.2338\n",
      "2025-05-20 01:02:50 [INFO]: Epoch 033 - training loss (default): 10831.9075, validation loss: 29421.1014\n",
      "2025-05-20 01:02:51 [INFO]: Epoch 034 - training loss (default): 10831.8166, validation loss: 29384.4782\n",
      "2025-05-20 01:02:52 [INFO]: Epoch 035 - training loss (default): 10831.7412, validation loss: 29351.3310\n",
      "2025-05-20 01:02:53 [INFO]: Epoch 036 - training loss (default): 10831.6646, validation loss: 29317.0117\n",
      "2025-05-20 01:02:53 [INFO]: Finished training. The best model is from epoch#36.\n",
      "2025-05-20 01:02:53 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010209/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-20 01:02:54,453] Trial 8 finished with value: 7864.355183212267 and parameters: {'lr': 0.00010433242531794538, 'epochs': 36, 'batch_size': 32, 'length_scale': 4.345805014963809, 'beta': 0.45117333706517215}. Best is trial 4 with value: 7664.664779482548.\n",
      "2025-05-20 01:02:54 [INFO]: Using the given device: cuda\n",
      "2025-05-20 01:02:54 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010254\n",
      "2025-05-20 01:02:54 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010254/tensorboard\n",
      "2025-05-20 01:02:54 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/a97c268b9cd649d381f0d97cfd1818ed\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 01:02:55 [INFO]: Epoch 001 - training loss (default): 476.0661, validation loss: 44.4445\n",
      "2025-05-20 01:02:56 [INFO]: Epoch 002 - training loss (default): 456.7899, validation loss: 43.9670\n",
      "2025-05-20 01:02:58 [INFO]: Epoch 003 - training loss (default): 455.6383, validation loss: 43.5770\n",
      "2025-05-20 01:02:59 [INFO]: Epoch 004 - training loss (default): 454.8821, validation loss: 43.2697\n",
      "2025-05-20 01:03:00 [INFO]: Epoch 005 - training loss (default): 454.1021, validation loss: 43.0018\n",
      "2025-05-20 01:03:01 [INFO]: Epoch 006 - training loss (default): 453.1842, validation loss: 42.7158\n",
      "2025-05-20 01:03:03 [INFO]: Epoch 007 - training loss (default): 452.5992, validation loss: 42.4859\n",
      "2025-05-20 01:03:04 [INFO]: Epoch 008 - training loss (default): 452.3109, validation loss: 42.2584\n",
      "2025-05-20 01:03:05 [INFO]: Epoch 009 - training loss (default): 452.1425, validation loss: 42.0828\n",
      "2025-05-20 01:03:06 [INFO]: Epoch 010 - training loss (default): 452.0164, validation loss: 41.8983\n",
      "2025-05-20 01:03:07 [INFO]: Epoch 011 - training loss (default): 451.8102, validation loss: 41.7147\n",
      "2025-05-20 01:03:09 [INFO]: Epoch 012 - training loss (default): 451.8274, validation loss: 41.5682\n",
      "2025-05-20 01:03:10 [INFO]: Epoch 013 - training loss (default): 451.7115, validation loss: 41.3969\n",
      "2025-05-20 01:03:11 [INFO]: Epoch 014 - training loss (default): 451.6799, validation loss: 41.2482\n",
      "2025-05-20 01:03:11 [INFO]: Finished training. The best model is from epoch#14.\n",
      "2025-05-20 01:03:11 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010254/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-20 01:03:12,053] Trial 9 finished with value: 7724.759744442614 and parameters: {'lr': 0.0005285519017606626, 'epochs': 14, 'batch_size': 32, 'length_scale': 0.759798862029593, 'beta': 0.2714230654017563}. Best is trial 4 with value: 7664.664779482548.\n",
      "2025-05-20 01:03:12 [INFO]: Using the given device: cuda\n",
      "2025-05-20 01:03:12 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010312\n",
      "2025-05-20 01:03:12 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010312/tensorboard\n",
      "2025-05-20 01:03:12 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/e538ff9c7a794766b67de28bf61209ac\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 01:03:12 [INFO]: Epoch 001 - training loss (default): 670.6750, validation loss: 182.3160\n",
      "2025-05-20 01:03:13 [INFO]: Epoch 002 - training loss (default): 608.5812, validation loss: 180.3310\n",
      "2025-05-20 01:03:14 [INFO]: Epoch 003 - training loss (default): 552.8417, validation loss: 178.2213\n",
      "2025-05-20 01:03:14 [INFO]: Epoch 004 - training loss (default): 517.3310, validation loss: 176.3697\n",
      "2025-05-20 01:03:15 [INFO]: Epoch 005 - training loss (default): 500.1164, validation loss: 175.0572\n",
      "2025-05-20 01:03:15 [INFO]: Epoch 006 - training loss (default): 493.8599, validation loss: 174.1248\n",
      "2025-05-20 01:03:16 [INFO]: Epoch 007 - training loss (default): 491.0825, validation loss: 173.4272\n",
      "2025-05-20 01:03:17 [INFO]: Epoch 008 - training loss (default): 489.4451, validation loss: 172.8874\n",
      "2025-05-20 01:03:18 [INFO]: Epoch 009 - training loss (default): 488.2642, validation loss: 172.4269\n",
      "2025-05-20 01:03:19 [INFO]: Epoch 010 - training loss (default): 487.6115, validation loss: 172.0203\n",
      "2025-05-20 01:03:19 [INFO]: Epoch 011 - training loss (default): 487.0235, validation loss: 171.6619\n",
      "2025-05-20 01:03:20 [INFO]: Epoch 012 - training loss (default): 486.5555, validation loss: 171.3274\n",
      "2025-05-20 01:03:21 [INFO]: Epoch 013 - training loss (default): 486.1467, validation loss: 171.0289\n",
      "2025-05-20 01:03:21 [INFO]: Epoch 014 - training loss (default): 485.8054, validation loss: 170.7565\n",
      "2025-05-20 01:03:22 [INFO]: Epoch 015 - training loss (default): 485.5216, validation loss: 170.5050\n",
      "2025-05-20 01:03:22 [INFO]: Epoch 016 - training loss (default): 485.1573, validation loss: 170.2791\n",
      "2025-05-20 01:03:23 [INFO]: Epoch 017 - training loss (default): 484.9384, validation loss: 170.0719\n",
      "2025-05-20 01:03:24 [INFO]: Epoch 018 - training loss (default): 484.8148, validation loss: 169.8826\n",
      "2025-05-20 01:03:24 [INFO]: Epoch 019 - training loss (default): 484.6516, validation loss: 169.7041\n",
      "2025-05-20 01:03:25 [INFO]: Epoch 020 - training loss (default): 484.3862, validation loss: 169.5293\n",
      "2025-05-20 01:03:25 [INFO]: Finished training. The best model is from epoch#20.\n",
      "2025-05-20 01:03:25 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010312/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-20 01:03:25,985] Trial 10 finished with value: 7854.583647898926 and parameters: {'lr': 0.00010872555931990879, 'epochs': 20, 'batch_size': 128, 'length_scale': 1.887251258669379, 'beta': 0.1068312281901258}. Best is trial 4 with value: 7664.664779482548.\n",
      "2025-05-20 01:03:26 [INFO]: Using the given device: cuda\n",
      "2025-05-20 01:03:26 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010326\n",
      "2025-05-20 01:03:26 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010326/tensorboard\n",
      "2025-05-20 01:03:26 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/885c5da4ad2e4e9cbe0eb8d2256fef82\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 01:03:26 [INFO]: Epoch 001 - training loss (default): 9271.4218, validation loss: 10704.8013\n",
      "2025-05-20 01:03:27 [INFO]: Epoch 002 - training loss (default): 4581.7434, validation loss: 10409.1409\n",
      "2025-05-20 01:03:28 [INFO]: Epoch 003 - training loss (default): 3544.2606, validation loss: 10257.9053\n",
      "2025-05-20 01:03:28 [INFO]: Epoch 004 - training loss (default): 3459.6607, validation loss: 10172.1635\n",
      "2025-05-20 01:03:29 [INFO]: Epoch 005 - training loss (default): 3441.5273, validation loss: 10108.2076\n",
      "2025-05-20 01:03:30 [INFO]: Epoch 006 - training loss (default): 3433.9532, validation loss: 10057.5363\n",
      "2025-05-20 01:03:30 [INFO]: Epoch 007 - training loss (default): 3429.9052, validation loss: 10015.7954\n",
      "2025-05-20 01:03:31 [INFO]: Epoch 008 - training loss (default): 3427.4687, validation loss: 9978.2260\n",
      "2025-05-20 01:03:32 [INFO]: Epoch 009 - training loss (default): 3425.8663, validation loss: 9945.6511\n",
      "2025-05-20 01:03:33 [INFO]: Epoch 010 - training loss (default): 3424.7772, validation loss: 9916.9274\n",
      "2025-05-20 01:03:33 [INFO]: Epoch 011 - training loss (default): 3423.9774, validation loss: 9891.0599\n",
      "2025-05-20 01:03:34 [INFO]: Epoch 012 - training loss (default): 3423.3757, validation loss: 9867.1288\n",
      "2025-05-20 01:03:35 [INFO]: Epoch 013 - training loss (default): 3422.8159, validation loss: 9845.3196\n",
      "2025-05-20 01:03:35 [INFO]: Epoch 014 - training loss (default): 3422.3416, validation loss: 9824.2687\n",
      "2025-05-20 01:03:36 [INFO]: Epoch 015 - training loss (default): 3421.9787, validation loss: 9804.9838\n",
      "2025-05-20 01:03:37 [INFO]: Epoch 016 - training loss (default): 3421.5979, validation loss: 9787.0818\n",
      "2025-05-20 01:03:37 [INFO]: Epoch 017 - training loss (default): 3421.2498, validation loss: 9769.9419\n",
      "2025-05-20 01:03:38 [INFO]: Epoch 018 - training loss (default): 3420.9276, validation loss: 9753.8659\n",
      "2025-05-20 01:03:39 [INFO]: Epoch 019 - training loss (default): 3420.6986, validation loss: 9738.3922\n",
      "2025-05-20 01:03:39 [INFO]: Epoch 020 - training loss (default): 3420.4512, validation loss: 9723.3449\n",
      "2025-05-20 01:03:40 [INFO]: Epoch 021 - training loss (default): 3420.1776, validation loss: 9709.3162\n",
      "2025-05-20 01:03:41 [INFO]: Epoch 022 - training loss (default): 3419.9803, validation loss: 9695.7586\n",
      "2025-05-20 01:03:41 [INFO]: Epoch 023 - training loss (default): 3419.8118, validation loss: 9683.1853\n",
      "2025-05-20 01:03:42 [INFO]: Epoch 024 - training loss (default): 3419.6253, validation loss: 9670.5370\n",
      "2025-05-20 01:03:43 [INFO]: Epoch 025 - training loss (default): 3419.4964, validation loss: 9658.7696\n",
      "2025-05-20 01:03:44 [INFO]: Epoch 026 - training loss (default): 3419.3867, validation loss: 9647.4997\n",
      "2025-05-20 01:03:44 [INFO]: Epoch 027 - training loss (default): 3419.2389, validation loss: 9636.6048\n",
      "2025-05-20 01:03:45 [INFO]: Epoch 028 - training loss (default): 3419.1694, validation loss: 9625.8574\n",
      "2025-05-20 01:03:46 [INFO]: Epoch 029 - training loss (default): 3419.0160, validation loss: 9615.8770\n",
      "2025-05-20 01:03:46 [INFO]: Epoch 030 - training loss (default): 3418.9587, validation loss: 9606.5270\n",
      "2025-05-20 01:03:47 [INFO]: Epoch 031 - training loss (default): 3418.8278, validation loss: 9596.3511\n",
      "2025-05-20 01:03:48 [INFO]: Epoch 032 - training loss (default): 3418.7857, validation loss: 9587.5374\n",
      "2025-05-20 01:03:48 [INFO]: Epoch 033 - training loss (default): 3418.6895, validation loss: 9578.3954\n",
      "2025-05-20 01:03:49 [INFO]: Epoch 034 - training loss (default): 3418.6409, validation loss: 9569.6793\n",
      "2025-05-20 01:03:50 [INFO]: Epoch 035 - training loss (default): 3418.5844, validation loss: 9561.5251\n",
      "2025-05-20 01:03:50 [INFO]: Epoch 036 - training loss (default): 3418.5137, validation loss: 9552.9770\n",
      "2025-05-20 01:03:51 [INFO]: Epoch 037 - training loss (default): 3418.4498, validation loss: 9544.8339\n",
      "2025-05-20 01:03:52 [INFO]: Epoch 038 - training loss (default): 3418.3581, validation loss: 9537.1340\n",
      "2025-05-20 01:03:52 [INFO]: Epoch 039 - training loss (default): 3418.3235, validation loss: 9529.2350\n",
      "2025-05-20 01:03:53 [INFO]: Epoch 040 - training loss (default): 3418.3272, validation loss: 9522.0138\n",
      "2025-05-20 01:03:54 [INFO]: Epoch 041 - training loss (default): 3418.2426, validation loss: 9514.6692\n",
      "2025-05-20 01:03:54 [INFO]: Epoch 042 - training loss (default): 3418.1990, validation loss: 9507.8604\n",
      "2025-05-20 01:03:55 [INFO]: Epoch 043 - training loss (default): 3418.1565, validation loss: 9500.7169\n",
      "2025-05-20 01:03:56 [INFO]: Epoch 044 - training loss (default): 3418.1353, validation loss: 9494.2345\n",
      "2025-05-20 01:03:57 [INFO]: Epoch 045 - training loss (default): 3418.0903, validation loss: 9487.3729\n",
      "2025-05-20 01:03:57 [INFO]: Epoch 046 - training loss (default): 3418.1154, validation loss: 9480.8564\n",
      "2025-05-20 01:03:58 [INFO]: Epoch 047 - training loss (default): 3418.0064, validation loss: 9474.4524\n",
      "2025-05-20 01:03:59 [INFO]: Epoch 048 - training loss (default): 3417.9950, validation loss: 9468.3085\n",
      "2025-05-20 01:03:59 [INFO]: Epoch 049 - training loss (default): 3417.9648, validation loss: 9462.1103\n",
      "2025-05-20 01:04:00 [INFO]: Epoch 050 - training loss (default): 3417.9223, validation loss: 9456.2529\n",
      "2025-05-20 01:04:00 [INFO]: Finished training. The best model is from epoch#50.\n",
      "2025-05-20 01:04:00 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010326/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-20 01:04:00,884] Trial 11 finished with value: 7731.685925499208 and parameters: {'lr': 0.00020666308989589628, 'epochs': 50, 'batch_size': 96, 'length_scale': 2.8693244504795192, 'beta': 0.6965711335224195}. Best is trial 4 with value: 7664.664779482548.\n",
      "2025-05-20 01:04:00 [INFO]: Using the given device: cuda\n",
      "2025-05-20 01:04:00 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010400\n",
      "2025-05-20 01:04:00 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010400/tensorboard\n",
      "2025-05-20 01:04:00 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/4038f4670a344eaca00d15fbf8094028\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 01:04:01 [INFO]: Epoch 001 - training loss (default): 1745.7657, validation loss: 1533.1462\n",
      "2025-05-20 01:04:02 [INFO]: Epoch 002 - training loss (default): 1035.1328, validation loss: 1492.9006\n",
      "2025-05-20 01:04:02 [INFO]: Epoch 003 - training loss (default): 876.7581, validation loss: 1472.5517\n",
      "2025-05-20 01:04:03 [INFO]: Epoch 004 - training loss (default): 862.2320, validation loss: 1462.5298\n",
      "2025-05-20 01:04:04 [INFO]: Epoch 005 - training loss (default): 858.4526, validation loss: 1455.5197\n",
      "2025-05-20 01:04:05 [INFO]: Epoch 006 - training loss (default): 856.2639, validation loss: 1449.9067\n",
      "2025-05-20 01:04:05 [INFO]: Epoch 007 - training loss (default): 854.8336, validation loss: 1445.0288\n",
      "2025-05-20 01:04:06 [INFO]: Epoch 008 - training loss (default): 853.9370, validation loss: 1440.9417\n",
      "2025-05-20 01:04:07 [INFO]: Epoch 009 - training loss (default): 853.2285, validation loss: 1437.0120\n",
      "2025-05-20 01:04:07 [INFO]: Epoch 010 - training loss (default): 852.7816, validation loss: 1433.4145\n",
      "2025-05-20 01:04:08 [INFO]: Epoch 011 - training loss (default): 852.3824, validation loss: 1430.1472\n",
      "2025-05-20 01:04:09 [INFO]: Epoch 012 - training loss (default): 852.0723, validation loss: 1427.1014\n",
      "2025-05-20 01:04:09 [INFO]: Epoch 013 - training loss (default): 851.7677, validation loss: 1424.4055\n",
      "2025-05-20 01:04:10 [INFO]: Epoch 014 - training loss (default): 851.5234, validation loss: 1421.6740\n",
      "2025-05-20 01:04:11 [INFO]: Epoch 015 - training loss (default): 851.2160, validation loss: 1419.2296\n",
      "2025-05-20 01:04:11 [INFO]: Epoch 016 - training loss (default): 851.0364, validation loss: 1416.8388\n",
      "2025-05-20 01:04:12 [INFO]: Epoch 017 - training loss (default): 850.8269, validation loss: 1414.7034\n",
      "2025-05-20 01:04:13 [INFO]: Epoch 018 - training loss (default): 850.6352, validation loss: 1412.6769\n",
      "2025-05-20 01:04:13 [INFO]: Epoch 019 - training loss (default): 850.4899, validation loss: 1410.7069\n",
      "2025-05-20 01:04:14 [INFO]: Epoch 020 - training loss (default): 850.3822, validation loss: 1408.9288\n",
      "2025-05-20 01:04:15 [INFO]: Epoch 021 - training loss (default): 850.1735, validation loss: 1407.1377\n",
      "2025-05-20 01:04:15 [INFO]: Epoch 022 - training loss (default): 850.0332, validation loss: 1405.4763\n",
      "2025-05-20 01:04:16 [INFO]: Epoch 023 - training loss (default): 849.8982, validation loss: 1403.9449\n",
      "2025-05-20 01:04:17 [INFO]: Epoch 024 - training loss (default): 849.8261, validation loss: 1402.4680\n",
      "2025-05-20 01:04:17 [INFO]: Epoch 025 - training loss (default): 849.7245, validation loss: 1401.0063\n",
      "2025-05-20 01:04:18 [INFO]: Epoch 026 - training loss (default): 849.6368, validation loss: 1399.6595\n",
      "2025-05-20 01:04:19 [INFO]: Epoch 027 - training loss (default): 849.5330, validation loss: 1398.3243\n",
      "2025-05-20 01:04:19 [INFO]: Epoch 028 - training loss (default): 849.4234, validation loss: 1396.9894\n",
      "2025-05-20 01:04:20 [INFO]: Epoch 029 - training loss (default): 849.3797, validation loss: 1395.7710\n",
      "2025-05-20 01:04:21 [INFO]: Epoch 030 - training loss (default): 849.3086, validation loss: 1394.5282\n",
      "2025-05-20 01:04:22 [INFO]: Epoch 031 - training loss (default): 849.2560, validation loss: 1393.3785\n",
      "2025-05-20 01:04:22 [INFO]: Epoch 032 - training loss (default): 849.2188, validation loss: 1392.2396\n",
      "2025-05-20 01:04:23 [INFO]: Epoch 033 - training loss (default): 849.1470, validation loss: 1391.1679\n",
      "2025-05-20 01:04:24 [INFO]: Epoch 034 - training loss (default): 849.1571, validation loss: 1390.0785\n",
      "2025-05-20 01:04:24 [INFO]: Epoch 035 - training loss (default): 849.0201, validation loss: 1389.0735\n",
      "2025-05-20 01:04:25 [INFO]: Epoch 036 - training loss (default): 848.9964, validation loss: 1388.0926\n",
      "2025-05-20 01:04:26 [INFO]: Epoch 037 - training loss (default): 848.9786, validation loss: 1387.1098\n",
      "2025-05-20 01:04:26 [INFO]: Epoch 038 - training loss (default): 848.9223, validation loss: 1386.2092\n",
      "2025-05-20 01:04:27 [INFO]: Epoch 039 - training loss (default): 848.8468, validation loss: 1385.2795\n",
      "2025-05-20 01:04:28 [INFO]: Epoch 040 - training loss (default): 848.8092, validation loss: 1384.4346\n",
      "2025-05-20 01:04:28 [INFO]: Epoch 041 - training loss (default): 848.7648, validation loss: 1383.5943\n",
      "2025-05-20 01:04:29 [INFO]: Epoch 042 - training loss (default): 848.6592, validation loss: 1382.7363\n",
      "2025-05-20 01:04:30 [INFO]: Epoch 043 - training loss (default): 848.6444, validation loss: 1381.9032\n",
      "2025-05-20 01:04:30 [INFO]: Epoch 044 - training loss (default): 848.6243, validation loss: 1381.0704\n",
      "2025-05-20 01:04:31 [INFO]: Epoch 045 - training loss (default): 848.6075, validation loss: 1380.2122\n",
      "2025-05-20 01:04:32 [INFO]: Epoch 046 - training loss (default): 848.6044, validation loss: 1379.3994\n",
      "2025-05-20 01:04:32 [INFO]: Epoch 047 - training loss (default): 848.5491, validation loss: 1378.6099\n",
      "2025-05-20 01:04:33 [INFO]: Epoch 048 - training loss (default): 848.5558, validation loss: 1377.7954\n",
      "2025-05-20 01:04:34 [INFO]: Epoch 049 - training loss (default): 848.4985, validation loss: 1376.9785\n",
      "2025-05-20 01:04:34 [INFO]: Finished training. The best model is from epoch#49.\n",
      "2025-05-20 01:04:34 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010400/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-20 01:04:34,776] Trial 12 finished with value: 7669.15731490494 and parameters: {'lr': 0.0002005559598063997, 'epochs': 49, 'batch_size': 96, 'length_scale': 2.023676863540397, 'beta': 0.667226308340292}. Best is trial 4 with value: 7664.664779482548.\n",
      "2025-05-20 01:04:34 [INFO]: Using the given device: cuda\n",
      "2025-05-20 01:04:34 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010434\n",
      "2025-05-20 01:04:34 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010434/tensorboard\n",
      "2025-05-20 01:04:34 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/04e4c0e53a854d8cb1deac168cd57366\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 01:04:35 [INFO]: Epoch 001 - training loss (default): 1472.4955, validation loss: 1241.1260\n",
      "2025-05-20 01:04:36 [INFO]: Epoch 002 - training loss (default): 903.3558, validation loss: 1206.6314\n",
      "2025-05-20 01:04:37 [INFO]: Epoch 003 - training loss (default): 809.5294, validation loss: 1193.2080\n",
      "2025-05-20 01:04:38 [INFO]: Epoch 004 - training loss (default): 799.6141, validation loss: 1185.9316\n",
      "2025-05-20 01:04:38 [INFO]: Epoch 005 - training loss (default): 796.5939, validation loss: 1180.1626\n",
      "2025-05-20 01:04:39 [INFO]: Epoch 006 - training loss (default): 794.8908, validation loss: 1175.4418\n",
      "2025-05-20 01:04:40 [INFO]: Epoch 007 - training loss (default): 793.8057, validation loss: 1171.4098\n",
      "2025-05-20 01:04:41 [INFO]: Epoch 008 - training loss (default): 793.2595, validation loss: 1167.9206\n",
      "2025-05-20 01:04:42 [INFO]: Epoch 009 - training loss (default): 792.6180, validation loss: 1164.7672\n",
      "2025-05-20 01:04:42 [INFO]: Epoch 010 - training loss (default): 792.2148, validation loss: 1161.9093\n",
      "2025-05-20 01:04:43 [INFO]: Epoch 011 - training loss (default): 791.8865, validation loss: 1159.3305\n",
      "2025-05-20 01:04:44 [INFO]: Epoch 012 - training loss (default): 791.6291, validation loss: 1157.0127\n",
      "2025-05-20 01:04:45 [INFO]: Epoch 013 - training loss (default): 791.4303, validation loss: 1154.9057\n",
      "2025-05-20 01:04:46 [INFO]: Epoch 014 - training loss (default): 791.1504, validation loss: 1152.8838\n",
      "2025-05-20 01:04:46 [INFO]: Epoch 015 - training loss (default): 791.2350, validation loss: 1150.9921\n",
      "2025-05-20 01:04:47 [INFO]: Epoch 016 - training loss (default): 790.8833, validation loss: 1149.2192\n",
      "2025-05-20 01:04:48 [INFO]: Epoch 017 - training loss (default): 790.7214, validation loss: 1147.4889\n",
      "2025-05-20 01:04:49 [INFO]: Epoch 018 - training loss (default): 790.5465, validation loss: 1145.8118\n",
      "2025-05-20 01:04:50 [INFO]: Epoch 019 - training loss (default): 790.5259, validation loss: 1144.2371\n",
      "2025-05-20 01:04:50 [INFO]: Epoch 020 - training loss (default): 790.3378, validation loss: 1142.7794\n",
      "2025-05-20 01:04:51 [INFO]: Epoch 021 - training loss (default): 790.2658, validation loss: 1141.3394\n",
      "2025-05-20 01:04:52 [INFO]: Epoch 022 - training loss (default): 790.2042, validation loss: 1139.9370\n",
      "2025-05-20 01:04:53 [INFO]: Epoch 023 - training loss (default): 790.1416, validation loss: 1138.6813\n",
      "2025-05-20 01:04:54 [INFO]: Epoch 024 - training loss (default): 790.0862, validation loss: 1137.3887\n",
      "2025-05-20 01:04:54 [INFO]: Epoch 025 - training loss (default): 789.8873, validation loss: 1136.1908\n",
      "2025-05-20 01:04:55 [INFO]: Epoch 026 - training loss (default): 789.9005, validation loss: 1135.0272\n",
      "2025-05-20 01:04:56 [INFO]: Epoch 027 - training loss (default): 789.7095, validation loss: 1133.9706\n",
      "2025-05-20 01:04:57 [INFO]: Epoch 028 - training loss (default): 789.6956, validation loss: 1132.9220\n",
      "2025-05-20 01:04:58 [INFO]: Epoch 029 - training loss (default): 789.6597, validation loss: 1131.9107\n",
      "2025-05-20 01:04:58 [INFO]: Epoch 030 - training loss (default): 789.4677, validation loss: 1130.9215\n",
      "2025-05-20 01:04:59 [INFO]: Epoch 031 - training loss (default): 789.5236, validation loss: 1129.8978\n",
      "2025-05-20 01:05:00 [INFO]: Epoch 032 - training loss (default): 789.4986, validation loss: 1128.9976\n",
      "2025-05-20 01:05:01 [INFO]: Epoch 033 - training loss (default): 789.4657, validation loss: 1128.0722\n",
      "2025-05-20 01:05:02 [INFO]: Epoch 034 - training loss (default): 789.1012, validation loss: 1127.1588\n",
      "2025-05-20 01:05:02 [INFO]: Epoch 035 - training loss (default): 789.2758, validation loss: 1126.2270\n",
      "2025-05-20 01:05:03 [INFO]: Epoch 036 - training loss (default): 789.0569, validation loss: 1125.2869\n",
      "2025-05-20 01:05:04 [INFO]: Epoch 037 - training loss (default): 789.2766, validation loss: 1124.5147\n",
      "2025-05-20 01:05:05 [INFO]: Epoch 038 - training loss (default): 788.9555, validation loss: 1123.6023\n",
      "2025-05-20 01:05:06 [INFO]: Epoch 039 - training loss (default): 789.0867, validation loss: 1122.7916\n",
      "2025-05-20 01:05:06 [INFO]: Finished training. The best model is from epoch#39.\n",
      "2025-05-20 01:05:06 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010434/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-20 01:05:06,593] Trial 13 finished with value: 7670.815126466501 and parameters: {'lr': 0.00015039393075808947, 'epochs': 39, 'batch_size': 64, 'length_scale': 1.8094513531125571, 'beta': 0.9150480155907227}. Best is trial 4 with value: 7664.664779482548.\n",
      "2025-05-20 01:05:06 [INFO]: Using the given device: cuda\n",
      "2025-05-20 01:05:06 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010506\n",
      "2025-05-20 01:05:06 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010506/tensorboard\n",
      "2025-05-20 01:05:06 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/f38bbe1d526d4485bb23a0493fc7d0df\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 01:05:07 [INFO]: Epoch 001 - training loss (default): 2969.6740, validation loss: 2829.6861\n",
      "2025-05-20 01:05:08 [INFO]: Epoch 002 - training loss (default): 1820.8667, validation loss: 2769.4239\n",
      "2025-05-20 01:05:08 [INFO]: Epoch 003 - training loss (default): 1309.5530, validation loss: 2724.3452\n",
      "2025-05-20 01:05:09 [INFO]: Epoch 004 - training loss (default): 1223.6584, validation loss: 2699.3177\n",
      "2025-05-20 01:05:10 [INFO]: Epoch 005 - training loss (default): 1206.0836, validation loss: 2682.9581\n",
      "2025-05-20 01:05:11 [INFO]: Epoch 006 - training loss (default): 1199.5232, validation loss: 2670.8293\n",
      "2025-05-20 01:05:12 [INFO]: Epoch 007 - training loss (default): 1195.9972, validation loss: 2660.6425\n",
      "2025-05-20 01:05:12 [INFO]: Epoch 008 - training loss (default): 1193.7717, validation loss: 2651.9840\n",
      "2025-05-20 01:05:13 [INFO]: Epoch 009 - training loss (default): 1192.2481, validation loss: 2644.4151\n",
      "2025-05-20 01:05:14 [INFO]: Epoch 010 - training loss (default): 1191.1324, validation loss: 2637.7565\n",
      "2025-05-20 01:05:14 [INFO]: Epoch 011 - training loss (default): 1190.4422, validation loss: 2631.8481\n",
      "2025-05-20 01:05:15 [INFO]: Epoch 012 - training loss (default): 1189.8651, validation loss: 2626.4501\n",
      "2025-05-20 01:05:16 [INFO]: Epoch 013 - training loss (default): 1189.3577, validation loss: 2621.6800\n",
      "2025-05-20 01:05:16 [INFO]: Epoch 014 - training loss (default): 1188.9035, validation loss: 2617.1749\n",
      "2025-05-20 01:05:17 [INFO]: Epoch 015 - training loss (default): 1188.5849, validation loss: 2612.9053\n",
      "2025-05-20 01:05:18 [INFO]: Epoch 016 - training loss (default): 1188.3544, validation loss: 2608.9323\n",
      "2025-05-20 01:05:18 [INFO]: Epoch 017 - training loss (default): 1188.0396, validation loss: 2605.3076\n",
      "2025-05-20 01:05:19 [INFO]: Epoch 018 - training loss (default): 1187.7640, validation loss: 2601.9986\n",
      "2025-05-20 01:05:20 [INFO]: Epoch 019 - training loss (default): 1187.5685, validation loss: 2598.6790\n",
      "2025-05-20 01:05:21 [INFO]: Epoch 020 - training loss (default): 1187.3518, validation loss: 2595.6446\n",
      "2025-05-20 01:05:21 [INFO]: Epoch 021 - training loss (default): 1187.2074, validation loss: 2592.7630\n",
      "2025-05-20 01:05:22 [INFO]: Epoch 022 - training loss (default): 1186.9842, validation loss: 2590.0053\n",
      "2025-05-20 01:05:23 [INFO]: Epoch 023 - training loss (default): 1186.7988, validation loss: 2587.2769\n",
      "2025-05-20 01:05:23 [INFO]: Epoch 024 - training loss (default): 1186.6468, validation loss: 2584.5463\n",
      "2025-05-20 01:05:24 [INFO]: Epoch 025 - training loss (default): 1186.5436, validation loss: 2581.9252\n",
      "2025-05-20 01:05:25 [INFO]: Epoch 026 - training loss (default): 1186.3690, validation loss: 2579.4734\n",
      "2025-05-20 01:05:25 [INFO]: Epoch 027 - training loss (default): 1186.2553, validation loss: 2577.0281\n",
      "2025-05-20 01:05:26 [INFO]: Epoch 028 - training loss (default): 1186.1771, validation loss: 2574.6870\n",
      "2025-05-20 01:05:27 [INFO]: Epoch 029 - training loss (default): 1186.0473, validation loss: 2572.3539\n",
      "2025-05-20 01:05:27 [INFO]: Epoch 030 - training loss (default): 1186.0167, validation loss: 2570.1635\n",
      "2025-05-20 01:05:28 [INFO]: Epoch 031 - training loss (default): 1185.8519, validation loss: 2568.0694\n",
      "2025-05-20 01:05:29 [INFO]: Epoch 032 - training loss (default): 1185.8292, validation loss: 2565.8897\n",
      "2025-05-20 01:05:29 [INFO]: Epoch 033 - training loss (default): 1185.7506, validation loss: 2563.9126\n",
      "2025-05-20 01:05:30 [INFO]: Epoch 034 - training loss (default): 1185.6839, validation loss: 2561.9330\n",
      "2025-05-20 01:05:31 [INFO]: Epoch 035 - training loss (default): 1185.6074, validation loss: 2560.0315\n",
      "2025-05-20 01:05:32 [INFO]: Epoch 036 - training loss (default): 1185.5703, validation loss: 2558.2198\n",
      "2025-05-20 01:05:32 [INFO]: Epoch 037 - training loss (default): 1185.4742, validation loss: 2556.3630\n",
      "2025-05-20 01:05:33 [INFO]: Epoch 038 - training loss (default): 1185.4206, validation loss: 2554.5339\n",
      "2025-05-20 01:05:34 [INFO]: Epoch 039 - training loss (default): 1185.3241, validation loss: 2552.7499\n",
      "2025-05-20 01:05:34 [INFO]: Epoch 040 - training loss (default): 1185.3050, validation loss: 2551.0346\n",
      "2025-05-20 01:05:35 [INFO]: Epoch 041 - training loss (default): 1185.2758, validation loss: 2549.3417\n",
      "2025-05-20 01:05:35 [INFO]: Finished training. The best model is from epoch#41.\n",
      "2025-05-20 01:05:35 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010506/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-20 01:05:35,867] Trial 14 finished with value: 7665.744823679699 and parameters: {'lr': 0.0001503162230264827, 'epochs': 41, 'batch_size': 96, 'length_scale': 2.2121061893036917, 'beta': 0.7723305118047362}. Best is trial 4 with value: 7664.664779482548.\n",
      "2025-05-20 01:05:35 [INFO]: Using the given device: cuda\n",
      "2025-05-20 01:05:35 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010535\n",
      "2025-05-20 01:05:35 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010535/tensorboard\n",
      "2025-05-20 01:05:35 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/75383959ecf14212b1dcc0519bc6d69b\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 01:05:36 [INFO]: Epoch 001 - training loss (default): 23567.9265, validation loss: 28805.5323\n",
      "2025-05-20 01:05:37 [INFO]: Epoch 002 - training loss (default): 11046.0269, validation loss: 28100.8187\n",
      "2025-05-20 01:05:38 [INFO]: Epoch 003 - training loss (default): 9075.0433, validation loss: 27782.6474\n",
      "2025-05-20 01:05:39 [INFO]: Epoch 004 - training loss (default): 8881.0312, validation loss: 27578.3022\n",
      "2025-05-20 01:05:39 [INFO]: Epoch 005 - training loss (default): 8825.5529, validation loss: 27430.6219\n",
      "2025-05-20 01:05:40 [INFO]: Epoch 006 - training loss (default): 8801.7859, validation loss: 27317.8296\n",
      "2025-05-20 01:05:41 [INFO]: Epoch 007 - training loss (default): 8789.2793, validation loss: 27212.8851\n",
      "2025-05-20 01:05:42 [INFO]: Epoch 008 - training loss (default): 8782.1162, validation loss: 27121.6610\n",
      "2025-05-20 01:05:43 [INFO]: Epoch 009 - training loss (default): 8777.8493, validation loss: 27042.6999\n",
      "2025-05-20 01:05:44 [INFO]: Epoch 010 - training loss (default): 8774.6671, validation loss: 26968.3554\n",
      "2025-05-20 01:05:44 [INFO]: Epoch 011 - training loss (default): 8772.7100, validation loss: 26902.5671\n",
      "2025-05-20 01:05:45 [INFO]: Epoch 012 - training loss (default): 8771.4045, validation loss: 26839.1580\n",
      "2025-05-20 01:05:46 [INFO]: Epoch 013 - training loss (default): 8770.1733, validation loss: 26777.7397\n",
      "2025-05-20 01:05:47 [INFO]: Epoch 014 - training loss (default): 8769.5457, validation loss: 26720.6864\n",
      "2025-05-20 01:05:48 [INFO]: Epoch 015 - training loss (default): 8768.7247, validation loss: 26666.0839\n",
      "2025-05-20 01:05:48 [INFO]: Epoch 016 - training loss (default): 8768.3386, validation loss: 26616.4861\n",
      "2025-05-20 01:05:49 [INFO]: Epoch 017 - training loss (default): 8767.5596, validation loss: 26570.3605\n",
      "2025-05-20 01:05:50 [INFO]: Epoch 018 - training loss (default): 8767.2953, validation loss: 26523.2908\n",
      "2025-05-20 01:05:51 [INFO]: Epoch 019 - training loss (default): 8766.8888, validation loss: 26482.2209\n",
      "2025-05-20 01:05:52 [INFO]: Epoch 020 - training loss (default): 8766.6891, validation loss: 26442.2701\n",
      "2025-05-20 01:05:52 [INFO]: Epoch 021 - training loss (default): 8766.6281, validation loss: 26402.5145\n",
      "2025-05-20 01:05:53 [INFO]: Epoch 022 - training loss (default): 8766.2337, validation loss: 26366.0480\n",
      "2025-05-20 01:05:54 [INFO]: Epoch 023 - training loss (default): 8765.8615, validation loss: 26330.7621\n",
      "2025-05-20 01:05:55 [INFO]: Epoch 024 - training loss (default): 8765.7085, validation loss: 26296.7398\n",
      "2025-05-20 01:05:56 [INFO]: Epoch 025 - training loss (default): 8765.5642, validation loss: 26264.3606\n",
      "2025-05-20 01:05:56 [INFO]: Epoch 026 - training loss (default): 8765.2004, validation loss: 26233.1351\n",
      "2025-05-20 01:05:57 [INFO]: Epoch 027 - training loss (default): 8765.1007, validation loss: 26202.7410\n",
      "2025-05-20 01:05:58 [INFO]: Epoch 028 - training loss (default): 8765.0961, validation loss: 26174.6498\n",
      "2025-05-20 01:05:59 [INFO]: Epoch 029 - training loss (default): 8764.7538, validation loss: 26145.4373\n",
      "2025-05-20 01:06:00 [INFO]: Epoch 030 - training loss (default): 8764.5531, validation loss: 26118.9331\n",
      "2025-05-20 01:06:01 [INFO]: Epoch 031 - training loss (default): 8764.5758, validation loss: 26093.1360\n",
      "2025-05-20 01:06:01 [INFO]: Epoch 032 - training loss (default): 8764.3466, validation loss: 26067.6175\n",
      "2025-05-20 01:06:02 [INFO]: Epoch 033 - training loss (default): 8764.0701, validation loss: 26043.2009\n",
      "2025-05-20 01:06:03 [INFO]: Epoch 034 - training loss (default): 8764.0864, validation loss: 26020.2567\n",
      "2025-05-20 01:06:04 [INFO]: Epoch 035 - training loss (default): 8764.1174, validation loss: 25997.6846\n",
      "2025-05-20 01:06:05 [INFO]: Epoch 036 - training loss (default): 8763.9367, validation loss: 25974.7943\n",
      "2025-05-20 01:06:05 [INFO]: Finished training. The best model is from epoch#36.\n",
      "2025-05-20 01:06:05 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010535/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-20 01:06:05,488] Trial 15 finished with value: 7785.683721577448 and parameters: {'lr': 0.00013776655442489678, 'epochs': 36, 'batch_size': 64, 'length_scale': 3.381012701607, 'beta': 0.822443359429768}. Best is trial 4 with value: 7664.664779482548.\n",
      "2025-05-20 01:06:05 [INFO]: Using the given device: cuda\n",
      "2025-05-20 01:06:05 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010605\n",
      "2025-05-20 01:06:05 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010605/tensorboard\n",
      "2025-05-20 01:06:05 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/eaec2f1376154c00bafbc69cd142f268\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 01:06:06 [INFO]: Epoch 001 - training loss (default): 4493.9357, validation loss: 4315.7177\n",
      "2025-05-20 01:06:06 [INFO]: Epoch 002 - training loss (default): 3400.7521, validation loss: 4272.0326\n",
      "2025-05-20 01:06:07 [INFO]: Epoch 003 - training loss (default): 2309.5217, validation loss: 4226.7852\n",
      "2025-05-20 01:06:08 [INFO]: Epoch 004 - training loss (default): 1808.7393, validation loss: 4191.9220\n",
      "2025-05-20 01:06:08 [INFO]: Epoch 005 - training loss (default): 1665.8845, validation loss: 4170.1793\n",
      "2025-05-20 01:06:09 [INFO]: Epoch 006 - training loss (default): 1624.2300, validation loss: 4155.3594\n",
      "2025-05-20 01:06:10 [INFO]: Epoch 007 - training loss (default): 1607.6605, validation loss: 4144.3818\n",
      "2025-05-20 01:06:10 [INFO]: Epoch 008 - training loss (default): 1599.6806, validation loss: 4134.3438\n",
      "2025-05-20 01:06:11 [INFO]: Epoch 009 - training loss (default): 1594.9072, validation loss: 4125.6857\n",
      "2025-05-20 01:06:12 [INFO]: Epoch 010 - training loss (default): 1592.0309, validation loss: 4118.0476\n",
      "2025-05-20 01:06:12 [INFO]: Epoch 011 - training loss (default): 1589.7917, validation loss: 4111.2667\n",
      "2025-05-20 01:06:13 [INFO]: Epoch 012 - training loss (default): 1588.1054, validation loss: 4104.8147\n",
      "2025-05-20 01:06:14 [INFO]: Epoch 013 - training loss (default): 1586.9706, validation loss: 4098.7951\n",
      "2025-05-20 01:06:14 [INFO]: Epoch 014 - training loss (default): 1586.0859, validation loss: 4092.8866\n",
      "2025-05-20 01:06:15 [INFO]: Epoch 015 - training loss (default): 1585.3644, validation loss: 4087.4459\n",
      "2025-05-20 01:06:16 [INFO]: Epoch 016 - training loss (default): 1584.8113, validation loss: 4082.1251\n",
      "2025-05-20 01:06:16 [INFO]: Epoch 017 - training loss (default): 1584.2679, validation loss: 4076.9507\n",
      "2025-05-20 01:06:17 [INFO]: Epoch 018 - training loss (default): 1583.8722, validation loss: 4072.0492\n",
      "2025-05-20 01:06:18 [INFO]: Epoch 019 - training loss (default): 1583.5835, validation loss: 4067.4024\n",
      "2025-05-20 01:06:18 [INFO]: Epoch 020 - training loss (default): 1583.2437, validation loss: 4063.0600\n",
      "2025-05-20 01:06:19 [INFO]: Epoch 021 - training loss (default): 1582.9264, validation loss: 4058.8223\n",
      "2025-05-20 01:06:20 [INFO]: Epoch 022 - training loss (default): 1582.7536, validation loss: 4054.8098\n",
      "2025-05-20 01:06:20 [INFO]: Epoch 023 - training loss (default): 1582.4687, validation loss: 4050.9137\n",
      "2025-05-20 01:06:21 [INFO]: Epoch 024 - training loss (default): 1582.2149, validation loss: 4047.0673\n",
      "2025-05-20 01:06:22 [INFO]: Epoch 025 - training loss (default): 1582.0900, validation loss: 4043.4991\n",
      "2025-05-20 01:06:22 [INFO]: Epoch 026 - training loss (default): 1581.8818, validation loss: 4039.9580\n",
      "2025-05-20 01:06:23 [INFO]: Epoch 027 - training loss (default): 1581.7225, validation loss: 4036.5840\n",
      "2025-05-20 01:06:24 [INFO]: Epoch 028 - training loss (default): 1581.4847, validation loss: 4033.2578\n",
      "2025-05-20 01:06:24 [INFO]: Epoch 029 - training loss (default): 1581.3699, validation loss: 4030.0789\n",
      "2025-05-20 01:06:25 [INFO]: Epoch 030 - training loss (default): 1581.2908, validation loss: 4026.6584\n",
      "2025-05-20 01:06:26 [INFO]: Epoch 031 - training loss (default): 1581.1593, validation loss: 4023.4072\n",
      "2025-05-20 01:06:26 [INFO]: Epoch 032 - training loss (default): 1581.0389, validation loss: 4020.2147\n",
      "2025-05-20 01:06:27 [INFO]: Epoch 033 - training loss (default): 1580.9129, validation loss: 4017.1243\n",
      "2025-05-20 01:06:28 [INFO]: Epoch 034 - training loss (default): 1580.7893, validation loss: 4013.9863\n",
      "2025-05-20 01:06:28 [INFO]: Epoch 035 - training loss (default): 1580.5595, validation loss: 4011.0035\n",
      "2025-05-20 01:06:29 [INFO]: Epoch 036 - training loss (default): 1580.6435, validation loss: 4008.1236\n",
      "2025-05-20 01:06:29 [INFO]: Epoch 037 - training loss (default): 1580.4497, validation loss: 4005.2955\n",
      "2025-05-20 01:06:30 [INFO]: Epoch 038 - training loss (default): 1580.3945, validation loss: 4002.5657\n",
      "2025-05-20 01:06:31 [INFO]: Epoch 039 - training loss (default): 1580.3112, validation loss: 3999.8264\n",
      "2025-05-20 01:06:31 [INFO]: Epoch 040 - training loss (default): 1580.2978, validation loss: 3997.1793\n",
      "2025-05-20 01:06:32 [INFO]: Epoch 041 - training loss (default): 1580.2367, validation loss: 3994.5420\n",
      "2025-05-20 01:06:33 [INFO]: Epoch 042 - training loss (default): 1580.1763, validation loss: 3991.9985\n",
      "2025-05-20 01:06:33 [INFO]: Finished training. The best model is from epoch#42.\n",
      "2025-05-20 01:06:33 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010605/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-20 01:06:33,702] Trial 16 finished with value: 7686.786999274495 and parameters: {'lr': 0.00012482136418713805, 'epochs': 42, 'batch_size': 128, 'length_scale': 2.3752912420933403, 'beta': 0.7978168675538333}. Best is trial 4 with value: 7664.664779482548.\n",
      "2025-05-20 01:06:33 [INFO]: Using the given device: cuda\n",
      "2025-05-20 01:06:33 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010633\n",
      "2025-05-20 01:06:33 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010633/tensorboard\n",
      "2025-05-20 01:06:33 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/a0464e71da4e47cba7e96dc20d785590\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 01:06:34 [INFO]: Epoch 001 - training loss (default): 630.3319, validation loss: 182.6553\n",
      "2025-05-20 01:06:35 [INFO]: Epoch 002 - training loss (default): 546.0525, validation loss: 180.0120\n",
      "2025-05-20 01:06:35 [INFO]: Epoch 003 - training loss (default): 526.0583, validation loss: 179.4218\n",
      "2025-05-20 01:06:36 [INFO]: Epoch 004 - training loss (default): 520.8473, validation loss: 178.3478\n",
      "2025-05-20 01:06:37 [INFO]: Epoch 005 - training loss (default): 519.1317, validation loss: 177.6463\n",
      "2025-05-20 01:06:37 [INFO]: Epoch 006 - training loss (default): 518.3940, validation loss: 177.1719\n",
      "2025-05-20 01:06:38 [INFO]: Epoch 007 - training loss (default): 517.9708, validation loss: 176.7881\n",
      "2025-05-20 01:06:39 [INFO]: Epoch 008 - training loss (default): 517.6482, validation loss: 176.4862\n",
      "2025-05-20 01:06:39 [INFO]: Epoch 009 - training loss (default): 517.4393, validation loss: 176.2450\n",
      "2025-05-20 01:06:40 [INFO]: Epoch 010 - training loss (default): 517.1016, validation loss: 176.0259\n",
      "2025-05-20 01:06:41 [INFO]: Epoch 011 - training loss (default): 516.9155, validation loss: 175.8515\n",
      "2025-05-20 01:06:41 [INFO]: Epoch 012 - training loss (default): 516.6890, validation loss: 175.6963\n",
      "2025-05-20 01:06:42 [INFO]: Epoch 013 - training loss (default): 516.4972, validation loss: 175.5794\n",
      "2025-05-20 01:06:43 [INFO]: Epoch 014 - training loss (default): 516.3135, validation loss: 175.4563\n",
      "2025-05-20 01:06:44 [INFO]: Epoch 015 - training loss (default): 516.1020, validation loss: 175.3421\n",
      "2025-05-20 01:06:44 [INFO]: Epoch 016 - training loss (default): 515.9386, validation loss: 175.2686\n",
      "2025-05-20 01:06:45 [INFO]: Epoch 017 - training loss (default): 515.7608, validation loss: 175.1920\n",
      "2025-05-20 01:06:46 [INFO]: Epoch 018 - training loss (default): 515.6484, validation loss: 175.1317\n",
      "2025-05-20 01:06:46 [INFO]: Epoch 019 - training loss (default): 515.5007, validation loss: 175.0942\n",
      "2025-05-20 01:06:47 [INFO]: Epoch 020 - training loss (default): 515.3749, validation loss: 175.0549\n",
      "2025-05-20 01:06:48 [INFO]: Epoch 021 - training loss (default): 515.2309, validation loss: 175.0597\n",
      "2025-05-20 01:06:48 [INFO]: Epoch 022 - training loss (default): 515.1376, validation loss: 175.0882\n",
      "2025-05-20 01:06:49 [INFO]: Epoch 023 - training loss (default): 515.0018, validation loss: 175.1599\n",
      "2025-05-20 01:06:49 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-05-20 01:06:49 [INFO]: Finished training. The best model is from epoch#20.\n",
      "2025-05-20 01:06:49 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010633/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-20 01:06:50,012] Trial 17 finished with value: 7694.538604954915 and parameters: {'lr': 0.00025918970883937993, 'epochs': 32, 'batch_size': 96, 'length_scale': 1.2605550406001043, 'beta': 0.48261393074719566}. Best is trial 4 with value: 7664.664779482548.\n",
      "2025-05-20 01:06:50 [INFO]: Using the given device: cuda\n",
      "2025-05-20 01:06:50 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010650\n",
      "2025-05-20 01:06:50 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010650/tensorboard\n",
      "2025-05-20 01:06:50 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/4d8251559bfa4fcabd576dbc04a7cb3b\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 01:06:50 [INFO]: Epoch 001 - training loss (default): 54201.9327, validation loss: 67773.3948\n",
      "2025-05-20 01:06:51 [INFO]: Epoch 002 - training loss (default): 26064.0104, validation loss: 65885.0124\n",
      "2025-05-20 01:06:52 [INFO]: Epoch 003 - training loss (default): 22824.6580, validation loss: 65168.2907\n",
      "2025-05-20 01:06:53 [INFO]: Epoch 004 - training loss (default): 22569.3374, validation loss: 64755.7475\n",
      "2025-05-20 01:06:54 [INFO]: Epoch 005 - training loss (default): 22497.4194, validation loss: 64425.7128\n",
      "2025-05-20 01:06:54 [INFO]: Epoch 006 - training loss (default): 22465.9395, validation loss: 64145.6955\n",
      "2025-05-20 01:06:55 [INFO]: Epoch 007 - training loss (default): 22449.1510, validation loss: 63905.2061\n",
      "2025-05-20 01:06:56 [INFO]: Epoch 008 - training loss (default): 22439.0125, validation loss: 63695.7872\n",
      "2025-05-20 01:06:57 [INFO]: Epoch 009 - training loss (default): 22432.6114, validation loss: 63512.5545\n",
      "2025-05-20 01:06:58 [INFO]: Epoch 010 - training loss (default): 22427.8128, validation loss: 63346.8681\n",
      "2025-05-20 01:06:58 [INFO]: Epoch 011 - training loss (default): 22424.7963, validation loss: 63196.3396\n",
      "2025-05-20 01:06:59 [INFO]: Epoch 012 - training loss (default): 22422.1335, validation loss: 63054.2854\n",
      "2025-05-20 01:07:00 [INFO]: Epoch 013 - training loss (default): 22420.4132, validation loss: 62928.2812\n",
      "2025-05-20 01:07:01 [INFO]: Epoch 014 - training loss (default): 22418.9011, validation loss: 62807.8824\n",
      "2025-05-20 01:07:02 [INFO]: Epoch 015 - training loss (default): 22417.7201, validation loss: 62694.9759\n",
      "2025-05-20 01:07:03 [INFO]: Epoch 016 - training loss (default): 22416.7542, validation loss: 62590.2063\n",
      "2025-05-20 01:07:03 [INFO]: Epoch 017 - training loss (default): 22415.9026, validation loss: 62489.0869\n",
      "2025-05-20 01:07:04 [INFO]: Epoch 018 - training loss (default): 22415.1767, validation loss: 62395.5071\n",
      "2025-05-20 01:07:05 [INFO]: Epoch 019 - training loss (default): 22414.5962, validation loss: 62304.1955\n",
      "2025-05-20 01:07:06 [INFO]: Epoch 020 - training loss (default): 22413.9484, validation loss: 62219.1096\n",
      "2025-05-20 01:07:07 [INFO]: Epoch 021 - training loss (default): 22413.6138, validation loss: 62138.7645\n",
      "2025-05-20 01:07:07 [INFO]: Epoch 022 - training loss (default): 22413.1304, validation loss: 62063.3118\n",
      "2025-05-20 01:07:08 [INFO]: Epoch 023 - training loss (default): 22412.9127, validation loss: 61980.9855\n",
      "2025-05-20 01:07:09 [INFO]: Epoch 024 - training loss (default): 22412.4214, validation loss: 61894.5648\n",
      "2025-05-20 01:07:09 [INFO]: Finished training. The best model is from epoch#24.\n",
      "2025-05-20 01:07:09 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010650/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-20 01:07:09,943] Trial 18 finished with value: 8044.142798093759 and parameters: {'lr': 0.00015586061225537153, 'epochs': 24, 'batch_size': 64, 'length_scale': 4.7600643770276365, 'beta': 0.7974871928040819}. Best is trial 4 with value: 7664.664779482548.\n",
      "2025-05-20 01:07:09 [INFO]: Using the given device: cuda\n",
      "2025-05-20 01:07:09 [INFO]: Model files will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010709\n",
      "2025-05-20 01:07:09 [INFO]: Tensorboard file will be saved to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010709/tensorboard\n",
      "2025-05-20 01:07:09 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 209,678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/a87360c5d14a499d9e48c7ec1c8b18e2\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 01:07:10 [INFO]: Epoch 001 - training loss (default): 14176.3963, validation loss: 17276.3695\n",
      "2025-05-20 01:07:11 [INFO]: Epoch 002 - training loss (default): 6433.6481, validation loss: 16789.6317\n",
      "2025-05-20 01:07:12 [INFO]: Epoch 003 - training loss (default): 5533.9209, validation loss: 16617.4325\n",
      "2025-05-20 01:07:12 [INFO]: Epoch 004 - training loss (default): 5482.6444, validation loss: 16533.7953\n",
      "2025-05-20 01:07:13 [INFO]: Epoch 005 - training loss (default): 5467.5532, validation loss: 16467.5840\n",
      "2025-05-20 01:07:14 [INFO]: Epoch 006 - training loss (default): 5459.5771, validation loss: 16410.7124\n",
      "2025-05-20 01:07:14 [INFO]: Epoch 007 - training loss (default): 5454.8097, validation loss: 16362.3166\n",
      "2025-05-20 01:07:15 [INFO]: Epoch 008 - training loss (default): 5451.8163, validation loss: 16318.2273\n",
      "2025-05-20 01:07:16 [INFO]: Epoch 009 - training loss (default): 5449.7641, validation loss: 16278.8545\n",
      "2025-05-20 01:07:16 [INFO]: Epoch 010 - training loss (default): 5448.2195, validation loss: 16242.2120\n",
      "2025-05-20 01:07:17 [INFO]: Epoch 011 - training loss (default): 5447.0604, validation loss: 16208.8744\n",
      "2025-05-20 01:07:18 [INFO]: Epoch 012 - training loss (default): 5446.2214, validation loss: 16177.5776\n",
      "2025-05-20 01:07:19 [INFO]: Epoch 013 - training loss (default): 5445.4407, validation loss: 16149.1363\n",
      "2025-05-20 01:07:19 [INFO]: Epoch 014 - training loss (default): 5444.8158, validation loss: 16122.2998\n",
      "2025-05-20 01:07:20 [INFO]: Epoch 015 - training loss (default): 5444.2022, validation loss: 16097.4194\n",
      "2025-05-20 01:07:21 [INFO]: Epoch 016 - training loss (default): 5443.6950, validation loss: 16072.9623\n",
      "2025-05-20 01:07:22 [INFO]: Epoch 017 - training loss (default): 5443.1906, validation loss: 16049.8144\n",
      "2025-05-20 01:07:22 [INFO]: Epoch 018 - training loss (default): 5442.7867, validation loss: 16028.5945\n",
      "2025-05-20 01:07:23 [INFO]: Epoch 019 - training loss (default): 5442.4294, validation loss: 16007.5089\n",
      "2025-05-20 01:07:24 [INFO]: Epoch 020 - training loss (default): 5442.1168, validation loss: 15987.7298\n",
      "2025-05-20 01:07:24 [INFO]: Epoch 021 - training loss (default): 5441.8312, validation loss: 15968.4381\n",
      "2025-05-20 01:07:25 [INFO]: Epoch 022 - training loss (default): 5441.6349, validation loss: 15950.5000\n",
      "2025-05-20 01:07:26 [INFO]: Epoch 023 - training loss (default): 5441.4420, validation loss: 15933.6935\n",
      "2025-05-20 01:07:26 [INFO]: Epoch 024 - training loss (default): 5441.2707, validation loss: 15916.3104\n",
      "2025-05-20 01:07:27 [INFO]: Epoch 025 - training loss (default): 5441.1256, validation loss: 15900.6315\n",
      "2025-05-20 01:07:28 [INFO]: Epoch 026 - training loss (default): 5441.0354, validation loss: 15884.7552\n",
      "2025-05-20 01:07:28 [INFO]: Epoch 027 - training loss (default): 5440.8189, validation loss: 15869.5481\n",
      "2025-05-20 01:07:29 [INFO]: Epoch 028 - training loss (default): 5440.7660, validation loss: 15855.5957\n",
      "2025-05-20 01:07:30 [INFO]: Epoch 029 - training loss (default): 5440.6648, validation loss: 15841.4104\n",
      "2025-05-20 01:07:31 [INFO]: Epoch 030 - training loss (default): 5440.5383, validation loss: 15827.9411\n",
      "2025-05-20 01:07:31 [INFO]: Epoch 031 - training loss (default): 5440.4495, validation loss: 15814.3924\n",
      "2025-05-20 01:07:32 [INFO]: Epoch 032 - training loss (default): 5440.3733, validation loss: 15801.7284\n",
      "2025-05-20 01:07:33 [INFO]: Epoch 033 - training loss (default): 5440.3226, validation loss: 15788.7598\n",
      "2025-05-20 01:07:33 [INFO]: Epoch 034 - training loss (default): 5440.2348, validation loss: 15776.8609\n",
      "2025-05-20 01:07:34 [INFO]: Epoch 035 - training loss (default): 5440.1790, validation loss: 15764.8200\n",
      "2025-05-20 01:07:35 [INFO]: Epoch 036 - training loss (default): 5440.1121, validation loss: 15753.1992\n",
      "2025-05-20 01:07:35 [INFO]: Finished training. The best model is from epoch#36.\n",
      "2025-05-20 01:07:35 [INFO]: Saved the model to /home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model/20250520_T010709/GPVAE.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gp_vae_imputation is (1056, 2, 20, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-20 01:07:35,670] Trial 19 finished with value: 7742.206275543968 and parameters: {'lr': 0.00024272238891417084, 'epochs': 36, 'batch_size': 96, 'length_scale': 3.5478580789988197, 'beta': 0.40201733936255823}. Best is trial 4 with value: 7664.664779482548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run GP-VAE-Trial at: http://localhost:5000/#/experiments/832352739106302318/runs/be94307aeecf4034a1e4d0c9fd03f9e4\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n",
      "Best Parameters: {'lr': 0.00012021352690757769, 'epochs': 43, 'batch_size': 64, 'length_scale': 2.3487754392417886, 'beta': 0.45088192430534046}\n",
      "Best Objective Value: 7664.664779482548\n",
      "🏃 View run GPVAE_Optuna_Study(2) at: http://localhost:5000/#/experiments/832352739106302318/runs/fc110587ae72462385114c2ee013d857\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/832352739106302318\n"
     ]
    }
   ],
   "source": [
    "#Import Pypots Library\n",
    "from pypots.optim import Adam\n",
    "from pypots.imputation import GPVAE\n",
    "#from pypots.utils.metrics import calc_mae\n",
    "from pypots.nn.functional import calc_mae\n",
    "\n",
    "\n",
    "import argparse\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import data_insight\n",
    "from data_insight import setup_duckdb\n",
    "from duckdb import DuckDBPyConnection as DuckDB\n",
    "from duckdb import DuckDBPyRelation as Relation\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "from duckdb import DuckDBPyConnection as DuckDB\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna \n",
    "from optuna.visualization import plot_optimization_history\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, Dataset\n",
    "from pygrinder.missing_completely_at_random import mcar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import sensor_imputation_thesis.shared.load_data as load\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "#PatchTST might be an ideal choise if SAITS is too slow \n",
    "\n",
    "##Drop columns with different indexes while loading data.. Or the mean values \n",
    "\n",
    "df=pd.read_parquet(\"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/ny_df_for_pypots.parquet\")\n",
    "\n",
    "len(df)\n",
    "\n",
    "#current length of the dataframe is 119439\n",
    "\n",
    "# Check nan values in each column\n",
    "for col in df.columns:\n",
    "    print(f\"Column {col} has {df[col].isna().sum()} NaN values\")\n",
    "    missing_rate=df[col].isna().sum()/len(df[col])\n",
    "    print(f\"Column {col} has {missing_rate} Missing_rate\")\n",
    "\n",
    "\n",
    "#Try with smaller dataset, size 4000\n",
    "##SAMPLE the percengtage of the dataset, df.sample (averagely pick samples)\n",
    "#not df.sample cuz it will randomly select \n",
    "original_size=len(df)\n",
    "desired_fraction=0.3 #Select data every 3 minutes \n",
    "step=int(1/desired_fraction) #step_size=10 (sample every 10th (3/10) minute)\n",
    "\n",
    "#Systematic sampling: Start at a random offset to avoid bias \n",
    "start=np.random.randint(0,step) #Random start between 0-9\n",
    "df1=df.iloc[start::step].reset_index(drop=True)\n",
    "\n",
    "print(f\"Original size:{len(df)}, Sampled size: {len(df1)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Custom Dataset class\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Data processing code\n",
    "sensor_cols = [col for col in df1.columns if col != \"time\"]\n",
    "data = df1[sensor_cols].values\n",
    "\n",
    "#¤get feature names for printing mae later \n",
    "feature_names=df1[sensor_cols].columns.tolist()\n",
    "\n",
    "## Convert data to 3D arrays of shape n_samples, n_timesteps, n_features, X_ori refers to the original data without missing values \n",
    "## Reconstruct all columns simultaneously  #num_features: 119\n",
    "n_features = data.shape[1]  # exclude the time column\n",
    "n_steps = 20 #60 (was 60 previously) #(TRY TO CHANGE HERE)  # # window length, 1440 steps = 24 hours of 1-minute data, but here is revised to 60 again\n",
    "#total_elements = data.shape[0] * data.shape[1]\n",
    "n_samples = data.shape[0] // n_steps \n",
    "\n",
    "\n",
    "\n",
    "# Reshape to (n_samples // n_steps, n_steps, n_features)\n",
    "#data_reshaped = data.reshape((n_samples, n_steps, n_features))\n",
    "data_reshaped=data[:n_samples*n_steps].reshape(n_samples,n_steps,n_features)\n",
    "print(f\"Reshaped data:{data.shape}\")\n",
    "\n",
    "#Split into train, test, val, fit scaler only on the train set (prevent data leakage)\n",
    "\n",
    "#train_size = int(0.6 * len(data))\n",
    "#val_size = int(0.2 * len(data))\n",
    "#test_size = len(data) - train_size - val_size\n",
    "\n",
    "#train_data = data_reshaped[:train_size]\n",
    "#val_data = data_reshaped[train_size:train_size + val_size]\n",
    "#test_data= data_reshaped[train_size + val_size:]\n",
    "\n",
    "\n",
    "#Apply time series split \n",
    "#Split into train(60%), val(20%), and test (20%)\n",
    "train_data, temp_data=train_test_split(data_reshaped,test_size=0.4,shuffle=True)\n",
    "val_data, test_data=train_test_split(temp_data, test_size=0.5, shuffle=False)\n",
    "\n",
    "##Normalization is important because of the nature of mse calculation of saits, columns with large \n",
    "#values dominate the loss, making metrics meaningless. SAITS computes MSE/MAE column-wise and averages \n",
    "#them across all columns \n",
    "#  Apply minmax scaler here \n",
    "#normalize each feature independently\n",
    "scalers={}\n",
    "\n",
    "\n",
    "#train_scaled = np.zeros_like(data_reshaped[train_size])  # Initialize the normalized data array\n",
    "#val_scaled=np.zeros_like(data_reshaped[train_size:train_size + val_size])\n",
    "#test_scaled=np.zeros_like(data_reshaped[train_size + val_size:])\n",
    "\n",
    "train_scaled = np.zeros_like(train_data)\n",
    "val_scaled = np.zeros_like(val_data)\n",
    "test_scaled = np.zeros_like(test_data)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(data_reshaped.shape[2]):\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1)) #changed to -1,1\n",
    "    # Flatten timesteps and samples for scaling\n",
    "    train_scaled[:, :, i] = scaler.fit_transform(train_data[:, :, i].reshape(-1, 1)).reshape(train_data.shape[0], train_data.shape[1])\n",
    "    val_scaled[:, :, i] = scaler.transform(val_data[:, :, i].reshape(-1, 1)).reshape(val_data.shape[0], val_data.shape[1])\n",
    "    test_scaled[:, :, i] = scaler.transform(test_data[:, :, i].reshape(-1, 1)).reshape(test_data.shape[0], test_data.shape[1])\n",
    "    scalers[i] = scaler  # Save scalers to inverse-transform later\n",
    "\n",
    "#Inverse Scale\n",
    "def inverse_scale(imputation, scalers):\n",
    "    n_samples, n_timesteps, n_features = imputation.shape\n",
    "    imputation_denorm = np.empty_like(imputation)\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        reshaped = imputation[:, :, i].reshape(-1, 1)\n",
    "        inversed = scalers[i].inverse_transform(reshaped)\n",
    "        imputation_denorm[:, :, i] = inversed.reshape(n_samples, n_timesteps)\n",
    "    \n",
    "    return imputation_denorm\n",
    "\n",
    "\n",
    "\n",
    "#Optional: Artificially mask. Mask 20% of the data (MIT part). Try masking 30% here \n",
    "def mcar_f(X, mask_ratio=0.3):\n",
    "    \"\"\"Apply MCAR only to observed values.\"\"\"\n",
    "    observed_mask=~np.isnan(X) #find observed positions\n",
    "    artificial_mask=mcar(X,mask_ratio).astype(bool) #generate MCAR mask, cast to boolean\n",
    "    #combine masks \n",
    "    combined_mask=observed_mask & artificial_mask\n",
    "\n",
    "    #Apply masking\n",
    "    X_masked=X.copy()\n",
    "    X_masked[combined_mask]=np.nan\n",
    "    return X_masked,combined_mask\n",
    "\n",
    "\n",
    "#Use mcar on validation data \n",
    "val_X_masked, val_mask =mcar_f(val_scaled)\n",
    "val_X_ori=val_scaled.copy() \n",
    "\n",
    "test_X_masked, test_mask =mcar_f(test_scaled)\n",
    "test_X_ori=test_scaled.copy() \n",
    "\n",
    "\n",
    "#?? Problem: Can't have the best input for testing\n",
    "#1.Create synthetic test_data cuz if I drop nan values for test set, there's basically nothing left\n",
    "#synthetic_data=np.random.randn(n_samples,n_steps,n_features)\n",
    "#test_X_masked,test_mask=mcar_f(synthetic_data)\n",
    "#test_X_ori=synthetic_data.copy() #Ground truth\n",
    "\n",
    "# 2, Ensure no NaN values in synthetic data\n",
    "#test_X_masked = np.nan_to_num(test_X_masked, nan=np.nanmean(test_X_masked))\n",
    "#test_X_ori = np.nan_to_num(test_X_ori, nan=np.nanmean(test_X_ori))\n",
    "\n",
    "\n",
    "\n",
    "class Config:\n",
    "    no_cuda = False\n",
    "    no_mps = False\n",
    "    seed = 1\n",
    "\n",
    "args=Config()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "use_mps = not args.no_mps and torch.backends.mps.is_available()\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "\n",
    "if args.cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA\")\n",
    "elif use_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "train_scaled = torch.tensor(train_scaled, dtype=torch.float32)\n",
    "val_X_masked = torch.tensor(val_X_masked, dtype=torch.float32)\n",
    "val_X_ori = torch.tensor(val_X_ori, dtype=torch.float32)\n",
    "\n",
    "train_scaled = train_scaled.to(device)\n",
    "val_X_masked = val_X_masked.to(device)\n",
    "val_X_ori = val_X_ori.to(device)\n",
    "\n",
    "\n",
    "#MLflow set up\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "mlflow.set_experiment(\"GP_VAE_2\")\n",
    "\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"lr\": trial.suggest_float(\"lr\", 1e-4, 1e-3, log=True),\n",
    "        \"epochs\": trial.suggest_int(\"epochs\", 10, 50),\n",
    "        \"batch_size\": trial.suggest_int(\"batch_size\", 32, 128, step=32),\n",
    "        \"length_scale\": trial.suggest_float(\"length_scale\",0.5,5.0),\n",
    "        \"beta\": trial.suggest_float(\"beta\",0.1,1.0)\n",
    " }\n",
    "\n",
    "    with mlflow.start_run(run_name=\"GP-VAE-Trial\", nested=True) as run:\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        gp_vae = GPVAE(\n",
    "            n_steps=data_reshaped.shape[1],\n",
    "            n_features=data_reshaped.shape[2],\n",
    "            latent_size=37, #should be the latent dimensions \n",
    "            encoder_sizes=(128,128), #should I change it here too?\n",
    "            decoder_sizes=(256,256), #should I change the model size?\n",
    "            kernel=\"cauchy\",\n",
    "            beta=params[\"beta\"], #The weight of KL divergence in ELBO\n",
    "            M=1,  #The number of Monte Carlo samples for ELBO estimation during training.\n",
    "            K=1,  #The number of importance weights for IWAE model training loss.\n",
    "            sigma=1.005, # The scale parameter for a kernel function\n",
    "            length_scale=params[\"length_scale\"], #The length scale parameter for a kernel function\n",
    "            kernel_scales=1, #The number of different length scales over latent space dimensions\n",
    "            window_size=24,  # Window size for the inference CNN.\n",
    "            batch_size=params[\"batch_size\"],\n",
    "            # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "            epochs=params[\"epochs\"],\n",
    "            # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "            # You can leave it to defualt as None to disable early stopping.\n",
    "            patience=3,\n",
    "            # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "            # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "            optimizer=Adam(lr=params[\"lr\"]),\n",
    "            # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "            # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "            # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "            num_workers=0,\n",
    "            # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "            # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "            device=device,\n",
    "            # set the path for saving tensorboard and trained model files \n",
    "            saving_path=\"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/nadire/gp_vae/best_model\",\n",
    "            # only save the best model after training finished.\n",
    "            # You can also set it as \"better\" to save models performing better ever during training.\n",
    "            model_saving_strategy=\"best\",\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
    "        gp_vae.fit(train_set={\"X\": train_scaled}, val_set={\"X\": val_X_masked, \"X_ori\": val_X_ori})\n",
    "        gp_vae_results = gp_vae.predict({\"X\": test_X_masked}, n_sampling_times=2)\n",
    "        gp_vae_imputation = gp_vae_results[\"imputation\"]\n",
    "\n",
    "        print(f\"The shape of gp_vae_imputation is {gp_vae_imputation.shape}\")\n",
    "\n",
    "        # for error calculation, we need to take the mean value of the multiple samplings for each data sample\n",
    "        mean_gp_vae_imputation = gp_vae_imputation.mean(axis=1)\n",
    "\n",
    "        test_imputation_denorm = inverse_scale(mean_gp_vae_imputation, scalers)\n",
    "        test_ori_denorm = inverse_scale(test_X_ori, scalers)\n",
    "\n",
    "\n",
    "         # Calculate metrics\n",
    "        mae_per_feature = []\n",
    "        rmse_per_feature=[]\n",
    "        percentage_mae_per_feature = []\n",
    "\n",
    "        for i in range(n_features):\n",
    "            imputation_i = test_imputation_denorm[:, :, i]\n",
    "            ground_truth_i = test_ori_denorm[:, :, i]\n",
    "            mask_i = test_mask[:, :, i]\n",
    "            if np.isnan(imputation_i).any() or np.isnan(ground_truth_i).any():\n",
    "                continue\n",
    "            mae_i = calc_mae(imputation_i, ground_truth_i, mask_i)\n",
    "            mae_per_feature.append(mae_i)\n",
    "            rmse_i = np.sqrt(mean_squared_error(imputation_i, ground_truth_i))\n",
    "            rmse_per_feature.append(rmse_i)\n",
    "\n",
    "            #Calculate the original standard deviation for the feature\n",
    "            std_dev_i = np.std(ground_truth_i[mask_i == 1])\n",
    "             # Calculate the percentage of MAE relative to the standard deviation   \n",
    "            if std_dev_i != 0:\n",
    "                percentage_mae_i = (mae_i / std_dev_i) * 100\n",
    "                percentage_mae_per_feature.append(percentage_mae_i)\n",
    "            else:\n",
    "                 percentage_mae_i = float('inf')\n",
    "            \n",
    "            mlflow.log_metric(f\"MAE_{feature_names[i]}\", mae_i)\n",
    "            mlflow.log_metric(f\"RMSE_{feature_names[i]}\",rmse_i)\n",
    "            mlflow.log_metric(f\"Percentage_MAE_{feature_names[i]}\", percentage_mae_i)\n",
    "\n",
    "        avg_mae = np.mean(mae_per_feature)\n",
    "        avg_rmse=np.mean(rmse_per_feature)\n",
    "       \n",
    "        mlflow.log_metric(\"avg_mae\", avg_mae)\n",
    "        mlflow.log_metric(\"avg_rmse\", avg_rmse)\n",
    "\n",
    "        trial.set_user_attr(\"mlflow_run_id\", run.info.run_id)\n",
    "\n",
    "        return avg_mae\n",
    "\n",
    "    print(\"MAE per feature:\", mae_per_feature)\n",
    "    print(\"RMSE per feature\",rmse_per_feature)\n",
    "    print(\"Percentage MAE per feature:\", percentage_mae_per_feature)\n",
    "   \n",
    "\n",
    "# Run Optuna study\n",
    "mlflow.set_experiment(\"GP-VAE-2\")\n",
    "with mlflow.start_run(run_name=\"GPVAE_Optuna_Study(2)\") as parent_run:\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "\n",
    "    best_params = study.best_trial.params\n",
    "    best_value = study.best_trial.value\n",
    "    best_run_id = study.best_trial.user_attrs[\"mlflow_run_id\"]\n",
    "\n",
    "\n",
    "    \n",
    "    # Log best parameters\n",
    "    mlflow.log_params(best_params)\n",
    "\n",
    "    # Log best metric(s)\n",
    "    mlflow.log_metric(\"best_objective_value\", best_value)\n",
    "    mlflow.log_param(\"best_run_id\", best_run_id)\n",
    "\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Best Objective Value:\", best_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
